{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retrieval Dataset\n",
    "\n",
    "In this notebook, we'll create a dataset for our retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell and fill in details if you are running external Python\n",
    "import os\n",
    "key=\"\"\n",
    "with open(\"api-key.txt\", \"r\") as f:\n",
    "    key = f.read().rstrip()\n",
    "os.environ['HOPSWORKS_PROJECT']=\"hm\"\n",
    "os.environ['HOPSWORKS_HOST']=\"35.240.81.237\"\n",
    "os.environ['HOPSWORKS_API_KEY']=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://35.240.81.237:443/p/119\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "First, we'll load the feature groups we created in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_feature_group(\"transactions\",version=1)\n",
    "customers_fg = fs.get_feature_group(\"customers\",version=1)\n",
    "articles_fg = fs.get_feature_group(\"articles\",version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to join these three data sources to make the data compatible with out retrieval model. Recall that each row in the `transactions` feature group relates information about which customer bought which item. We'll join this feature group with the `customers` and `articles` feature groups to inject customer and item features into each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = trans_fg.select([\"customer_id\", \"article_id\", \"month_sin\", \"month_cos\"])\\\n",
    "    .join(customers_fg.select([\"age\"]), on=\"customer_id\")\\\n",
    "    .join(articles_fg.select([\"garment_group_name\", \"index_group_name\"]), on=\"article_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature View Creation\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://35.240.81.237:443/p/119/fs/67/fv/retrieval/version/1\n"
     ]
    }
   ],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='retrieval',\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and explore data in the feature view we can retrieve batch data using the `get_batch_data()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset Creation\n",
    "\n",
    "Finally, we can create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://35.240.81.237/p/119/jobs/named/retrieval_1_2_create_fv_td_28062022103918/executions\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "The Hopsworks Job failed, use the Hopsworks UI to access the job logs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_251702/3526838152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m td_version, td_job = feature_view.create_train_validation_test_splits(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'retrieval_dataset_split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mcreate_train_validation_test_splits\u001b[0;34m(self, val_size, test_size, train_start, train_end, val_start, val_end, test_start, test_end, storage_connector, location, description, data_format, coalesce, seed, statistics_config, write_options)\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# td_job is used only if the python engine is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         td, td_job = self._feature_view_engine.create_training_dataset(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mcreate_training_dataset\u001b[0;34m(self, feature_view_obj, training_dataset_obj, user_write_options)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n\u001b[0;32m--> 144\u001b[0;31m         td_job = self.compute_training_dataset(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0muser_write_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mcompute_training_dataset\u001b[0;34m(self, feature_view_obj, user_write_options, training_dataset_obj, training_dataset_version)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mwith_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n\u001b[0;32m--> 286\u001b[0;31m         td_job = engine.get_instance().write_training_dataset(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mbatch_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mwrite_training_dataset\u001b[0;34m(self, training_dataset, dataset, user_write_options, save_mode, feature_view_obj, to_df)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# If the user passed the wait_for_job option consider it,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# otherwise use the default True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_write_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtd_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36m_wait_for_job\u001b[0;34m(self, job, user_write_options)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 raise exceptions.FeatureStoreException(\n\u001b[0m\u001b[1;32m    705\u001b[0m                     \u001b[0;34m\"The Hopsworks Job failed, use the Hopsworks UI to access the job logs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 )\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: The Hopsworks Job failed, use the Hopsworks UI to access the job logs"
     ]
    }
   ],
   "source": [
    "feature_view = fs.get_feature_view(\"retrieval\", version=1)\n",
    "\n",
    "td_version, td_job = feature_view.create_train_validation_test_splits(\n",
    "    description = 'retrieval_dataset_split',\n",
    "    data_format = 'csv',\n",
    "    val_size = 0.1,\n",
    "    test_size = 0.1,\n",
    "    write_options = {'wait_for_job': True},\n",
    "    coalesce = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
