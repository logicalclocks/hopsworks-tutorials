{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Engineering Development\n",
    "In this section of the demo, we will utilize Snowpark's Python client-side Dataframe API to build and develope code for the **ML Ops pipeline**.  We will take the functions and model training/inference definition from the data scientist and put it into production using the Snowpark server-side runtime and Snowpark Python user-defined functions for ML model training and inference.\n",
    "\n",
    "The ML Engineer will start by exploring the deoployment options and testing the deployed model before building a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.  Feature engineering, train, predict functions from data scientist.  \n",
    "Output: Prediction models available to business users in SQL. Evaluation reports for monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load  credentials and connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect('./include/state.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Create Feature Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F \n",
    "from dags.feature_engineering import generate_holiday_df, generate_weather_df\n",
    "from datetime import datetime\n",
    "\n",
    "state_dict['trips_table_name']='TRIPS'\n",
    "state_dict['holiday_table_name']='HOLIDAYS'\n",
    "state_dict['weather_database_name'] = 'WEATHER_NYC'\n",
    "state_dict['weather_table_name'] = state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY'\n",
    "state_dict['weather_view_name'] = 'WEATHER_NYC_VW'\n",
    "state_dict['model_stage_name']='MODEL_STAGE'\n",
    "state_dict['run_date']='2020_01_01'\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will materialize the holiday dataset as a table instead of calculating each time in the inference and training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = generate_holiday_df(session, state_dict['holiday_table_name']) \n",
    "holiday_df.write.mode('overwrite').saveAsTable(state_dict['holiday_table_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we check that the weather data has been subscribed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"CREATE OR REPLACE DATABASE \"+state_dict['weather_database_name']+\\\n",
    "            \" FROM SHARE \"+state_dict['weather_listing_id']).collect()\n",
    "\n",
    "session.use_database(state_dict['connection_parameters']['database'])\n",
    "session.use_schema(state_dict['connection_parameters']['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a view in the project database referencing our specific features of that weather database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = generate_weather_df(session, state_dict['weather_table_name']) \n",
    "weather_df.create_or_replace_view(state_dict['weather_view_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to forecast features for weather (PRECIP and TEMP) and also holidays.  For weather we could use forecast from the Snowflake Marketplace providers like Weather Source.  Since we went back in time for this hands-on-lab the weather \"forecast\" is actually in the historical weather tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forecast_df(session, trips_table_name:str, holiday_table_name:str, weather_view_name:str, steps:int):\n",
    "    from dags.feature_engineering import generate_holiday_df\n",
    "    from datetime import timedelta, datetime\n",
    "    from snowflake.snowpark import functions as F \n",
    "    \n",
    "    start_date = session.table(trips_table_name)\\\n",
    "                        .select(F.to_date(F.max('STARTTIME'))).collect()[0][0]+timedelta(days=1)\n",
    "    #start_date = datetime.strptime('2020-03-01', '%Y-%m-%d')\n",
    "    end_date = start_date+timedelta(days=steps)\n",
    "\n",
    "    #check if it tables already materialized, otherwise generate DF\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns()\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "    weather_df = session.table(weather_view_name)\n",
    "        \n",
    "    forecast_df = holiday_df.join(weather_df[['DATE','PRECIP','TEMP']], 'DATE', join_type='right')\\\n",
    "                            .na.fill({'HOLIDAY':0})\\\n",
    "                            .filter((F.col('DATE') >= start_date) &\\\n",
    "                                    (F.col('DATE') <= end_date))\\\n",
    "                            .sort('DATE', ascending=True)\n",
    "    return forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_steps=30\n",
    "forecast_df = create_forecast_df(session=session, \n",
    "                                 trips_table_name=state_dict['trips_table_name'],\n",
    "                                 holiday_table_name=state_dict['holiday_table_name'], \n",
    "                                 weather_view_name=state_dict['weather_view_name'], \n",
    "                                 steps=forecast_steps)\n",
    "forecast_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized feature generation\n",
    "Previously the data scientist picked one station for training and predictions.  We want to generate features for all stations in parallel.  We can leverage the power of the Snowflake SQL execution engine for this and Snowpark allows us to write it in python.  \n",
    "\n",
    "Snowflake [window functions](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.html#snowflake.snowpark.Window) are a powerful tool for vectorizing work.  Our initial feature engineering code from the data scientist used window functions to calculate the lag features.\n",
    "\n",
    "We can create multi-level window functions to partition by station_id and then group by date within that window.  \n",
    "  \n",
    "Also we will create a second window in order to make sure that each station of our features have at least 2 years worth of data (needed for the model to pickup seasonality) and has activity up to the latest date of the current dataset (to build forecast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sid_date_window = snp.Window.partition_by(F.col('STATION_ID')).order_by(F.col('DATE').asc())\n",
    "sid_window = snp.Window.partition_by(F.col('STATION_ID'))\n",
    "latest_date = session.table(state_dict['trips_table_name']).select(F.to_char(F.to_date(F.max('STARTTIME')))).collect()[0][0]\n",
    "\n",
    "historical_df = session.table(state_dict['trips_table_name'])\\\n",
    "                       .select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                               F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                       .group_by(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                            .count()\\\n",
    "                       .with_column('LAG_1', F.lag(F.col('COUNT'), offset=1).over(sid_date_window))\\\n",
    "                       .with_column('LAG_7', F.lag(F.col('COUNT'), offset=7).over(sid_date_window))\\\n",
    "                       .with_column('LAG_90', F.lag(F.col('COUNT'), offset=90).over(sid_date_window))\\\n",
    "                       .with_column('LAG_365', F.lag(F.col('COUNT'), offset=365).over(sid_date_window))\\\n",
    "                            .na.drop()\\\n",
    "                       .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                       .join(weather_df[['DATE','PRECIP','TEMP']], 'DATE', 'inner')\\\n",
    "                       .with_column('DAY_COUNT', F.count(F.col('DATE')).over(sid_window))\\\n",
    "                            .filter(F.col('DAY_COUNT') >= 365*2)\\\n",
    "                       .with_column('MAX_DATE', F.max('DATE').over(sid_window))\\\n",
    "                            .filter(F.col('MAX_DATE') == latest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all stations have data up to the latest date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df.group_by('STATION_ID').max('DATE').select(F.min(F.col('MAX(DATE)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature set should not include any stations with less than 730 days (365*2) of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df.select(F.min('DAY_COUNT')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop the day count and max date columns since we don't really need them beyond verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df = historical_df.drop(['DAY_COUNT', 'MAX_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how many stations have at least two years of data with recent activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df.select('STATION_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create UDF for Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a time series prediction we will retrain a model each time we do inference.  We don't need to save the model artefacts but we will save the predictions in an predictions table.  \n",
    "  \n",
    "Here we can use Snowpark User Defined Functions for training as well as inference without having to pull data out of Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/station_train_predict.py\n",
    "def station_train_predict_func(historical_data:list, \n",
    "                               historical_column_names:list, \n",
    "                               target_column:str,\n",
    "                               cutpoint: int, \n",
    "                               max_epochs: int, \n",
    "                               forecast_data:list,\n",
    "                               forecast_column_names:list,\n",
    "                               lag_values:list):\n",
    "    \n",
    "    from torch import tensor\n",
    "    import pandas as pd\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "    \n",
    "    feature_columns = historical_column_names.copy()\n",
    "    feature_columns.remove('DATE')\n",
    "    feature_columns.remove(target_column)\n",
    "    forecast_steps = len(forecast_data)\n",
    "    \n",
    "    df = pd.DataFrame(historical_data, columns = historical_column_names)\n",
    "    \n",
    "    ##In order to do train/valid split on time-based portion the input data must be sorted by date    \n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.sort_values(by='DATE', ascending=True)\n",
    "    \n",
    "    y_valid = df[target_column][-cutpoint:].values.reshape(-1, 1)\n",
    "    X_valid = df[feature_columns][-cutpoint:].values\n",
    "    y_train = df[target_column][:-cutpoint].values.reshape(-1, 1)\n",
    "    X_train = df[feature_columns][:-cutpoint].values\n",
    "    \n",
    "    model = TabNetRegressor()\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=100,\n",
    "        batch_size=128, \n",
    "        virtual_batch_size=64,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "    \n",
    "    df['PRED'] = model.predict(tensor(df[feature_columns].values))\n",
    "\n",
    "    #Now make the multi-step forecast\n",
    "    if len(lag_values) > 0:\n",
    "        forecast_df = pd.DataFrame(forecast_data, columns = forecast_column_names)\n",
    "        \n",
    "        for step in range(forecast_steps):\n",
    "            #station_id = df.iloc[-1]['STATION_ID']\n",
    "            future_date = df.iloc[-1]['DATE']+timedelta(days=1)\n",
    "            lags=[df.shift(lag-1).iloc[-1]['COUNT'] for lag in lag_values]\n",
    "            forecast=forecast_df.loc[forecast_df['DATE']==future_date.strftime('%Y-%m-%d')]\n",
    "            forecast=forecast.drop(labels='DATE', axis=1).values.tolist()[0]\n",
    "            features=[*lags, *forecast]\n",
    "            pred=round(model.predict(np.array([features]))[0][0])\n",
    "            row=[future_date, pred, *features, pred]\n",
    "            df.loc[len(df)]=row\n",
    "    \n",
    "    explain_df = pd.DataFrame(model.explain(df[feature_columns].astype(float).values)[0], \n",
    "                         columns = feature_columns).add_prefix('EXPL_').round(2)\n",
    "    df = pd.concat([df.set_index('DATE').reset_index(), explain_df], axis=1)\n",
    "    df['DATE'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return [df[:-forecast_steps].to_json(orient='records', lines=False), \n",
    "            df[-forecast_steps:].to_json(orient='records', lines=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Snowpark server-side Anaconda runtime has a large [list of Python modules included](https://docs.snowflake.com/en/LIMITEDACCESS/udf-python-packages.html#list-of-the-third-party-packages-from-anaconda) for our UDF.  However, the data scientist built this code based on pytorch-tabnet which is not currently in the Snowpark distribution.\n",
    "  \n",
    "  We can simply add [pytorch_tabnet](https://github.com/dreamquark-ai/tabnet), as well as our own team's python code, as import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.station_train_predict import station_train_predict_func\n",
    "from snowflake.snowpark import types as T\n",
    "import os \n",
    "\n",
    "#We can add dependencies from locally installed directories\n",
    "#source_dir = os.environ['CONDA_PREFIX']+'/lib/python3.8/site-packages/'\n",
    "\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS '+state_dict['model_stage_name']).collect()\n",
    "\n",
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "dep_packages=[\"pandas==1.3.5\", \"pytorch==1.10.2\", \"scipy==1.7.1\", \"scikit-learn==1.0.2\", \"setuptools==58.0.4\", \"cloudpickle==2.0.0\"]\n",
    "dep_imports=['./include/pytorch_tabnet.zip', 'dags']\n",
    "\n",
    "station_train_predict_udf = session.udf.register(station_train_predict_func, \n",
    "                                                 name=\"station_train_predict_udf\",\n",
    "                                                 is_permanent=True,\n",
    "                                                 stage_location='@'+state_dict['model_stage_name'], \n",
    "                                                 imports=dep_imports,\n",
    "                                                 packages=dep_packages,\n",
    "                                                 input_types=[T.ArrayType(), \n",
    "                                                              T.ArrayType(), \n",
    "                                                              T.StringType(), \n",
    "                                                              T.IntegerType(), \n",
    "                                                              T.IntegerType(), \n",
    "                                                              T.ArrayType(), \n",
    "                                                              T.ArrayType(), \n",
    "                                                              T.ArrayType()],\n",
    "                                                 return_type=T.VariantType(),\n",
    "                                                 replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the training and inference\n",
    "\n",
    "Because we currently only have scalar functions for Snowpark Python UDFs we must aggregate the features for each station to a single cell.   This will be much easier in the future with vectorized input and user-defined table functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_column_list = historical_df.columns\n",
    "historical_column_list.remove('STATION_ID')\n",
    "historical_column_names = F.array_construct(*[F.lit(x) for x in historical_column_list])\n",
    "\n",
    "historical_df = historical_df.group_by(F.col('STATION_ID'))\\\n",
    "                             .agg(F.array_agg(F.array_construct(*historical_column_list))\\\n",
    "                                  .alias('HISTORICAL_DATA'))\n",
    "\n",
    "forecast_column_list = forecast_df.columns\n",
    "forecast_column_names = F.array_construct(*[F.lit(x) for x in forecast_column_list])\n",
    "forecast_df = forecast_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('FORECAST_DATA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the training/inference pipeline and prediction output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the array-stuffed dataframes we can now call the UDF for training, predictions and forecasting.  We will write the predictions to a table in variant (JSON) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse(state_dict['compute_parameters']['train_warehouse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cutpoint=365\n",
    "max_epochs = 10\n",
    "target_column = 'COUNT'\n",
    "lag_values=[1,7,90,365]\n",
    "lag_values_array = F.array_construct(*[F.lit(x) for x in lag_values])\n",
    "\n",
    "historical_df.join(forecast_df)\\\n",
    "             .select(F.col('STATION_ID'),\n",
    "                     F.call_udf('station_train_predict_udf', \n",
    "                                F.col('HISTORICAL_DATA'),\n",
    "                                F.lit(historical_column_names), \n",
    "                                F.lit(target_column),\n",
    "                                F.lit(cutpoint), \n",
    "                                F.lit(max_epochs),\n",
    "                                F.col('FORECAST_DATA'),\n",
    "                                F.lit(forecast_column_names),\n",
    "                                F.lit(lag_values_array)).alias('PRED_DATA'))\\\n",
    "           .write.mode('overwrite')\\\n",
    "           .save_as_table('PRED_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how the model performs for some stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pred_df=session.table('PRED_test').filter(F.col('STATION_ID') == '519')\\\n",
    "                .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[0]).alias('PRED_DATA'))\\\n",
    "\n",
    "output_json=pred_df.select('PRED_DATA').collect()\n",
    "\n",
    "df = pd.read_json(output_json[0]['PRED_DATA'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, x_lab:str, y_true_lab:str, y_pred_lab:str):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    df = pd.melt(df, id_vars=[x_lab], value_vars=[y_true_lab, y_pred_lab])\n",
    "    ax = sns.lineplot(x=x_lab, y='value', hue='variable', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "   \n",
    "plot(df[-90:], 'DATE', 'COUNT', 'PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "For ML governance we need to monitor model performance over time. We will be building 100's of models (one per station) so as part of the pipeline we will add a step to evaluate model performance and save metrics for each training/inference run.\n",
    "\n",
    "Since the data science teams may use many different model frameworks, we want to have a standard evaluation framework instead of using the model's built-in evaluation which may different for each framework or version. \n",
    "\n",
    "We will use [rexmex](https://rexmex.readthedocs.io/en/latest/index.html) for consistent evaluation rather than the models' built-in eval metrics. \n",
    "\n",
    "We will deploy the evaluation functions to the Snowpark Python server-side runtime as a Stored Procedure. We will save the model performance metrics in tables for historical analysis and drift detection as well as full reproducibility to support the company's GDPR policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rexmex import RatingMetricSet, ScoreCard\n",
    "\n",
    "metric_set = RatingMetricSet()\n",
    "score_card = ScoreCard(metric_set)\n",
    "\n",
    "pred_df = session.table('PRED_test')\\\n",
    "                 .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[0]).alias('PRED_DATA'))\n",
    "\n",
    "output_df=pd.DataFrame()\n",
    "for row in pred_df.to_local_iterator():\n",
    "    eval_df = pd.read_json(row.as_dict()['PRED_DATA'])\n",
    "    eval_df = eval_df.rename(columns={'COUNT': 'y_true', 'PRED':'y_score'})\n",
    "    eval_df = score_card.generate_report(eval_df) #.reset_index()\n",
    "    eval_df['STATION_ID'] = row.as_dict()['STATION_ID']\n",
    "    output_df = pd.concat([output_df, eval_df.drop('pearson_correlation', axis=1)])\n",
    "\n",
    "session.create_dataframe(output_df).write.mode('overwrite').save_as_table('EVAL_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create a Snowpark Python UDF to evaluate performance for all models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/model_eval.py\n",
    "def eval_model_func(input_data: str, \n",
    "                    y_true_name: str, \n",
    "                    y_score_name: str):\n",
    "    import pandas as pd\n",
    "    from rexmex import RatingMetricSet, ScoreCard\n",
    "        \n",
    "    metric_set = RatingMetricSet()\n",
    "    score_card = ScoreCard(metric_set)\n",
    "\n",
    "    df = pd.read_json(input_data)\n",
    "    df.rename(columns={y_true_name: 'y_true', y_score_name:'y_score'}, inplace=True)\n",
    "    \n",
    "    df = score_card.generate_report(df).reset_index()\n",
    "    \n",
    "    return df.to_json(orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the UDF to Snowflake makes it available for all users.  This is a regression evaluation.  Likely we will want to deploy a categorical function as well or add if/then logic to our single instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.model_eval import eval_model_func\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "model_stage_name = state_dict['model_stage_name']\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS '+model_stage_name).collect()\n",
    "\n",
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "dep_packages=[\"pandas==1.3.5\", \"scikit-learn==1.0.2\", \"cloudpickle==2.0.0\"]\n",
    "dep_imports=['./include/rexmex.zip', 'dags']\n",
    "\n",
    "eval_model_output_udf = session.udf.register(eval_model_func, \n",
    "                                             name=\"eval_model_output_udf\",\n",
    "                                             is_permanent=True,\n",
    "                                             stage_location='@'+str(model_stage_name), \n",
    "                                             imports=dep_imports,\n",
    "                                             packages=dep_packages,\n",
    "                                             input_types=[T.StringType(), \n",
    "                                                          T.StringType(), \n",
    "                                                          T.StringType()],\n",
    "                                             return_type=T.VariantType(),\n",
    "                                             replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_true_name='COUNT'\n",
    "y_score_name='PRED'\n",
    "run_date = datetime.strptime(state_dict['run_date'], '%Y_%m_%d').date()\n",
    "\n",
    "session.table('PRED_test')\\\n",
    "       .select('STATION_ID',\n",
    "               F.call_udf('eval_model_output_udf',\n",
    "                          F.parse_json(F.col('PRED_DATA')[0]),\n",
    "                          F.lit(y_true_name),\n",
    "                          F.lit(y_score_name)).alias('EVAL_DATA'))\\\n",
    "       .with_column('RUN_DATE', F.to_date(F.lit(run_date)))\\\n",
    "       .write.mode('overwrite')\\\n",
    "       .save_as_table('EVAL_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = session.table('EVAL_TEST')\n",
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could use a Stored Procedure.  This gives us lots of flexibility but will be slower for parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stage_name = state_dict['model_stage_name']\n",
    "_ = session.sql('CREATE STAGE IF NOT EXISTS '+model_stage_name).collect()\n",
    "\n",
    "session.file.put('./include/rexmex.zip', model_stage_name)\n",
    "\n",
    "sql_cmd = '''\n",
    "CREATE OR REPLACE PROCEDURE EVAL_MODEL_SPROC(pred_table STRING, \n",
    "                                             y_true_name STRING, \n",
    "                                             y_score_name STRING,\n",
    "                                             eval_table STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.8'\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pandas==1.3.5', 'scikit-learn')\n",
    "IMPORTS = ('@model_stage/rexmex.zip')\n",
    "HANDLER = 'eval_model_func'\n",
    "AS\n",
    "$$\n",
    "def eval_model_func(session, \n",
    "                    pred_table:str, \n",
    "                    y_true_name:str, \n",
    "                    y_score_name:str,\n",
    "                    eval_table:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    import pandas as pd\n",
    "    from rexmex import RatingMetricSet, ScoreCard\n",
    "\n",
    "    metric_set = RatingMetricSet()\n",
    "    score_card = ScoreCard(metric_set)\n",
    "\n",
    "    pred_df = session.table(pred_table)\\\n",
    "                     .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[0]).alias('PRED_DATA'))\n",
    "    output_df=pd.DataFrame()\n",
    "    for row in pred_df.to_local_iterator():\n",
    "        eval_df = pd.read_json(row.as_dict()['PRED_DATA'])\n",
    "        eval_df = eval_df.rename(columns={y_true_name: 'y_true', y_score_name:'y_score'})\n",
    "        eval_df = score_card.generate_report(eval_df)\n",
    "        eval_df['STATION_ID'] = row.as_dict()['STATION_ID']\n",
    "        output_df = pd.concat([output_df, eval_df.drop('pearson_correlation', axis=1)])\n",
    "\n",
    "    session.create_dataframe(output_df).write.mode('overwrite').save_as_table('EVAL_test')\n",
    "\n",
    "    return \"SUCCESS\"\n",
    "$$\n",
    "'''\n",
    "\n",
    "#session.sql(sql_cmd).collect()\n",
    "#session.sql(\"CALL EVAL_MODEL_SPROC(\\'PRED_test\\', \\'COUNT\\', \\'PRED\\', \\'EVAL_test\\')\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we will create flattened versions of the PRED, FORECAST and EVAL tables by parsing the variant (JSON) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_table_name='PRED_test'\n",
    "forecast_table_name='FORECAST_test'\n",
    "eval_table_name='EVAL_test'\n",
    "session.table(pred_table_name)\\\n",
    "       .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[0]).alias('PRED_DATA'))\\\n",
    "       .flatten('PRED_DATA').select('STATION_ID', F.col('VALUE').alias('PRED_DATA'))\\\n",
    "       .select('STATION_ID', \n",
    "               F.to_date(F.col('PRED_DATA')['DATE']).alias('DATE'),\n",
    "               F.as_integer(F.col('PRED_DATA')['COUNT']).alias('COUNT'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_1']).alias('LAG_1'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_7']).alias('LAG_7'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_90']).alias('LAG_90'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_365']).alias('LAG_365'),\n",
    "               F.as_integer(F.col('PRED_DATA')['HOLIDAY']).alias('HOLIDAY'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['PRECIP']).alias('PRECIP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['TEMP']).alias('TEMP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['PRED']).alias('PRED'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_1']).alias('EXPL_LAG_1'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_7']).alias('EXPL_LAG_7'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_90']).alias('EXPL_LAG_90'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_365']).alias('EXPL_LAG_365'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_HOLIDAY']).alias('EXPL_HOLIDAY'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_PRECIP']).alias('EXPL_PRECIP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_TEMP']).alias('EXPL_TEMP'))\\\n",
    "       .write.mode('overwrite').save_as_table('flat_'+pred_table_name)\n",
    "\n",
    "#forecast are in position 2 of the pred_table\n",
    "session.table(pred_table_name)\\\n",
    "       .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[1]).alias('PRED_DATA'))\\\n",
    "       .flatten('PRED_DATA').select('STATION_ID', F.col('VALUE').alias('PRED_DATA'))\\\n",
    "       .select('STATION_ID', \n",
    "               F.to_date(F.col('PRED_DATA')['DATE']).alias('DATE'),\n",
    "               F.as_integer(F.col('PRED_DATA')['COUNT']).alias('COUNT'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_1']).alias('LAG_1'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_7']).alias('LAG_7'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_90']).alias('LAG_90'),\n",
    "               F.as_integer(F.col('PRED_DATA')['LAG_365']).alias('LAG_365'),\n",
    "               F.as_integer(F.col('PRED_DATA')['HOLIDAY']).alias('HOLIDAY'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['PRECIP']).alias('PRECIP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['TEMP']).alias('TEMP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['PRED']).alias('PRED'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_1']).alias('EXPL_LAG_1'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_7']).alias('EXPL_LAG_7'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_90']).alias('EXPL_LAG_90'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_365']).alias('EXPL_LAG_365'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_HOLIDAY']).alias('EXPL_HOLIDAY'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_PRECIP']).alias('EXPL_PRECIP'),\n",
    "               F.as_decimal(F.col('PRED_DATA')['EXPL_TEMP']).alias('EXPL_TEMP'))\\\n",
    "       .write.mode('overwrite').save_as_table('flat_'+forecast_table_name)\n",
    "\n",
    "session.table(eval_table_name)\\\n",
    "           .select('RUN_DATE', 'STATION_ID', F.parse_json(F.col('EVAL_DATA')).alias('EVAL_DATA'))\\\n",
    "           .flatten('EVAL_DATA').select('RUN_DATE', 'STATION_ID', F.col('VALUE').alias('EVAL_DATA'))\\\n",
    "           .select('RUN_DATE', 'STATION_ID', \n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mae'], 10, 2).alias('mae'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mape'], 10, 2).alias('mape'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mse'], 10, 2).alias('mse'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['r_squared'], 10, 2).alias('r_squared'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['rmse'], 10, 2).alias('rmse'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['smape'], 10, 2).alias('smape'),)\\\n",
    "           .write.mode('append').save_as_table('flat_EVAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will end by consolidating the functions we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/mlops_pipeline.py\n",
    "\n",
    "def materialize_holiday_table(session, holiday_table_name:str) -> str:\n",
    "    from dags.feature_engineering import generate_holiday_df\n",
    "    \n",
    "    holiday_df = generate_holiday_df(session=session, holiday_table_name=holiday_table_name)\n",
    "    holiday_df.write.mode('overwrite').saveAsTable(holiday_table_name)\n",
    "    \n",
    "    return holiday_table_name\n",
    "\n",
    "def subscribe_to_weather_data(session, \n",
    "                              weather_database_name:str, \n",
    "                              weather_listing_id:str) -> str:\n",
    "    \n",
    "    session.sql(\"CREATE DATABASE IF NOT EXISTS \"+weather_database_name+\\\n",
    "                \" FROM SHARE \"+weather_listing_id).collect()\n",
    "    \n",
    "    return weather_database_name\n",
    "\n",
    "def create_weather_view(session, weather_table_name:str, weather_view_name:str) -> str:\n",
    "    from dags.feature_engineering import generate_weather_df\n",
    "\n",
    "    weather_df = generate_weather_df(session=session, weather_table_name=weather_table_name)\n",
    "    \n",
    "    weather_df.create_or_replace_view(weather_view_name)\n",
    "    \n",
    "    return weather_view_name\n",
    "\n",
    "def deploy_pred_train_udf(session, udf_name:str, function_name:str, model_stage_name:str) -> str:\n",
    "    from dags.station_train_predict import station_train_predict_func\n",
    "    from snowflake.snowpark import types as T\n",
    "\n",
    "    session.clear_packages()\n",
    "    session.clear_imports()\n",
    "    dep_packages=[\"pandas==1.3.5\", \"pytorch==1.10.2\", \"scipy==1.7.1\", \"scikit-learn==1.0.2\", \"setuptools==58.0.4\", \"cloudpickle==2.0.0\"]\n",
    "    dep_imports=['./include/pytorch_tabnet.zip', 'dags']\n",
    "\n",
    "    station_train_predict_udf = session.udf.register(station_train_predict_func, \n",
    "                                                     name=udf_name,\n",
    "                                                     is_permanent=True,\n",
    "                                                     stage_location='@'+str(model_stage_name), \n",
    "                                                     imports=dep_imports,\n",
    "                                                     packages=dep_packages,\n",
    "                                                     input_types=[T.ArrayType(), \n",
    "                                                                  T.ArrayType(), \n",
    "                                                                  T.StringType(), \n",
    "                                                                  T.IntegerType(), \n",
    "                                                                  T.IntegerType(), \n",
    "                                                                  T.ArrayType(), \n",
    "                                                                  T.ArrayType(), \n",
    "                                                                  T.ArrayType()],\n",
    "                                                     return_type=T.VariantType(),\n",
    "                                                     replace=True)\n",
    "    return station_train_predict_udf.name\n",
    "\n",
    "\n",
    "def deploy_eval_udf(session, udf_name:str, function_name:str, model_stage_name:str) -> str:\n",
    "    from dags.model_eval import eval_model_func\n",
    "    from snowflake.snowpark import types as T\n",
    "\n",
    "    session.clear_packages()\n",
    "    session.clear_imports()\n",
    "    dep_packages=['pandas==1.3.5', 'scikit-learn==1.0.2', \"cloudpickle==2.0.0\"]\n",
    "    dep_imports=['./include/rexmex.zip', 'dags']\n",
    "\n",
    "    eval_model_output_udf = session.udf.register(eval_model_func, \n",
    "                                                 name=udf_name,\n",
    "                                                 is_permanent=True,\n",
    "                                                 stage_location='@'+str(model_stage_name), \n",
    "                                                 imports=dep_imports,\n",
    "                                                 packages=dep_packages,\n",
    "                                                 input_types=[T.StringType(), \n",
    "                                                              T.StringType(), \n",
    "                                                              T.StringType()],\n",
    "                                                 return_type=T.VariantType(),\n",
    "                                                 replace=True)\n",
    "    return eval_model_output_udf.name\n",
    "\n",
    "def create_forecast_table(session, \n",
    "                          trips_table_name:str,\n",
    "                          holiday_table_name:str, \n",
    "                          weather_view_name:str, \n",
    "                          forecast_table_name:str,\n",
    "                          steps:int):\n",
    "    \n",
    "    from dags.feature_engineering import generate_holiday_df\n",
    "    from datetime import timedelta, datetime\n",
    "    from snowflake.snowpark import functions as F \n",
    "    \n",
    "    start_date = session.table(trips_table_name)\\\n",
    "                        .select(F.to_date(F.max('STARTTIME'))).collect()[0][0]+timedelta(days=1)\n",
    "    end_date = start_date+timedelta(days=steps)\n",
    "\n",
    "    #check if it tables already materialized, otherwise generate DF\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "    weather_df = session.table(weather_view_name)\n",
    "        \n",
    "    forecast_df = holiday_df.join(weather_df[['DATE','PRECIP','TEMP']], 'DATE', join_type='right')\\\n",
    "                            .na.fill({'HOLIDAY':0})\\\n",
    "                            .filter((F.col('DATE') >= start_date) &\\\n",
    "                                    (F.col('DATE') <= end_date))\\\n",
    "                            .sort('DATE', ascending=True)\n",
    "    \n",
    "    forecast_df.write.mode('overwrite').save_as_table(forecast_table_name)\n",
    "    \n",
    "    return forecast_table_name\n",
    "\n",
    "\n",
    "def create_feature_table(session, \n",
    "                         trips_table_name:str, \n",
    "                         holiday_table_name:str, \n",
    "                         weather_view_name:str,\n",
    "                         feature_table_name:str) -> list:\n",
    "\n",
    "    import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F \n",
    "    from dags.feature_engineering import generate_holiday_df, generate_weather_df\n",
    "    \n",
    "    #check if it tables already materialized, otherwise generate DF\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "    weather_df = session.table(weather_view_name)\n",
    "    \n",
    "    sid_date_window = snp.Window.partition_by(F.col('STATION_ID')).order_by(F.col('DATE').asc())\n",
    "    sid_window = snp.Window.partition_by(F.col('STATION_ID'))\n",
    "    latest_date = session.table(trips_table_name).select(F.to_char(F.to_date(F.max('STARTTIME')))).collect()[0][0]\n",
    "    \n",
    "    feature_df = session.table(trips_table_name)\\\n",
    "                        .select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                                F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                        .group_by(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                                .count()\\\n",
    "                        .with_column('LAG_1', F.lag(F.col('COUNT'), offset=1).over(sid_date_window))\\\n",
    "                        .with_column('LAG_7', F.lag(F.col('COUNT'), offset=7).over(sid_date_window))\\\n",
    "                        .with_column('LAG_90', F.lag(F.col('COUNT'), offset=90).over(sid_date_window))\\\n",
    "                        .with_column('LAG_365', F.lag(F.col('COUNT'), offset=365).over(sid_date_window))\\\n",
    "                            .na.drop()\\\n",
    "                        .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0})\\\n",
    "                        .join(weather_df[['DATE','PRECIP','TEMP']], 'DATE', 'inner')\\\n",
    "                        .with_column('DAY_COUNT', F.count(F.col('DATE')).over(sid_window))\\\n",
    "                            .filter(F.col('DAY_COUNT') >= 365*2)\\\n",
    "                        .with_column('MAX_DATE', F.max('DATE').over(sid_window))\\\n",
    "                            .filter(F.col('MAX_DATE') == latest_date)\\\n",
    "                        .drop(['DAY_COUNT', 'MAX_DATE'])\n",
    "    \n",
    "    feature_df.write.mode('overwrite').save_as_table(feature_table_name)\n",
    "    \n",
    "    return feature_table_name\n",
    "\n",
    "def train_predict(session, \n",
    "                  station_train_pred_udf_name:str, \n",
    "                  feature_table_name:str, \n",
    "                  forecast_table_name:str,\n",
    "                  pred_table_name:str) -> list:\n",
    "    \n",
    "    from snowflake.snowpark import functions as F\n",
    "    \n",
    "    cutpoint=365\n",
    "    max_epochs = 10\n",
    "    target_column = 'COUNT'\n",
    "    lag_values=[1,7,90,365]\n",
    "    lag_values_array = F.array_construct(*[F.lit(x) for x in lag_values])\n",
    "    \n",
    "    historical_df = session.table(feature_table_name)\n",
    "    historical_column_list = historical_df.columns\n",
    "    historical_column_list.remove('STATION_ID')\n",
    "    historical_column_names = F.array_construct(*[F.lit(x) for x in historical_column_list])\n",
    "\n",
    "    historical_df = historical_df.group_by(F.col('STATION_ID'))\\\n",
    "                                 .agg(F.array_agg(F.array_construct(*historical_column_list))\\\n",
    "                                      .alias('HISTORICAL_DATA'))\n",
    "\n",
    "    forecast_df = session.table(forecast_table_name)\n",
    "    forecast_column_names = F.array_construct(*[F.lit(x) for x in forecast_df.columns])\n",
    "    forecast_df = forecast_df.select(F.array_agg(F.array_construct(F.col('*'))).alias('FORECAST_DATA'))\n",
    "\n",
    "    pred_df = historical_df.join(forecast_df)\\\n",
    "                           .select(F.col('STATION_ID'),\n",
    "                         F.call_udf(station_train_pred_udf_name, \n",
    "                                    F.col('HISTORICAL_DATA'),\n",
    "                                    F.lit(historical_column_names), \n",
    "                                    F.lit(target_column),\n",
    "                                    F.lit(cutpoint), \n",
    "                                    F.lit(max_epochs),\n",
    "                                    F.col('FORECAST_DATA'),\n",
    "                                    F.lit(forecast_column_names),\n",
    "                                    F.lit(lag_values_array)).alias('PRED_DATA'))\\\n",
    "                 .write.mode('overwrite')\\\n",
    "                 .save_as_table(pred_table_name)\n",
    "\n",
    "    return pred_table_name\n",
    "\n",
    "def evaluate_station_model(session, \n",
    "                           run_date:str, \n",
    "                           eval_model_udf_name:str, \n",
    "                           pred_table_name:str, \n",
    "                           eval_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    from datetime import datetime\n",
    "    \n",
    "    y_true_name='COUNT'\n",
    "    y_score_name='PRED'\n",
    "    run_date=datetime.strptime(run_date, '%Y_%m_%d').date()\n",
    "\n",
    "    session.table(pred_table_name)\\\n",
    "           .select('STATION_ID',\n",
    "                   F.call_udf(eval_model_udf_name,\n",
    "                              F.parse_json(F.col('PRED_DATA')[0]),\n",
    "                              F.lit(y_true_name),\n",
    "                              F.lit(y_score_name)).alias('EVAL_DATA'))\\\n",
    "           .with_column('RUN_DATE', F.to_date(F.lit(run_date)))\\\n",
    "           .write.mode('overwrite')\\\n",
    "           .save_as_table(eval_table_name)\n",
    "    \n",
    "    return eval_table_name\n",
    "\n",
    "def flatten_tables(session, pred_table_name:str, forecast_table_name:str, eval_table_name:str):\n",
    "    from snowflake.snowpark import functions as F\n",
    "    \n",
    "    session.table(pred_table_name)\\\n",
    "           .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[0]).alias('PRED_DATA'))\\\n",
    "           .flatten('PRED_DATA').select('STATION_ID', F.col('VALUE').alias('PRED_DATA'))\\\n",
    "           .select('STATION_ID', \n",
    "                   F.to_date(F.col('PRED_DATA')['DATE']).alias('DATE'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['COUNT']).alias('COUNT'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_1']).alias('LAG_1'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_7']).alias('LAG_7'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_90']).alias('LAG_90'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_365']).alias('LAG_365'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['HOLIDAY']).alias('HOLIDAY'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['PRECIP']).alias('PRECIP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['TEMP']).alias('TEMP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['PRED']).alias('PRED'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_1']).alias('EXPL_LAG_1'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_7']).alias('EXPL_LAG_7'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_90']).alias('EXPL_LAG_90'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_365']).alias('EXPL_LAG_365'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_HOLIDAY']).alias('EXPL_HOLIDAY'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_PRECIP']).alias('EXPL_PRECIP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_TEMP']).alias('EXPL_TEMP'))\\\n",
    "           .write.mode('overwrite').save_as_table('flat_PRED')\n",
    "\n",
    "    #forecast are in position 2 of the pred_table\n",
    "    session.table(pred_table_name)\\\n",
    "           .select('STATION_ID', F.parse_json(F.col('PRED_DATA')[1]).alias('PRED_DATA'))\\\n",
    "           .flatten('PRED_DATA').select('STATION_ID', F.col('VALUE').alias('PRED_DATA'))\\\n",
    "           .select('STATION_ID', \n",
    "                   F.to_date(F.col('PRED_DATA')['DATE']).alias('DATE'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['COUNT']).alias('COUNT'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_1']).alias('LAG_1'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_7']).alias('LAG_7'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_90']).alias('LAG_90'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['LAG_365']).alias('LAG_365'),\n",
    "                   F.as_integer(F.col('PRED_DATA')['HOLIDAY']).alias('HOLIDAY'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['PRECIP']).alias('PRECIP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['TEMP']).alias('TEMP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['PRED']).alias('PRED'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_1']).alias('EXPL_LAG_1'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_7']).alias('EXPL_LAG_7'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_90']).alias('EXPL_LAG_90'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_LAG_365']).alias('EXPL_LAG_365'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_HOLIDAY']).alias('EXPL_HOLIDAY'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_PRECIP']).alias('EXPL_PRECIP'),\n",
    "                   F.as_decimal(F.col('PRED_DATA')['EXPL_TEMP']).alias('EXPL_TEMP'))\\\n",
    "           .write.mode('overwrite').save_as_table('flat_FORECAST')\n",
    "\n",
    "    session.table(eval_table_name)\\\n",
    "           .select('RUN_DATE', 'STATION_ID', F.parse_json(F.col('EVAL_DATA')).alias('EVAL_DATA'))\\\n",
    "           .flatten('EVAL_DATA').select('RUN_DATE', 'STATION_ID', F.col('VALUE').alias('EVAL_DATA'))\\\n",
    "           .select('RUN_DATE', 'STATION_ID', \n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mae'], 10, 2).alias('mae'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mape'], 10, 2).alias('mape'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['mse'], 10, 2).alias('mse'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['r_squared'], 10, 2).alias('r_squared'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['rmse'], 10, 2).alias('rmse'),\n",
    "                   F.as_decimal(F.col('EVAL_DATA')['smape'], 10, 2).alias('smape'),)\\\n",
    "           .write.mode('append').save_as_table('flat_EVAL')\n",
    "    \n",
    "    return 'flat_PRED', 'flat_FORECAST', 'flat_EVAL'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
