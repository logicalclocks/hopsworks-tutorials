{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f991f3",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Pipeline</span>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/advanced_tutorials/bitcoin/3_bitcoin_training_pipeline.ipynb)\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This is the third part of advanced tutorials about Hopsworks Feature Store. This notebook explains how to read from a feature group, create training dataset within the feature store, train a model, register it in Hopsworks Model Registry and then use for batch predictions.</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections: \n",
    "\n",
    "1. Fetch Feature Groups.\n",
    "2. Define Transformation functions.\n",
    "3. Create Feature Views.\n",
    "4. Create Training Dataset with training, validation and test splits.\n",
    "5. Train the model.\n",
    "6. Register model in Hopsworks model registry.\n",
    "7. Online model deployment.\n",
    "\n",
    "![part2](../../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26879f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üìù Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U hopsworks --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e77a6-d4a5-44fc-b43d-a27e6025747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "import inspect \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfb4b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d58833",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to the Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a94824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea412aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_price_fg = fs.get_or_create_feature_group(\n",
    "    name='bitcoin_price',\n",
    "    version=1,\n",
    ")\n",
    "# btc_price_fg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_textblob_fg = fs.get_or_create_feature_group(\n",
    "    name='bitcoin_tweets_textblob',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9377661",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_vader_fg = fs.get_or_create_feature_group(\n",
    "    name='bitcoin_tweets_vader',\n",
    "    version=1,\n",
    ")\n",
    "# tweets_vader_fg.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177d12e",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> üñç Feature View Creation and Retrieving </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2966e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query Preparation\n",
    "query = btc_price_fg.select_except([\"date\"]) \\\n",
    "               .join(tweets_textblob_fg.select([\"subjectivity\",\"polarity\"])) \\\n",
    "               .join(tweets_vader_fg.select(\"compound\"))\n",
    "\n",
    "final_df = query.read()\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c11153-651f-4cc5-b876-71f41c3a9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa931945",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_transform = final_df.columns\n",
    "columns_to_transform = columns_to_transform.tolist()\n",
    "columns_to_transform.remove(\"unix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0a265-3101-4a16-abd1-be5942f21264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "\n",
    "# Map features to transformation functions.\n",
    "transformation_functions = {col: min_max_scaler for col in columns_to_transform}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ecda6-9dfb-44e7-8fa1-9851064814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='bitcoin_feature_view',\n",
    "    version=1,\n",
    "    transformation_functions=transformation_functions,\n",
    "    query=query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea634203-9eef-4b52-90d1-cf0c5c6c4de8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "To create training dataset you will use the `FeatureView.train_test_split()` method.\n",
    "\n",
    "Here are some importand things:\n",
    "\n",
    "- It will inherit the name of FeatureView.\n",
    "\n",
    "- The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "- You can choose necessary format using **data_format** parameter.\n",
    "\n",
    "- **start_time** and **end_time** in order to filter dataset in specific time range.\n",
    "\n",
    "- You can create **train, test** splits using `create_train_test_split()`. \n",
    "\n",
    "- You can create **train,validation, test** splits using `train_validation_test_splits()` methods.\n",
    "\n",
    "- The only thing is that we should specify desired ratio of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84394f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can combine different datetime formats.\n",
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
    "    train_start=int(final_df.unix.min()),\n",
    "    train_end=int(np.percentile(final_df.unix, 80)), # get the date that represents 80th percentile\n",
    "    test_start=int(np.percentile(final_df.unix, 81)), # get the date that represents 81th percentile\n",
    "    test_end=int(final_df.unix.max()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc385fe6-a333-4724-9ea0-f581a3cb203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26654b81-b599-4187-aa9e-955476e63ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove redundant column \"unix\"\n",
    "X_train.drop(columns=[\"unix\"], inplace=True)\n",
    "X_test.drop(columns=[\"unix\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb27c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[[\"close\"]]\n",
    "y_test = X_test[[\"close\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40620977",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\">ü§ñ Time series model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e922a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define Tensorflow Dataset as we are going to train keras tensorflow model\n",
    "\n",
    "def windowed_dataset(dataset, target, window_size, batch_size):\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    ds = ds.map(lambda window: tf.reshape(window[-1:], [-1, 33]))\n",
    "        \n",
    "    target_ds = target.window(window_size, shift=1, drop_remainder=True)\n",
    "    target_ds = target_ds.flat_map(lambda window: window.batch(window_size))\n",
    "    target_ds = target_ds.map(lambda window: window[-1:])\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((ds, target_ds))\n",
    "    ds = ds.batch(batch_size,True)\n",
    "    ds = ds.prefetch(1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18517f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(X_train.values, tf.float32)) \n",
    "training_target = tf.data.Dataset.from_tensor_slices(y_train.values.flatten().tolist()) \n",
    "training_dataset = training_dataset.repeat(500)\n",
    "training_dataset = windowed_dataset(training_dataset, training_target, window_size=2, batch_size=16)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(X_test.values, tf.float32))\n",
    "validation_target = tf.data.Dataset.from_tensor_slices(y_test.values.flatten().tolist()) \n",
    "training_dataset = training_dataset.repeat(500)\n",
    "test_dataset = windowed_dataset(test_dataset, validation_target, window_size=2, batch_size=16)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "    x = tf.keras.layers.Conv1D(filters = 128, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(33, activation=\"relu\", kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(1, activation=\"relu\", kernel_initializer=\"uniform\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    model.summary()\n",
    "    model.compile(loss='mse',optimizer='adam',metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01761fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model([1, X_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    steps_per_epoch=500,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=1,                    \n",
    ")\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e62a7f",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'>‚öñÔ∏è Model Validation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history_dict['mae']\n",
    "val_loss_values = history_dict['val_mae']\n",
    "\n",
    "loss_values50 = loss_values\n",
    "val_loss_values50 = val_loss_values\n",
    "epochs = range(1, len(loss_values50) + 1)\n",
    "plt.plot(epochs, loss_values50, 'b',color = 'blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values50, 'b',color='red', label='Validation loss')\n",
    "plt.rc('font', size = 18)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(epochs)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c11499",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = model.predict(X_test.values.reshape(-1, 1, X_test.shape[1]))\n",
    "y_pred_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "# Recall that you applied transformation functions, such as min max scaler and laber encoder. \n",
    "# Now you want to transform them back to human readable format.\n",
    "feature_view.init_serving(1)\n",
    "td_transformation_functions = feature_view._single_vector_server._transformation_functions\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred_scaled, columns=[\"close\"])\n",
    "\n",
    "for feature_name in td_transformation_functions:\n",
    "    if feature_name == \"close\":\n",
    "        td_transformation_function = td_transformation_functions[feature_name]\n",
    "        sig, foobar_locals = inspect.signature(td_transformation_function.transformation_fn), locals()\n",
    "        param_dict = dict([(param.name, param.default) for param in sig.parameters.values() if param.default != inspect._empty])\n",
    "        if td_transformation_function.name == \"min_max_scaler\":\n",
    "            y_pred[feature_name] = y_pred[feature_name].map(lambda x: x*(param_dict[\"max_value\"]-param_dict[\"min_value\"])+param_dict[\"min_value\"])\n",
    "            y_test[feature_name] = y_test[feature_name].map(lambda x: x*(param_dict[\"max_value\"]-param_dict[\"min_value\"])+param_dict[\"min_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c5432-19d5-4968-a102-328c600d3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['darkslategrey']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.plot(y_test, 'black')\n",
    "ax.plot(y_pred, 'orange')\n",
    "ax.set_ylabel('$price$')\n",
    "ax.set_xlabel('$time$')\n",
    "ax.grid(True)\n",
    "ax.legend([\"actual\", \"pred\"])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdebb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## <span style='color:#ff5f27'>üóÑ Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'bitcoin_price_model' directory will be saved to the model registry\n",
    "model_dir = \"bitcoin_price_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "fig.savefig(model_dir + \"/chart.png\") \n",
    "\n",
    "print('Exporting trained model to: {}'.format(model_dir))\n",
    "    \n",
    "tf.saved_model.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "metrics={'loss': history_dict['val_mae'][0]} \n",
    "\n",
    "today = datetime.today()\n",
    "latest_date = int(time.mktime(today.timetuple()) * 1000) # converting todays datetime to unix\n",
    "\n",
    "tf_model = mr.tensorflow.create_model(\n",
    "    name=\"bitcoin_price_model\", \n",
    "    metrics=metrics,\n",
    "    input_example=[latest_date],\n",
    "    description=\"Bitcoin daily price prediction model.\",\n",
    ")\n",
    "\n",
    "tf_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98bbb11",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üöÄ Model Deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63894db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile btc_model_transformer.py\n",
    "\n",
    "import os\n",
    "import hsfs\n",
    "import numpy as np\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):        \n",
    "        # get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # get feature views\n",
    "        self.fv = self.fs.get_feature_view(\"bitcoin_feature_view\", 1)\n",
    "        \n",
    "        # initialise serving\n",
    "        self.fv.init_serving(1)\n",
    "\n",
    "    def flat2gen(self, alist):\n",
    "        for item in alist:\n",
    "            if isinstance(item, list):\n",
    "                for subitem in item: yield subitem\n",
    "            else:\n",
    "                yield item\n",
    "        \n",
    "    def preprocess(self, inputs):\n",
    "        feature_vector = self.fv.get_feature_vector({\"unix\": inputs[\"instances\"][0][0]})\n",
    "        feature_vector = [*feature_vector[:9], *feature_vector[10:]]\n",
    "        return { \"instances\" :  np.array(list(self.flat2gen(feature_vector))).reshape(-1, 1, len(feature_vector)).tolist() }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4df257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"btc_model_transformer.py\", \"Resources\", overwrite=True)\n",
    "transformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n",
    "transformer_script = Transformer(script_file=transformer_script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611abd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can retrieve the model using code like this\n",
    "tf_model = mr.get_model(\"bitcoin_price_model\", version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = tf_model.deploy(\n",
    "    name=\"btcmodeldeployment\",\n",
    "    transformer=transformer_script\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546da30-d03c-45cd-b48b-0a63709f2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment is warming up...\")\n",
    "time.sleep(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91570d5",
   "metadata": {},
   "source": [
    "The deployment has now been registered. Lets retrieve it from Hopsworks for demonstration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = project.get_model_serving()\n",
    "\n",
    "# get deployment object\n",
    "deployment = ms.get_deployment(\"btcmodeldeployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c98b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ca8e",
   "metadata": {},
   "source": [
    "To start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.start(await_running=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1477bb",
   "metadata": {},
   "source": [
    "For trouble shooting one can use get_logs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bbab5",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üîÆ Predicting</span>\n",
    "\n",
    "Using the deployment let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today()\n",
    "latest_date = int(time.mktime(today.timetuple()) * 1000) # converting todays datetime to unix\n",
    "\n",
    "deployment_output = deployment.predict(inputs=[latest_date])\n",
    "# or deployment.predict({ \"instances\": [[latest_date]] })\n",
    "\n",
    "deployment_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95324a8-7772-45f2-94c0-bd771a934112",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_encoded = pd.DataFrame(\n",
    "    deployment_output[\"predictions\"],\n",
    "    columns=[\"close\"],\n",
    ") # since we applied transformation function to the 'close' columns,\n",
    "  # now we need to provide a df with the same column name to decode.\n",
    "                                                \n",
    "pred = decode_features(pred_encoded, feature_view=feature_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43c0b0-cc0c-4013-959a-9340222902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.rename(columns={\"close\": \"predicted_price\"})\n",
    "pred[\"datetime\"] = [today.strftime(\"%Y-%m-%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5e3ab-d096-4ce1-9c2c-f0749c7280cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08889a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For trouble shooting one you can use get_logs method.\n",
    "# deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe78327",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load Batch Data of last days</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "batch_data = feature_view.get_batch_data().drop('unix',axis=1)\n",
    "batch_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1dafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mr.get_model(\"bitcoin_price_model\", version=1)\n",
    "model_dir = model.download()\n",
    "\n",
    "loaded_model = tf.saved_model.load(model_dir)\n",
    "serving_function = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da030f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_batch = serving_function(\n",
    "    tf.constant(\n",
    "        batch_data.values.reshape(\n",
    "            -1, \n",
    "            batch_data.shape[0], \n",
    "            batch_data.shape[1]), \n",
    "        tf.float32\n",
    "    )\n",
    ")['dense_1'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcbcb9",
   "metadata": {},
   "source": [
    "Recall that you applied transformation functions, such as min max scaler and laber encoder. \n",
    "\n",
    "Now you want to transform them back to human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93cf7b-35f1-4cc9-937d-7ef0078dce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch_encoded = pd.DataFrame(\n",
    "    predictions_batch,\n",
    "    columns=[\"close\"]\n",
    ") # since we applied transformation function to the 'close' columns,\n",
    "  # now we need to provide a df with the same column name to decode.\n",
    "                                                \n",
    "pred_batch = decode_features(pred_batch_encoded, feature_view=feature_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e3224-a929-4508-a9cf-f5233f205874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af816335",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 04: Batch Inference</span>\n",
    "\n",
    "In the following notebook you will use your model for Batch Inference.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/bitcoin/4_bitcoin_batch_inference.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3502d2fd240c5a16c1ad36292676af1ecc0e366e100e74e6e336a5116ced841"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
