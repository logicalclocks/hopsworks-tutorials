{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa514fb7",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This notebook creates a data stream using Hopsworks Internal Kafka</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Creating Simulated Data\n",
    "2. Creating Kafka Topic and Schema in Hopsworks Feature Store\n",
    "3. Sending Data to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f5c11",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df72129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbeaa9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from synthetic_data import synthetic_data\n",
    "from confluent_kafka import Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b2938",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚úèÔ∏è Creating Simulated Data </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef161e6-3e87-4bd0-a190-c37fd1a3e444",
   "metadata": {},
   "source": [
    "A simulated dataset for credit card Transactions is created so that the data can be send using a Kafka stream. The data created is split into two different dataframes:\n",
    "\n",
    "* profiles_df: credit card user information such as birthdate and city of residence, along with credict card information such as the expiration date and provider.\n",
    "* trans_df: events containing information about when a credit card was used, such as a timestamp, location, and the amount spent. A boolean fraud_label variable (True/False) tells us whether a transaction was fraudulent or not.\n",
    "\n",
    "In a production system, these data would originate from separate data sources or tables, and probably separate data pipelines. Both files have a common credit card number column cc_num, which you will use later to join features together from the different datasets.\n",
    "\n",
    "Now you can go ahead and create the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40038992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simulater = synthetic_data.synthetic_data()\n",
    "\n",
    "profiles_df, trans_df = data_simulater.create_simulated_transactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c852a2",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd189e-2424-4462-9f5f-a634950be10a",
   "metadata": {},
   "source": [
    "After creating the simulated data let us connect with Hopsworks Feature Store.\n",
    "\n",
    "Hopsworks provides an internal Kafka which can be accessed using the KafkaAPI. See [documentation](https://docs.hopsworks.ai/3.7/user_guides/projects/kafka/create_schema/#introduction) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7aff338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "kafka_api = project.get_kafka_api()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e69f9f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Creating Feature Groups </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd5944-dea4-4a31-bbea-87aafbef0bc2",
   "metadata": {},
   "source": [
    "Profiles data can be directly inserted as a feature group directly since they are not update fequently.\n",
    "\n",
    "To create a feature group you need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group and a version number, if it is not defined it will automatically be incremented to `1`.\n",
    "\n",
    "A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2838cb7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m profile_fg \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241m.\u001b[39mget_or_create_feature_group(\n\u001b[1;32m      2\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         primary_key\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc_num\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m         partition_key\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m         version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m profile_df \u001b[38;5;241m=\u001b[39m profile_fg\u001b[38;5;241m.\u001b[39minsert(profiles_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "profile_fg = fs.get_or_create_feature_group(\n",
    "        name=\"profile\",\n",
    "        primary_key=[\"cc_num\"],\n",
    "        partition_key=[\"cc_provider\"],\n",
    "        version=1)\n",
    "\n",
    "profile_df = profile_fg.insert(profiles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e85615",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Kafka Topic and Schema Creation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d973e90-66fd-47c1-9b11-f9aaebdf0caf",
   "metadata": {},
   "source": [
    "To create a Kafka stream for transactions a topic and schema must be create. The schema used must follow Apache Avro specification, more details can be found in the [documentation](https://avro.apache.org/docs/1.11.1/specification/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba10159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kafka topic\n",
    "KAFKA_TOPIC_NAME = \"transactions_topic\"\n",
    "SCHEMA_NAME = \"transactions_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39dfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": SCHEMA_NAME,\n",
    "    \"namespace\": \"io.hops.examples.pyspark.example\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"tid\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"string\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"datetime\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                {\n",
    "                    \"type\": \"long\",\n",
    "                    \"logicalType\": \"timestamp-micros\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"cc_num\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"long\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"category\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"string\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"amount\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"double\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"latitude\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"double\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"longitude\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"double\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"city\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"string\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"country\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"string\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"fraud_label\",\n",
    "            \"type\": [\n",
    "                \"null\",\n",
    "                \"string\"\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6d27f-72f3-4142-ae4a-8a28803b5490",
   "metadata": {},
   "source": [
    "After the schema is created the topic and the associated schema must be registered in Hopsworks so that the topic can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ca36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KAFKA_TOPIC_NAME not in [topic.name for topic in kafka_api.get_topics()]:\n",
    "    kafka_api.create_schema(SCHEMA_NAME, schema)\n",
    "    kafka_api.create_topic(KAFKA_TOPIC_NAME, SCHEMA_NAME, 1, replicas=1, partitions=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408f5f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Sending Data using created Kafka Topic </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5bb3-1fcd-4136-8c02-d5003aa18b13",
   "metadata": {},
   "source": [
    "While sending data through Kafka we must make sure that the data types are in the same format specified in the schema. \n",
    "\n",
    "Let's make sure that the dataframe has all the components in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff97db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df[\"tid\"] = trans_df[\"tid\"].astype(\"string\")\n",
    "trans_df[\"datetime\"] = pd.to_datetime(trans_df[\"datetime\"])\n",
    "trans_df[\"cc_num\"] = trans_df[\"cc_num\"].astype(\"int64\")\n",
    "trans_df[\"category\"] = trans_df[\"cc_num\"].astype(\"string\")\n",
    "trans_df[\"amount\"] = trans_df[\"amount\"].astype(\"double\")\n",
    "trans_df[\"latitude\"] = trans_df[\"latitude\"].astype(\"double\")\n",
    "trans_df[\"longitude\"] = trans_df[\"longitude\"].astype(\"double\")\n",
    "trans_df[\"city\"] = trans_df[\"city\"].astype(\"string\")\n",
    "trans_df[\"country\"] = trans_df[\"country\"].astype(\"string\")\n",
    "trans_df[\"fraud_label\"] = trans_df[\"fraud_label\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c49b5-cfd6-451b-bdf7-a8130c261b5e",
   "metadata": {},
   "source": [
    "Lets get the configuration needed for the producer to used Hopsworks internal kafka using the KafkaAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f58727-2ca3-4c23-9c5f-a86fa934d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_config = kafka_api.get_default_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ea6ba-677d-4155-aea9-6d1be073086a",
   "metadata": {},
   "source": [
    "Finally, lets create a producer using the Kafka configuration and send data into it.\n",
    "\n",
    "It is important to note that the data passed to the producer must be a json or it must be avro encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70cf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = Producer(kafka_config)\n",
    "\n",
    "for index, transaction in trans_df.iterrows():\n",
    "    producer.produce(KAFKA_TOPIC_NAME, transaction.to_json())\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb76a7b",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 01: Feature Pipeline</span>\n",
    "\n",
    "In the following notebook you will use the created Kafka stream to insert data into a Feature Group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
