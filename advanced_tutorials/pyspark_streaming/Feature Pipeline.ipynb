{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49874d9",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Feature Pipeline</span>\n",
    "\n",
    "**Note**: This tutorial does not support Google Colab.\n",
    "\n",
    "This is the first part of the quick start series of tutorials about Hopsworks Feature Store. As part of this first module, you will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store**  for streaming data with a goal of training and saving a model that can predict fraudulent transactions. Then try it on retrieved from Feature Store batch data.\n",
    "\n",
    "\n",
    "## üóíÔ∏è This notebook is divided in 3 sections:\n",
    "1. Loading the data and feature engineeing,\n",
    "2. Connect to the Hopsworks Feature Store,\n",
    "3. Create feature groups and upload them to the Feature Store.\n",
    "\n",
    "![tutorial-flow](../../images/01_featuregroups.png)\n",
    "\n",
    "First of all you will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f58f72",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab11319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>8</td><td>application_1710149630761_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-16-4-245.us-east-2.compute.internal:8089/proxy/application_1710149630761_0019/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-16-4-74.us-east-2.compute.internal:8044/node/containerlogs/container_e08_1710149630761_0019_01_000001/pyspark__manujose\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fa848",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc70118-2fcf-4978-9821-3348fcef6873",
   "metadata": {},
   "source": [
    "In this tutorial a simulated data stream was created using Hopsworks internal Kafka.\n",
    "\n",
    "Hopsworks allows to access internal Kafka using the storage connector api. See more information in the [documention](https://docs.hopsworks.ai/feature-store-api/3.7/generated/api/storage_connector_api/#kafka)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5513a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "from hsfs.core.storage_connector_api import StorageConnectorApi\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "sc_api = StorageConnectorApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faab2a4-6ec6-49c5-8a4d-d6494f7191a9",
   "metadata": {},
   "source": [
    "Let get the kafka configurations needed for read from hopsworks internal Kafka using the storage connector api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5728b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_api = StorageConnectorApi()\n",
    "kafka_connector = sc_api.get_kafka_connector(feature_store_id=fs.id, external=False)\n",
    "kafka_config = kafka_connector.spark_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf43aa8",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üóÇ Reading from Kakfa Stream </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae009c-69e6-4197-b5ac-121afd59e52b",
   "metadata": {},
   "source": [
    "After obatining the Kafka configurations we can use it along with the topic name to create a streaming dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e018f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_TOPIC_NAME = \"transactions_topic\"\n",
    "\n",
    "df_read = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_config) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 100) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC_NAME) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf31aaa-5d50-4696-a440-647a3d971910",
   "metadata": {},
   "source": [
    "To extract the requierd data from streaming dataframe the correct schema has be defined and used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21bb5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_schema = StructType([StructField(\"tid\", StringType(), True),\n",
    "                           StructField(\"datetime\", TimestampType(), True),\n",
    "                           StructField(\"cc_num\", LongType(), True),\n",
    "                           StructField(\"category\", StringType(), True),\n",
    "                           StructField(\"amount\", DoubleType(), True),\n",
    "                           StructField(\"latitude\", DoubleType(), True),\n",
    "                           StructField(\"longitude\", DoubleType(), True),\n",
    "                           StructField(\"city\", StringType(), True),\n",
    "                           StructField(\"country\", StringType(), True),\n",
    "                           StructField(\"fraud_label\", StringType(), True),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3eb48e-f427-4b76-95d8-c310e268998d",
   "metadata": {},
   "source": [
    "Extracting data from the streaming dataframe and casting it to get the required schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc025cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize data from and create streaming query\n",
    "streaming_df = df_read.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", parse_schema).alias(\"value\")) \\\n",
    "    .select(\"value.tid\",\n",
    "            \"value.datetime\",\n",
    "            \"value.cc_num\",\n",
    "            \"value.category\",\n",
    "            \"value.amount\",\n",
    "            \"value.latitude\",\n",
    "            \"value.longitude\",\n",
    "            \"value.city\",\n",
    "            \"value.country\",\n",
    "            \"value.fraud_label\") \\\n",
    "    .selectExpr(\"CAST(tid as string)\",\n",
    "                \"CAST(datetime as timestamp)\",\n",
    "                \"CAST(cc_num as long)\",\n",
    "                \"CAST(category as string)\",\n",
    "                \"CAST(amount as double)\",\n",
    "                \"CAST(latitude as double)\",\n",
    "                \"CAST(longitude as double)\",\n",
    "                \"CAST(city as string)\",\n",
    "                \"CAST(country as string)\",\n",
    "                \"CAST(fraud_label as string)\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1022b9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e0fea-35f3-4aa6-8807-224d827d38d8",
   "metadata": {},
   "source": [
    "Now that we have a streaming dataframe that contains the data we can use it to engineer features. We would need the also need profiles to effectively engineer features.\n",
    "\n",
    "So next you can read data from profiles feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e44035c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_fg = fs.get_or_create_feature_group(\n",
    "    name=\"profile\",\n",
    "    version=1)\n",
    "\n",
    "profile_df = profile_fg.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d352f12-06e6-46d7-b29b-48d0f8dc6aa9",
   "metadata": {},
   "source": [
    "Fraudulent transactions can differ from regular ones in many different ways. Typical red flags would for instance be a large transaction volume/frequency in the span of a few hours. It could also be the case that elderly people in particular are targeted by fraudsters. To facilitate model learning you will create additional features based on these patterns. In particular, you will create two types of features:\n",
    "1. **Features that aggregate data from different data sources**. This could for instance be the age of a customer at the time of a transaction, which combines the `birthdate` feature from `profiles` with the `datetime` feature from `transactions`.\n",
    "2. **Features that aggregate data from multiple time steps**. An example of this could be the transaction frequency of a credit card in the span of a few hours, which is computed using a window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c834a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_streaming_df = (\n",
    "    streaming_df.join(profile_df.drop(\"city\"), on=\"cc_num\", how=\"left\")\n",
    "        .withColumn(\"cc_expiration_date\", to_timestamp(\"cc_expiration_date\", \"mm/dd\"))\n",
    "        .withColumn(\"age_at_transaction\", datediff(col(\"datetime\"),col(\"birthdate\")))\n",
    "        .withColumn(\"days_until_card_expires\", datediff(col(\"datetime\"),col(\"cc_expiration_date\")))\n",
    "        .select([\"tid\", \"datetime\", \"cc_num\", \"category\", \"amount\", \"latitude\", \"longitude\", \"city\", \"country\", \"fraud_label\", \"age_at_transaction\", \"days_until_card_expires\", \"cc_expiration_date\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e38b0-9c86-4a66-acec-6a56cda2073c",
   "metadata": {},
   "source": [
    "Next, you will create features that aggregate credit card data over a period of time.\n",
    "\n",
    "Here for simplicity we take the average, standard deviation and frequency of transaction amount over a period of 4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3745b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_4h_aggregation_df = (\n",
    "    streaming_df.withWatermark(\"datetime\", \"168 hours\")\n",
    "        .groupBy(window(\"datetime\", \"4 hours\", \"1 hour\"), \"cc_num\")\n",
    "        .agg(\n",
    "            avg(\"amount\").alias(\"avg_amt_per_4h\"),\n",
    "            stddev(\"amount\").alias(\"stdev_amt_per_4h\"),\n",
    "            count(\"cc_num\").alias(\"num_trans_per_4h\"),\n",
    "        )\n",
    "        .na.fill({\"stdev_amt_per_4h\":0})\n",
    "        .selectExpr(\n",
    "            \"cc_num\",\n",
    "            \"current_timestamp() as event_time\",\n",
    "            \"num_trans_per_4h\",\n",
    "            \"avg_amt_per_4h\",\n",
    "            \"stdev_amt_per_4h\",\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8cfae",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üëÆüèª‚Äç‚ôÇÔ∏è Great Expectations </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe44d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "\n",
    "# Set the expectation suite name to \"transactions_suite\"\n",
    "expectation_suite_transactions = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"transactions_suite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f234cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"kwargs\": {\"column\": \"tid\", \"mostly\": 0.0}, \"expectation_type\": \"expect_column_values_to_be_null\", \"meta\": {}}\n",
      "{\"kwargs\": {\"column\": \"datetime\", \"mostly\": 0.0}, \"expectation_type\": \"expect_column_values_to_be_null\", \"meta\": {}}\n",
      "{\"kwargs\": {\"column\": \"cc_num\", \"mostly\": 0.0}, \"expectation_type\": \"expect_column_values_to_be_null\", \"meta\": {}}"
     ]
    }
   ],
   "source": [
    "# Check binary fraud_label column to be in set [0,1]\n",
    "expectation_suite_transactions.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_distinct_values_to_be_in_set\",\n",
    "        kwargs={\n",
    "            \"column\": \"fraud_label\",\n",
    "            \"value_set\": [0, 1],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check amount column to be not negative\n",
    "expectation_suite_transactions.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"amount\",\n",
    "            \"min_value\": 0.0,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Loop through specified columns ('tid', 'datetime', 'cc_num') and add expectations for null values\n",
    "for column in ['tid', 'datetime', 'cc_num']:\n",
    "    expectation_suite_transactions.add_expectation(\n",
    "        ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_null\",\n",
    "            kwargs={\n",
    "                \"column\": column,\n",
    "                \"mostly\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3adf53",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíæ Storing streaming dataframes in Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8dee9a-de08-4ab7-a9ae-2cd667d0cff7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü™Ñ Creating Feature Groups </span>\n",
    "\n",
    "A [feature group](https://docs.hopsworks.ai/3.0/concepts/fs/feature_group/fg_overview/) can be seen as a collection of conceptually related features. In this case, you will create a feature group for the transaction data and a feature group for the windowed aggregations on the transaction data. Both will have `cc_num` as primary key, which will allow you to join them when creating a dataset in the next tutorial.\n",
    "\n",
    "Feature groups can also be used to define a namespace for features. For instance, in a real-life setting you would likely want to experiment with different window lengths. In that case, you can create feature groups with identical schema for each window length. \n",
    "\n",
    "Before you can create a feature group you need to connect to Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ceb25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_or_create_feature_group(\n",
    "    name=\"transactions_fraud_streaming_fg\",\n",
    "    version=1,\n",
    "    description=\"Transaction data\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    event_time=\"datetime\",\n",
    "    online_enabled=True,\n",
    "    stream=True,\n",
    "    expectation_suite=expectation_suite_transactions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef421264-6956-4226-87d5-4e7b1c5dd0b6",
   "metadata": {},
   "source": [
    "A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group).\n",
    "\n",
    "At this point, you have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To insert a streaming dataframe into a feature group you can use the streaming feature `insert_stream` function. You can find more details in the [documentation](https://docs.hopsworks.ai/feature-store-api/3.7/generated/api/feature_group_api/#insert_stream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b370459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192/fs/3140/fg/3118\n",
      "StatisticsWarning: Stream ingestion for feature group `transactions_fraud_streaming_fg`, with version `1` will not compute statistics."
     ]
    }
   ],
   "source": [
    "# Insert data into feature group\n",
    "trans_fg_query = trans_fg.insert_stream(transaction_streaming_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d564f6-c8f6-48ea-aca2-0ae22bd0bcd1",
   "metadata": {},
   "source": [
    "\n",
    "The `insert_stream` function returns a `StreamingQuery` object which be used to check the status of the streaming query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b86e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}"
     ]
    }
   ],
   "source": [
    "trans_fg_query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f288ea-f0ad-48a0-901e-de06cbea5617",
   "metadata": {},
   "source": [
    "The `insert_stream` function inserts the data into the online feature store so to materialize the data in the offline store you need to manually run the materialization job. The materialization job can also be run on schedule using the `schedule` function. You can find more details in the [documentation](https://docs.hopsworks.ai/hopsworks-api/3.7/generated/api/jobs/#schedule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de766b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: transactions_fraud_streaming_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192/jobs/named/transactions_fraud_streaming_fg_1_offline_fg_materialization/executions"
     ]
    }
   ],
   "source": [
    "trans_fg.materialization_job.schedule(cron_expression = \"0 10 * ? * * *\", start_time=datetime.datetime.now(tz=timezone.utc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2c385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>\n",
      "<hsfs.feature_group.FeatureGroup object at 0x7f1b871bec80>"
     ]
    }
   ],
   "source": [
    "# Update feature descriptions\n",
    "feature_descriptions = [\n",
    "    {\"name\": \"tid\", \"description\": \"Transaction id\"},\n",
    "    {\"name\": \"datetime\", \"description\": \"Transaction time\"},\n",
    "    {\"name\": \"cc_num\", \"description\": \"Number of the credit card performing the transaction\"},\n",
    "    {\"name\": \"category\", \"description\": \"Expense category\"},\n",
    "    {\"name\": \"amount\", \"description\": \"Dollar amount of the transaction\"},\n",
    "    {\"name\": \"latitude\", \"description\": \"Transaction location latitude\"},\n",
    "    {\"name\": \"longitude\", \"description\": \"Transaction location longitude\"},\n",
    "    {\"name\": \"city\", \"description\": \"City in which the transaction was made\"},\n",
    "    {\"name\": \"country\", \"description\": \"Country in which the transaction was made\"},\n",
    "    {\"name\": \"fraud_label\", \"description\": \"Whether the transaction was fraudulent or not\"},\n",
    "    {\"name\": \"age_at_transaction\", \"description\": \"Age of the card holder when the transaction was made\"},\n",
    "    {\"name\": \"days_until_card_expires\", \"description\": \"Card validity days left when the transaction was made\"},\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    trans_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3832bb4-7786-4893-ad50-8aa37e29d278",
   "metadata": {},
   "source": [
    "At the creation of the feature group, you will be prompted with an URL that will directly link to it; there you will be able to explore some of the aspects of your newly created feature group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103a4fc-c298-4760-98d3-953ad3f7e7cb",
   "metadata": {},
   "source": [
    "You can move on and do the same thing for the feature group with our windows aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f56bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create the 'transactions' feature group with specified window aggregations\n",
    "window_aggs_streaming_fg = fs.get_or_create_feature_group(\n",
    "    name=f\"transactions_aggs_fraud_streaming_fg\",\n",
    "    version=1,\n",
    "    description=f\"Aggregate transaction data over 5 minute windows.\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    event_time=\"event_time\",\n",
    "    online_enabled=True,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c054288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192/fs/3140/fg/3119\n",
      "StatisticsWarning: Stream ingestion for feature group `transactions_aggs_fraud_streaming_fg`, with version `1` will not compute statistics."
     ]
    }
   ],
   "source": [
    "window_aggs_streaming_fg_query = window_aggs_streaming_fg.insert_stream(windowed_4h_aggregation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c6e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Getting offsets from KafkaV2[Subscribe[transactions_topic]]', 'isDataAvailable': False, 'isTriggerActive': True}"
     ]
    }
   ],
   "source": [
    "window_aggs_streaming_fg_query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa973de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "The Hopsworks Job failed, use the Hopsworks UI to access the job logs\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/job.py\", line 124, in run\n",
      "    self._wait_for_job(await_termination=await_termination)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/job.py\", line 242, in _wait_for_job\n",
      "    raise FeatureStoreException(\n",
      "hsfs.client.exceptions.FeatureStoreException: The Hopsworks Job failed, use the Hopsworks UI to access the job logs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_aggs_streaming_fg.materialization_job.schedule(cron_expression = \"0 10 * ? * * *\", start_time=datetime.datetime.now(tz=timezone.utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70361d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 02: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will use your feature groups to create a dataset you can train a model on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
