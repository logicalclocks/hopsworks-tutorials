{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e21e38c",
   "metadata": {},
   "source": [
    "## Streamlit Application\n",
    "In this section of the hands-on-lab, we will utilize Streamlit with Snowpark's Python client-side Dataframe API to create a visual front-end application for the Citibike operations team to consume the insights from the ML forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66099ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile include/streamlit_app.py\n",
    "import sys, os\n",
    "sys.path.append(os.getcwd()+'/dags')\n",
    "\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowpark_connection import snowpark_connect\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil.relativedelta import *\n",
    "import calendar\n",
    "import altair as alt\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logging.getLogger().setLevel(logging.WARN)\n",
    "\n",
    "def update_forecast_table(forecast_df, stations:list, start_date, end_date):\n",
    "#     explainer_columns = [col for col in forecast_df.schema.names if 'EXP' in col]\n",
    "    explainer_columns=['EXPL_LAG_1', 'EXPL_LAG_7','EXPL_LAG_90','EXPL_LAG_365','EXPL_HOLIDAY','EXPL_PRECIP','EXPL_TEMP']\n",
    "    explainer_columns_new=['DAY', 'DAY_OF_WEEK', 'QUARTER', 'DAY_OF_YEAR','US_HOLIDAY', 'PRECIPITATION','TEMPERATURE']\n",
    "\n",
    "    cond = \"F.when\" + \".when\".join([\"(F.col('\" + c + \"') == F.col('EXPLAIN'), F.lit('\" + c + \"'))\" for c in explainer_columns])\n",
    "\n",
    "    df = forecast_df.filter((forecast_df['STATION_ID'].in_(stations)) &\n",
    "                       (F.col('DATE') >= start_date) & \n",
    "                       (F.col('DATE') <= end_date))\\\n",
    "                .select(['STATION_ID', \n",
    "                         F.to_char(F.col('DATE')).alias('DATE'), \n",
    "                         'PRED', \n",
    "                         'HOLIDAY',\n",
    "                         *explainer_columns])\\\n",
    "                .with_column('EXPLAIN', F.greatest(*explainer_columns))\\\n",
    "                .with_column('REASON', eval(cond))\\\n",
    "                .select(F.col('STATION_ID'), \n",
    "                        F.col('DATE'), \n",
    "                        F.col('PRED'), \n",
    "                        F.col('REASON'), \n",
    "                        F.col('EXPLAIN'), \n",
    "                        F.col('EXPL_LAG_1').alias('DAY'),\n",
    "                        F.col('EXPL_LAG_7').alias('DAY_OF_WEEK'),\n",
    "                        F.col('EXPL_LAG_90').alias('QUARTER'),\n",
    "                        F.col('EXPL_LAG_365').alias('DAY_OF_YEAR'),\n",
    "                        F.col('EXPL_HOLIDAY').alias('US_HOLIDAY'),\n",
    "                        F.col('EXPL_PRECIP').alias('PRECIPITATION'),\n",
    "                        F.col('EXPL_TEMP').alias('TEMPERATURE'),\n",
    "                       )\\\n",
    "                .to_pandas()\n",
    "    \n",
    "    df['REASON'] = pd.Categorical(df['REASON'])\n",
    "    df['REASON_CODE']=df['REASON'].cat.codes\n",
    "        \n",
    "    rect = alt.Chart(df).mark_rect().encode(alt.X('DATE:N'), \n",
    "                                        alt.Y('STATION_ID:N'), \n",
    "                                        alt.Color('REASON'),\n",
    "                                        tooltip=explainer_columns_new)\n",
    "    text = rect.mark_text(baseline='middle').encode(text='PRED:Q', color=alt.value('white'))\n",
    "\n",
    "    l = alt.layer(\n",
    "        rect, text\n",
    "    )\n",
    "\n",
    "    st.write(\"### Forecast\")\n",
    "    st.altair_chart(l, use_container_width=True)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def update_eval_table(eval_df, stations:list):\n",
    "    df = eval_df.select('STATION_ID', F.to_char(F.col('RUN_DATE')).alias('RUN_DATE'), 'RMSE')\\\n",
    "                .filter(eval_df['STATION_ID'].in_(stations))\\\n",
    "                .to_pandas()\n",
    "\n",
    "    data = df.pivot(index=\"RUN_DATE\", columns=\"STATION_ID\", values=\"RMSE\")\n",
    "    data = data.reset_index().melt('RUN_DATE', var_name='STATION_ID', value_name='RMSE')\n",
    "\n",
    "    nearest = alt.selection(type='single', nearest=True, on='mouseover',\n",
    "                            fields=['RUN_DATE'], empty='none')\n",
    "\n",
    "    line = alt.Chart(data).mark_line(interpolate='basis').encode(\n",
    "        x='RUN_DATE:N',\n",
    "        y='RMSE:Q',\n",
    "        color='STATION_ID:N'\n",
    "    )\n",
    "\n",
    "    selectors = alt.Chart(data).mark_point().encode(\n",
    "        x='RUN_DATE:N',\n",
    "        opacity=alt.value(0)\n",
    "    ).add_selection(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    points = line.mark_point().encode(\n",
    "        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "    )\n",
    "\n",
    "    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "        text=alt.condition(nearest, 'RMSE:Q', alt.value(' '))\n",
    "    )\n",
    "\n",
    "    rules = alt.Chart(data).mark_rule(color='gray').encode(\n",
    "        x='RUN_DATE:N',\n",
    "    ).transform_filter(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    l = alt.layer(\n",
    "        line, selectors, points, rules, text\n",
    "    ).properties(\n",
    "        width=600, height=300\n",
    "    )\n",
    "    st.write(\"### Model Monitor\")\n",
    "    st.altair_chart(l, use_container_width=True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def trigger_ingest(download_file_name, run_date):    \n",
    "    dag_url='http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns'\n",
    "    json_payload = {\"conf\": {\"files_to_download\": [download_file_name], \"run_date\": run_date}}\n",
    "    \n",
    "    response = requests.post(dag_url, \n",
    "                            json=json_payload,\n",
    "                            auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "    run_id = json.loads(response.text)['dag_run_id']\n",
    "    #run_id = 'manual__2022-04-07T15:02:29.166108+00:00'\n",
    "\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "    st.snow()\n",
    "\n",
    "    with st.spinner('Ingesting file: '+download_file_name):\n",
    "        while state != 'success':\n",
    "            time.sleep(5)\n",
    "            state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "    st.success('Ingested file: '+download_file_name+' State: '+str(state))\n",
    "\n",
    "#Main Body    \n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "forecast_df = session.table('FLAT_FORECAST')\n",
    "eval_df = session.table('FLAT_EVAL')\n",
    "trips_df = session.table('TRIPS')\n",
    "\n",
    "st.header('Citibike Forecast Application')\n",
    "st.write('In this application we leverage deep learning models to predict the number of trips started from '+\n",
    "         'a given station each day.  After selecting the stations and time range desired the application '+\\\n",
    "         'displays not only the forecast but also explains which features of the model were most used in making '+\\\n",
    "         'the prediction. Additionally users can see the historical performance of the deep learning model to '+\\\n",
    "         'monitor predictive capabilities over time.')\n",
    "\n",
    "last_trip_date = trips_df.select(F.to_date(F.max('STARTTIME'))).collect()[0][0]\n",
    "st.write('Data provided as of '+str(last_trip_date))\n",
    "\n",
    "#Create a sidebar for input\n",
    "min_date=forecast_df.select(F.min('DATE')).collect()[0][0]\n",
    "max_date=forecast_df.select(F.max('DATE')).collect()[0][0]\n",
    "\n",
    "start_date = st.sidebar.date_input('Start Date', value=min_date, min_value=min_date, max_value=max_date)\n",
    "show_days = st.sidebar.number_input('Number of days to show', value=7, min_value=1, max_value=30)\n",
    "end_date = start_date+timedelta(days=show_days)\n",
    "\n",
    "stations_df=forecast_df.select(F.col('STATION_ID')).distinct().to_pandas()\n",
    "\n",
    "sample_stations = [\"519\", \"497\", \"435\", \"402\", \"426\", \"285\", \"293\"]\n",
    "\n",
    "stations = st.sidebar.multiselect('Choose stations', stations_df['STATION_ID'], sample_stations)\n",
    "if not stations:\n",
    "    stations = stations_df['STATION_ID']\n",
    "\n",
    "update_forecast_table(forecast_df, stations, start_date, end_date)\n",
    "\n",
    "update_eval_table(eval_df, stations)\n",
    "\n",
    "\n",
    "next_ingest = last_trip_date+relativedelta(months=+1)\n",
    "next_ingest = next_ingest.replace(day=1)       \n",
    "\n",
    "if next_ingest <= datetime.strptime(\"2016-12-01\", \"%Y-%m-%d\").date():\n",
    "    download_file_name=next_ingest.strftime('%Y%m')+'-citibike-tripdata.zip'\n",
    "else:\n",
    "    download_file_name=next_ingest.strftime('%Y%m')+'-citibike-tripdata.zip'\n",
    "    \n",
    "run_date = next_ingest+relativedelta(months=+1)\n",
    "run_date = run_date.strftime('%Y_%m_%d')\n",
    "\n",
    "st.write('Next ingest for '+str(next_ingest))\n",
    "\n",
    "st.button('Run Ingest Taskflow', on_click=trigger_ingest, args=(download_file_name, run_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a1566",
   "metadata": {},
   "source": [
    "If running in SageMaker Studio Lab update the domain name from the URL in your browser. \n",
    "For example if the Studio Lab URL is ht<span>tps://**yyy9xxxxxxxxxxx**.studio.us-east-2.sagemaker.aws/studiolab/default/jupyter/lab </span>\n",
    "the domain name is **yyy9xxxxxxxxxxx**. ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "studiolab_domain = ''\n",
    "\n",
    "# launch\n",
    "if studiolab_domain:\n",
    "    studiolab_region = 'us-east-2'\n",
    "    url = f'https://{studiolab_domain}.studio.{studiolab_region}.sagemaker.aws/studiolab/default/jupyter/proxy/6006/'\n",
    "    \n",
    "else: \n",
    "    \n",
    "    url = f'http://127.0.0.1:6006'\n",
    "\n",
    "print(f'Wait a few seconds and then click the link below to open your Streamlit application \\n{url}\\n')\n",
    "\n",
    "!streamlit run --theme.base dark include/streamlit_app.py --server.port 6006 \\\n",
    "                                                          --server.address 127.0.0.1 \\\n",
    "                                                          --server.headless true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc188650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
