{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5029789b",
   "metadata": {},
   "source": [
    "![hopsworks_logo](../../images/hopsworks_logo.png)\n",
    "\n",
    "# Part 03: Model training & UI Exploration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/fraud_online/3_model_training.ipynb)\n",
    "\n",
    "In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage.\n",
    "\n",
    "## üóíÔ∏è This notebook is divided in 5 main sections:\n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Register model to Hopsworks model registry**.\n",
    "4. **Deploy the model on KServe behind Hopsworks for real-time inference requests**.\n",
    "5. **Test model deployment and use model serving rest APIs**.\n",
    "\n",
    "![tutorial-flow](../../images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166f52f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a6d7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0d57b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚¨áÔ∏è Training Dataset retrieval</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985aa4b",
   "metadata": {},
   "source": [
    "To retrieve training dataset from **Feature Store** we retrieve **Feature View** using `FeatureStore.get_feature_view` method.\n",
    "\n",
    "Then we can use **Feature View** in order to retrieve **training dataset** using `FeatureView.get_training_dataset` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47e3c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/164/featurestores/106/featureview/nyc_taxi_fares/version/1). Server response: \nHTTP code: 404, HTTP reason: Not Found, error code: 270181, error msg: Feature view wasn't found., user msg: There exists no feature view with the name nyc_taxi_fares and version 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nyc_fares_fv \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_view\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnyc_taxi_fares\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\hopsworks\\venv38\\lib\\site-packages\\hsfs\\feature_store.py:975\u001b[0m, in \u001b[0;36mFeatureStore.get_feature_view\u001b[1;34m(self, name, version)\u001b[0m\n\u001b[0;32m    968\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo version provided for getting feature view `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`, defaulting to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    970\u001b[0m             name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEFAULT_VERSION\n\u001b[0;32m    971\u001b[0m         ),\n\u001b[0;32m    972\u001b[0m         util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[0;32m    973\u001b[0m     )\n\u001b[0;32m    974\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEFAULT_VERSION\n\u001b[1;32m--> 975\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\hopsworks\\venv38\\lib\\site-packages\\hsfs\\core\\feature_view_engine.py:80\u001b[0m, in \u001b[0;36mFeatureViewEngine.get\u001b[1;34m(self, name, version)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version:\n\u001b[1;32m---> 80\u001b[0m         fv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_by_name_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m         fv\u001b[38;5;241m.\u001b[39mtransformation_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attached_transformation_fn(\n\u001b[0;32m     82\u001b[0m             fv\u001b[38;5;241m.\u001b[39mname, fv\u001b[38;5;241m.\u001b[39mversion\n\u001b[0;32m     83\u001b[0m         )\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\hopsworks\\venv38\\lib\\site-packages\\hsfs\\core\\feature_view_api.py:74\u001b[0m, in \u001b[0;36mFeatureViewApi.get_by_name_version\u001b[1;34m(self, name, version)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_by_name_version\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, version):\n\u001b[0;32m     72\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path \u001b[38;5;241m+\u001b[39m [name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_VERSION, version]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_view\u001b[38;5;241m.\u001b[39mFeatureView\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[1;32m---> 74\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\hopsworks\\venv38\\lib\\site-packages\\hsfs\\decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[1;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\hopsworks\\venv38\\lib\\site-packages\\hsfs\\client\\base.py:171\u001b[0m, in \u001b[0;36mClient._send_request\u001b[1;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[0;32m    168\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/164/featurestores/106/featureview/nyc_taxi_fares/version/1). Server response: \nHTTP code: 404, HTTP reason: Not Found, error code: 270181, error msg: Feature view wasn't found., user msg: There exists no feature view with the name nyc_taxi_fares and version 1."
     ]
    }
   ],
   "source": [
    "nyc_fares_fv = fs.get_feature_view(\n",
    "    name = 'nyc_taxi_fares',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = nyc_fares_fv.get_train_test_split(\n",
    "    training_dataset_version=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['ride_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(cols_to_drop, axis=1)\n",
    "X_test = X_test.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db69c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cabdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "y_train, y_test = np.ravel(y_train), np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24654c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aaa45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33d3ab",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öúÔ∏è Weights and Biases </span>\n",
    "\n",
    "[Weights and Biases](https://wandb.ai/) is a free Python library that allows you to track, compare, and visualize ML experiments -> build better models faster. \n",
    "\n",
    "In our case we will use **W&B** to track Data Lineage using **Artifacts**, find the best hyperparameters using **Sweep** and visualize model performance.\n",
    "\n",
    "To begin with, let's install `wandb` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dcf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae116c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'fraud_online'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef5c17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d70f84",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üóÉ W&B Artifacts </span>\n",
    "\n",
    "Use W&B Artifacts for dataset versioning, model versioning, and tracking dependencies and results across machine learning pipelines. Think of an artifact as a versioned folder of data. You can store entire datasets directly in artifacts, or use artifact references to point to data in other systems like S3, GCP, or your own system. \n",
    "\n",
    "Also you can visualize Data Lineage for better understanding of project pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a run in W&B\n",
    "\n",
    "run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    job_type=\"upload_feature_view\",\n",
    "    name='metadata'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8879ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create an artifact for all the raw data\n",
    "raw_data_at = wandb.Artifact(\n",
    "    name=\"nyc_fares_fv\", \n",
    "    type=\"feature_view\",\n",
    "    metadata = {key:value.__repr__() for key,value in nyc_fares_fv.to_dict().items()}\n",
    ")\n",
    "\n",
    "# save artifact to W&B\n",
    "run.log_artifact(raw_data_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23745d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=\"train_validation_test_split\",\n",
    "    job_type='split'\n",
    ")\n",
    "\n",
    "data_at = run.use_artifact(\"nyc_fares_fv:latest\")\n",
    "data_dir = data_at.download()\n",
    "\n",
    "artifacts = {}\n",
    "\n",
    "for split in ['train','validation','test']:\n",
    "    artifacts[split] = wandb.Artifact(f'{split}_split', type=\"split\")  \n",
    "    \n",
    "for split, artifact in artifacts.items():\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a269a",
   "metadata": {},
   "source": [
    "To check Data Lineage follow next steps:\n",
    "\n",
    "1. Go to the [W&B main page](https://wandb.ai/home).\n",
    "\n",
    "2. Select **\"nyc_taxi_fares\"** project.\n",
    "\n",
    "3. Select the **\"Artifacts\"** icon in the left sidebar.\n",
    "\n",
    "4. Inspect the `nyc_fares_fv` type artifact.\n",
    "\n",
    "5. Go to the **\"Lineage\"** and then press **\"Explode\"**.\n",
    "\n",
    "So for now Data Lineage should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c305204",
   "metadata": {},
   "source": [
    "![image.png](../images/wandb_nyc_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc247212",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94955210",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üèãÔ∏è‚Äç‚ôÄÔ∏è  Define Model Training Function</span>\n",
    "\n",
    "It is important to define a function which will be used by the [Sweep agent](https://docs.wandb.ai/guides/sweeps/python-api).\n",
    "\n",
    "In this function we:\n",
    "\n",
    "- Set default hyperparameters for the model.\n",
    "\n",
    "- Initialize a new W&B Run using `wandb.init`.\n",
    "\n",
    "- Register all hyperparameters through `wandb.config`.\n",
    "\n",
    "- Create a RandomForestClassifier with a set of hyperparameters.\n",
    "\n",
    "- Fit a RandomForestClassifier.\n",
    "\n",
    "- Predict and evaluate.\n",
    "\n",
    "- Log all metrics using `wandb.log`.\n",
    "\n",
    "- Plot beautiful plots using `wandb.sklearn.plot_classifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17955e",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#ff5f27;\"> üìù Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Set default configurations (Defaults will be overwritten during sweep)\n",
    "    config_defaults = {\n",
    "      'max_depth': 3, \n",
    "      'n_estimators': 50\n",
    "    }\n",
    "\n",
    "    # Start W&B\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Fit regression model on train set\n",
    "    model = RandomForestRegressor(\n",
    "      max_depth=config.max_depth,\n",
    "      n_estimators=config.n_estimators\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_preds = model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    mae_score = mean_absolute_error(y_test, y_preds)\n",
    "    r2 = r2_score(y_test, y_preds)\n",
    "    print(f\"MAE: {round(mae_score, 4)}, R¬≤: {round(r2, 2)}\")\n",
    "\n",
    "    # Log model performance metrics to W&B\n",
    "    wandb.log({\"mae\": mae_score, \"r2_score\": r2})\n",
    "    \n",
    "    # wandb.sklearn.plot_regressor(model, X_train, X_test, y_train, y_test, model_name='RandomForestRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4959ab5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcc389",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîß Define Sweep Configurations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64085513",
   "metadata": {},
   "source": [
    "The next step is to define configurations for Sweep.\n",
    "\n",
    "**Weights & Biases Sweeps** are used to automate hyperparameter optimization and explore the space of possible models.\n",
    "\n",
    "You will initialize Sweep in form of a dictionary.\n",
    "\n",
    "You should include next steps:\n",
    "\n",
    "- `method`: specify your search strategy (**bayesian**, **grid** and **random** searches.)\n",
    "\n",
    "- `metric`: define the name and goal (maximize or minimize) of the metric. **Example**:\n",
    "        name: mae,\n",
    "        goal: minimize\n",
    "\n",
    "- `parameters`: define the hyperparameters as the keys of a dictionary and their corresponding values to search over in the form of a list stored as the values of this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda46d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configs = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"mae\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"max_depth\": {\n",
    "            \"values\": [4, 6, 8, 10]\n",
    "        },\n",
    "        \"n_estimators\": {\n",
    "            \"values\": [200, 350, 500, 800]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67b093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configs, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1044dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=train_model, count=5)\n",
    "\n",
    "# if this cell crushes, restart the kernel and dont run cells with artifacts above\n",
    "# run only Sweep cells instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7f7bb",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üéâ Great! üìà</span>\n",
    "\n",
    "Now you can go to the **Weights & Biases** UI to look at the results\n",
    "\n",
    "There you can find some great plots which will help you to control model development process such as **Feature Importance**, **ROC Curve**, **Confusion Matrix** and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67493ea4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5a461",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa02c",
   "metadata": {},
   "source": [
    "![image.png](../images/wandb_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd940d8",
   "metadata": {},
   "source": [
    "### After sorting by *mae* column, we see that the best hyperparameters are:\n",
    "    - max_depth: 10\n",
    "    - n_estimators: 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ad28f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lets train our model using that params\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestRegressor\u001b[49m(\n\u001b[0;32m      3\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[0;32m      5\u001b[0m     n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# lets train our model using that params\n",
    "rf_model = RandomForestRegressor(\n",
    "    max_depth=10,\n",
    "    n_estimators=800,\n",
    "    n_jobs = -1,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f00c71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rf_preds \u001b[38;5;241m=\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m rf_r2_score \u001b[38;5;241m=\u001b[39m r2_score(y_test, rf_preds)\n\u001b[0;32m      4\u001b[0m rf_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, rf_preds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_model' is not defined"
     ]
    }
   ],
   "source": [
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "rf_r2_score = r2_score(y_test, rf_preds)\n",
    "rf_mae = mean_absolute_error(y_test, rf_preds)\n",
    "\n",
    "print(\"Random Forest Regressor R¬≤:\", rf_r2_score)\n",
    "print(\"Random Forest Regressor MAE:\", rf_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc12a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_test, rf_preds, color='#613F75')\n",
    "plt.title('Model Residuals')\n",
    "plt.xlabel('Obsevation #')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686c32e",
   "metadata": {},
   "source": [
    "The Random Forest did a better job, so we will register this specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37570b92",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#ff5f27;\">üîÆ Saving Model in W&B</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "run = wandb.init(project=\"fraud_batch\", job_type=\"model_building\", name = 'rfregressor') \n",
    "\n",
    "data_at = run.use_artifact(\"train_split:latest\")\n",
    "data_dir = data_at.download()\n",
    "\n",
    "model_artifact = wandb.Artifact(\n",
    "            \"RandomForestRegressor\", type=\"model\",\n",
    "            description=\"This model is trained on the data from  `Hopsworks  Feature View`.\\\n",
    "                You can check it on the https://app.hopsworks.ai.\\\n",
    "                Just Login and go to the `Feature Views` page and find **nyc_fares_fv**.\",\n",
    "            metadata=dict(sweep_configs))\n",
    "\n",
    "joblib.dump(clf, \"model.joblib\")\n",
    "\n",
    "model_artifact.add_file(\"model.joblib\")\n",
    "\n",
    "wandb.save(\"model.joblib\")\n",
    "\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1670c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4d4f5",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model in Hopsworks</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "Let's connect to the model registry using the [HSML library](https://docs.hopsworks.ai/machine-learning-api/latest) from Hopsworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cc61f",
   "metadata": {},
   "source": [
    "Before registering the model we will export it as a pickle file using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d072cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rf_model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387695e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efe537",
   "metadata": {},
   "source": [
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701b407",
   "metadata": {},
   "source": [
    "With the schema in place, we can finally register our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'mae': rf_mae,\n",
    "    'r2_score': rf_r2_score\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mr.sklearn.create_model(\n",
    "    name=\"nyc_taxi_fares_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Random Forest Regressor.\",\n",
    "    input_example=X_train.sample(),\n",
    "    model_schema=model_schema\n",
    ")\n",
    "\n",
    "model.save('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d11235",
   "metadata": {},
   "source": [
    "Here we have also saved an input example from the training data, which can be helpful for test purposes.\n",
    "\n",
    "It's important to know that every time you save a model with the same name, a new version of the model will be saved, so nothing will be overwritten. In this way, you can compare several versions of the same model - or create a model with a new name, if you prefer that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c735826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mr.get_model(\"nyc_taxi_fares_model\", version = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9e42a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#ff5f27\"> üöÄ Model Deployment</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97150a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_example.py\n",
    "import os\n",
    "import hsfs\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model\"\"\"        \n",
    "        # get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # load the trained model\n",
    "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/model.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(np.array(inputs).reshape(1, -1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\", overwrite=True)\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3eb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cluster, I have created this file manually cause it was not creating by itself\n",
    "\n",
    "# predictor_script_path = f'hdfs:///Projects/{fs.project_name}/Jupyter/predict_example.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5380aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_script_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afd50b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840103c6",
   "metadata": {},
   "source": [
    "## üì° Create the deployment\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it any name you want\n",
    "deployment = model.deploy(\n",
    "    name=\"nyctaxifares\", \n",
    "    model_server=\"PYTHON\",\n",
    "    script_file=predictor_script_path,\n",
    "#     serving_tool = \"KSERVE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3227d1",
   "metadata": {},
   "source": [
    "### The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9673c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acce2fa",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üîÆ Predicting using deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": model.input_example\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ab78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572c573",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b526c75",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ  Wrapping things up </span>\n",
    "\n",
    "We have now performed a simple training with training data that we have created in the feature store. This concludes the fisrt module and introduction to the core aspect of the Feature store. In the second module we will introduce streaming and external feature groups for a similar fraud use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
