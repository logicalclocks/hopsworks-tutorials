{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë©üèª‚Äçüî¨ Feature Engineering </span>\n",
    "\n",
    "**Your Python Jupyter notebook should be configured for >8GB of memory.**\n",
    "\n",
    "In this series of tutorials, we will build a recommender system for fashion items. It will consist of two models: a *retrieval model* and a *ranking model*. The idea is that the retrieval model should be able to quickly generate a small subset of candidate items from a large collection of items. This comes at the cost of granularity, which is why we also train a ranking model that can afford to use more features than the retrieval model.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will use data from the [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations) Kaggle competition.\n",
    "\n",
    "<!-- https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data\n",
    "\n",
    "For this challenge you are given the purchase history of customers across time, along with supporting metadata. Your challenge is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends. Customer who did not make any purchase during that time are excluded from the scoring. -->\n",
    "\n",
    "The full dataset contains images of all products, but here we will simply use the tabular data. We have three data sources:\n",
    "- `articles.csv`: info about fashion items.\n",
    "- `customers.csv`: info about users.\n",
    "- `transactions_train.csv`: info about transactions.\n",
    "\n",
    "You can use the *hopsworks* library to download these files locally, assuming that they are stored in your cluster. In this example, we have saved them to the `Resources` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/logicalclocks/hopsworks-api@main#egg=hopsworks&subdirectory=python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "# project = hopsworks.login()\n",
    "project = hopsworks.login(\n",
    "    host=\"staging.cloud.hopsworks.ai\",\n",
    "    project=\"tutorials\",\n",
    "    api_key_value=\"I79TEwMzeZicqhwU.zxe9UBK79AGWTP6LEdH5j9BTzxJDaru0yX0RQ1OfQUkQKf9U9W0Az1P7eYtYq8va\"\n",
    ")\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # download datasets\n",
    "# dataset_api = project.get_dataset_api()\n",
    "\n",
    "# data_dir = \"data/\"\n",
    "\n",
    "# if not os.path.exists(data_dir):\n",
    "#     os.mkdir(data_dir)\n",
    "\n",
    "# for file in [\"articles.parquet\", \"customers.parquet\", \"transactions_train.parquet\"]:\n",
    "#     dataset_api.download(f\"Resources/{file}\", local_path=data_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üóÑÔ∏è Read Articles Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_parquet(\"data/articles.parquet\")\n",
    "articles_df[\"article_id\"] = articles_df[\"article_id\"].astype(str)\n",
    "print(articles_df.shape)\n",
    "articles_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "articles_df.isna().sum()[articles_df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üóÑÔ∏è Read Customers Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_parquet(\"data/customers.parquet\")\n",
    "print(customers_df.shape)\n",
    "customers_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "customers_df.isna().sum()[customers_df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üóÑÔ∏è Read Transactions Train Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = pd.read_parquet(\"data/transactions_train.parquet\")\n",
    "print(trans_df.shape)\n",
    "trans_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "trans_df.isna().sum()[trans_df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df[\"article_id\"] = trans_df[\"article_id\"].astype(str)\n",
    "trans_df['t_dat'] = trans_df['t_dat'].apply(lambda x: pd.to_datetime(x))\n",
    "trans_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(trans_df):,} transactions in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a large dataset. For the sake of the tutorial, we will use a small subset of this dataset, which we generate by sampling 25'000 customers and using their transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = 25_000\n",
    "\n",
    "# Consider only customers with age defined.\n",
    "customers_df.dropna(inplace=True, subset=[\"age\"])\n",
    "customer_subset_df = customers_df.sample(N_USERS, random_state=27)\n",
    "trans_df = trans_df.merge(customer_subset_df[\"customer_id\"])\n",
    "\n",
    "print(f\"Subset has {len(trans_df):,} transactions in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë®üèª‚Äçüè≠ Feature Engineering</span>\n",
    "\n",
    "Next, we do some feature engineering.\n",
    "\n",
    "The time of the year a purchase was made should be a strong predictor, as seasonality plays a big factor in fashion purchases. Here, we will use the month of the purchase as a feature. Since this is a cyclical feature (January is as close to December as it is to February), we'll map each month to the unit circle using sine and cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transformations.py\n",
    "\n",
    "def month_sin(t_dat):\n",
    "    month = t_dat.month - 1\n",
    "    C = 2*np.pi/12\n",
    "    return np.sin(month*C).item()\n",
    "\n",
    "def month_cos(t_dat):\n",
    "    month = t_dat.month - 1\n",
    "    C = 2*np.pi/12\n",
    "    return np.cos(month*C).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformation functions for computing sin and cos of month\n",
    "from transformations import month_sin, month_cos\n",
    "\n",
    "fns = [fn.name for fn in fs.get_transformation_functions()]\n",
    "\n",
    "if \"month_sin\" not in fns:\n",
    "    month_to_sin = fs.create_transformation_function(month_sin, output_type=float, version=1)\n",
    "    month_to_sin.save()\n",
    "    \n",
    "if \"month_cos\" not in fns:\n",
    "    month_cos = fs.create_transformation_function(month_cos, output_type=float, version=1)\n",
    "    month_cos.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.dropna(axis=1, inplace=True)\n",
    "articles_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_df[\"month_sin\"] = trans_df[\"t_dat\"]\n",
    "trans_df[\"month_cos\"] = trans_df[\"t_dat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert python datetime object to unix epoch milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.t_dat = trans_df.t_dat.values.astype(np.int64) // 10 ** 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">ü™Ñ Feature Group Creation </span>\n",
    "\n",
    "A [feature group](https://docs.hopsworks.ai/feature-store-api/latest/generated/feature_group/) can be seen as a collection of conceptually related features.\n",
    "\n",
    "Before we can create a feature group we need to connect to our feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a feature group we need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg = fs.create_feature_group(\n",
    "    name=\"customers\",\n",
    "    description=\"Customers data including age and postal code\",\n",
    "    primary_key=[\"customer_id\"],\n",
    "    online_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have also set `online_enabled=True`, which enables low latency access to the data. A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group).\n",
    "\n",
    "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent we populate it with its associated data using the `save` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg.insert(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for the rest of the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_fg = fs.create_feature_group(\n",
    "    name=\"articles\",\n",
    "    description=\"Fashion items data including type of item, visual description and category\",\n",
    "    primary_key=[\"article_id\"],\n",
    "    online_enabled=True,\n",
    ")\n",
    "articles_fg.insert(articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.create_feature_group(\n",
    "    name=\"transactions\",\n",
    "    version=1,\n",
    "    description=\"Transactions data including customer, item, price, sales channel and transaction date\",\n",
    "    primary_key=[\"customer_id\", \"article_id\"], \n",
    "    online_enabled=True,\n",
    "    event_time=\"t_dat\",\n",
    ")\n",
    "trans_fg.insert(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to inspect the feature groups in the Hopsworks UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">‚öôÔ∏è Feature View Creation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [feature view](https://docs.hopsworks.ai/feature-store-api/latest/generated/feature_view/) can be seen as a logical view over a set of features that may come from different feature groups.\n",
    "\n",
    "Feature views provides an Offline and Online API that can be used to generate training data or retrieve online feature vectors at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create two feature views for customers and articles, that will be used during model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_query = customers_fg.select_all()\n",
    "customers_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_feature_view = fs.create_feature_view(\n",
    "    name='customers',\n",
    "    query=customers_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_query = articles_fg.select_all()\n",
    "articles_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_feature_view = fs.create_feature_view(\n",
    "    name='articles',\n",
    "    query=articles_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27\">‚è©Ô∏è Next Steps </span>\n",
    "In the next notebook we'll create a dataset that we can train a retrieval model on."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
