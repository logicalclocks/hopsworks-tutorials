{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Training Pipeline</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This notebook explains how to read from a feature group, create training dataset within the feature store, train a model and save it to model registry.</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Fetch Feature Groups.\n",
    "2. Define Transformation functions.\n",
    "3. Create Feature Views.\n",
    "4. Create Training Dataset with training, validation and test splits.\n",
    "5. Train the model.\n",
    "6. Register model in Hopsworks Model Registry.\n",
    "\n",
    "![part2](../../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You will start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "trans_fg = fs.get_feature_group(\n",
    "    name='transactions_fraud_streaming_fg', \n",
    "    version=1,\n",
    ")\n",
    "window_aggs_fg = fs.get_feature_group(\n",
    "    name='transactions_aggs_fraud_streaming_fg', \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data.\n",
    "selected_features = trans_fg.select([\"fraud_label\", \"category\", \"amount\", \"datetime\", \"age_at_transaction\", \"days_until_card_expires\"])\\\n",
    "    .join(window_aggs_fg.select_except([\"cc_num\", \"event_time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hive (21.07s) \n"
     ]
    }
   ],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "df = selected_features.read(read_options={\"use_hive\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you computed the features in `transactions_4h_aggs_fraud_batch_fg` using 4-hour aggregates. If you had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join you would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions </span>\n",
    "\n",
    "\n",
    "You will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"category\": label_encoder,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View you may use `fs.create_feature_view()`. Here we try first to get the feature view, and if we can't an exception is thrown and we create the feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://aff99120-da3e-11ee-8cd7-4f3734b3ce24.cloud.hopsworks.ai/p/3192/fs/3140/fv/transactions_view_fraud_batch_fv/version/1\n"
     ]
    }
   ],
   "source": [
    "# Get or create the 'transactions_view_fraud_batch_fv' feature view\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='transactions_view_fraud_batch_fv',\n",
    "    version=1,\n",
    "    query=selected_features,\n",
    "    labels=[\"fraud_label\"],\n",
    "    transformation_functions=transformation_functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view is now visible in the UI.\n",
    "\n",
    "![fg-overview](../images/fv_overview.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `feature_view.train_validation_test_split()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hive (25.04s) \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "DataType(null)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7be24c19cf8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m X_train, X_test, y_train, y_test = feature_view.train_test_split(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEST_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"use_hive\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/usage.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Disable usage AFTER import hsfs, return function itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m   2203\u001b[0m             \u001b[0mextra_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         )\n\u001b[0;32m-> 2205\u001b[0;31m         td, df = self._feature_view_engine.get_training_data(\n\u001b[0m\u001b[1;32m   2206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m             \u001b[0mread_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mspine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n\u001b[0;32m--> 354\u001b[0;31m             split_df = engine.get_instance().get_training_data(\n\u001b[0m\u001b[1;32m    355\u001b[0m                 \u001b[0mtd_updated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, training_dataset_obj, feature_view_obj, query_obj, read_options)\u001b[0m\n\u001b[1;32m    628\u001b[0m     ):\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             return self._prepare_transform_split_df(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 \u001b[0mquery_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36m_prepare_transform_split_df\u001b[0;34m(self, query_obj, training_dataset_obj, feature_view_obj, read_option)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;31m# apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# 1st parametrise transformation functions with dt split stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         transformation_function_engine.TransformationFunctionEngine.populate_builtin_transformation_functions(\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\u001b[0m in \u001b[0;36mpopulate_builtin_transformation_functions\u001b[0;34m(training_dataset, feature_view_obj, dataset)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;31m# compute statistics before transformations are applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 stats = (\n\u001b[0;32m--> 276\u001b[0;31m                     TransformationFunctionEngine.compute_transformation_fn_statistics(\n\u001b[0m\u001b[1;32m    277\u001b[0m                         \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                         \u001b[0mbuiltin_tffn_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/transformation_function_engine.py\u001b[0m in \u001b[0;36mcompute_transformation_fn_statistics\u001b[0;34m(training_dataset_obj, builtin_tffn_features, label_encoder_features, feature_dataframe, feature_view_obj)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     ) -> statistics.Statistics:\n\u001b[0;32m--> 240\u001b[0;31m         return training_dataset_obj._statistics_engine.compute_transformation_fn_statistics(\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mtd_metadata_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuiltin_tffn_features\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# excluding label encoded features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\u001b[0m in \u001b[0;36mcompute_transformation_fn_statistics\u001b[0;34m(self, td_metadata_instance, columns, label_encoder_features, feature_dataframe, feature_view_obj)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m    238\u001b[0m         \u001b[0mcomputation_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         stats_str = self._profile_transformation_fn_statistics(\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mfeature_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\u001b[0m in \u001b[0;36m_profile_transformation_fn_statistics\u001b[0;34m(self, feature_dataframe, columns, label_encoder_features)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# compute statistics for all features with transformation fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mall_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder_features\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         stats_str = engine.get_instance().profile(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mfeature_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(self, df, relevant_columns, correlations, histograms, exact_uniqueness)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_large_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_struct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             ) and PYARROW_HOPSWORKS_DTYPE_MAPPING[field.type] in [\"timestamp\", \"date\"]:\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: DataType(null)"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
    "    test_size = TEST_SIZE, read_options={\"use_hive\":True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the X_train DataFrame based on the \"datetime\" column in ascending order\n",
    "X_train = X_train.sort_values(\"datetime\")\n",
    "\n",
    "# Reindex the y_train Series to match the order of rows in the sorted X_train DataFrame\n",
    "y_train = y_train.reindex(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the X_test DataFrame based on the \"datetime\" column in ascending order\n",
    "X_test = X_test.sort_values(\"datetime\")\n",
    "\n",
    "# Reindex the y_test Series to match the order of rows in the sorted X_test DataFrame\n",
    "y_test = y_test.reindex(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"datetime\" column from the X_train DataFrame along the specified axis (axis=1 means columns)\n",
    "X_train.drop([\"datetime\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop the \"datetime\" column from the X_test DataFrame along the specified axis (axis=1 means columns)\n",
    "X_test.drop([\"datetime\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the normalized value counts of the y_train Series\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus you should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, you will use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üß¨ Modeling</span>\n",
    "\n",
    "Next you will train a model. Here, you set larger class weight for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBClassifier\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training data using the trained classifier\n",
    "y_pred_train = clf.predict(X_train.values)\n",
    "\n",
    "# Predict the test data using the trained classifier\n",
    "y_pred_test = clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute f1 score\n",
    "metrics = {\n",
    "    \"f1_score\": f1_score(y_test, y_pred_test, average='macro')\n",
    "}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix using the true labels (y_test) and predicted labels (y_pred_test)\n",
    "results = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the confusion matrix results with appropriate labels\n",
    "df_cm = pd.DataFrame(\n",
    "    results, \n",
    "    ['True Normal', 'True Fraud'],\n",
    "    ['Pred Normal', 'Pred Fraud'],\n",
    ")\n",
    "\n",
    "# Create a heatmap using seaborn with annotations\n",
    "cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "# Get the figure from the heatmap and display it\n",
    "fig = cm.get_figure()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/3.0/user_guides/mlops/registry/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Define the input schema using the values of X_train\n",
    "input_schema = Schema(X_train.values)\n",
    "\n",
    "# Define the output schema using y_train\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "# Create a ModelSchema object specifying the input and output schemas\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "# Convert the model schema to a dictionary for further inspection or serialization\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where the model will be saved\n",
    "model_dir = \"fraud_batch_model\"\n",
    "\n",
    "# Check if the directory exists, and create it if it doesn't\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Save the trained XGBoost model using joblib\n",
    "joblib.dump(clf, model_dir + '/xgboost_fraud_batch_model.pkl')\n",
    "\n",
    "# Save the confusion matrix heatmap as an image in the model directory\n",
    "fig.savefig(model_dir + \"/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Create a new model in the model registry\n",
    "fraud_model = mr.python.create_model(\n",
    "    name=\"xgboost_fraud_batch_model\",     # Name for the model\n",
    "    metrics=metrics,                      # Metrics used for evaluation\n",
    "    model_schema=model_schema,            # Schema defining the model's input and output\n",
    "    input_example=X_train.sample(),       # Example input data for reference\n",
    "    description=\"Fraud Batch Predictor\",  # Description of the model\n",
    ")\n",
    "\n",
    "# Save the model to the specified directory\n",
    "fraud_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Batch Inference</span>\n",
    "\n",
    "In the following notebook you will use your model for batch inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
