{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c42430",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Load, Engineer & Connect</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This is the first part of the Neo4j and Hopsworks Feature Store integration. As part of this first module, we will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store**  for batch data with a goal of training and deploying a model that can predict fraudulent transactions.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 3 sections:** \n",
    "1. Loading the data and do feature engineeing,\n",
    "2. Connect to the Neo4j database to represent transactions as a graph and compute node embeddings\n",
    "3. Connect to the Hopsworks feature store,\n",
    "4. Create feature groups and upload them to the feature store.\n",
    "\n",
    "![tutorial-flow](../../images/01_featuregroups.png)\n",
    "\n",
    "First of all we will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290652c9",
   "metadata": {},
   "source": [
    "### üìù Import librararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e28cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5505b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n",
    "\n",
    "The data we will use comes from three different CSV files:\n",
    "\n",
    "- `transactions.csv`: transaction information such as timestamp, location, and the amount. \n",
    "- `alert_transactions.csv`: Suspicious Activity Report (SAR) transactions.\n",
    "- `party.csv`: User profile information.\n",
    "\n",
    "In a production system, these CSV files would originate from separate data sources or tables, and probably separate data pipelines. **All three files have a customer id column `id` in common, which we can use for joins.**\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8404",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba169b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\", parse_dates = ['tran_timestamp'])\n",
    "transactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbd13d",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Alert Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/alert_transactions.csv\")\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8874447",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Party dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebe30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "party = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\")\n",
    "party.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b7cdc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n",
    "\n",
    "#### To investigate patterns of suspicious activities you will make time window aggregates such monthly frequency, total, mean and standard deviation of amount of incoming and outgoing transasactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename some columns\n",
    "transactions_df = transactions_df.rename(columns={\"src\": \"source\",\n",
    "                                                  \"dst\": \"target\"}, errors=\"raise\")\n",
    "\n",
    "# select interested columns\n",
    "transactions_df = transactions_df[[\"source\", \"target\", \"tran_timestamp\", \"tran_id\", \"base_amt\"]]\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a30fc3",
   "metadata": {},
   "source": [
    "##### Outgoing transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04daf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'source'])\\\n",
    "                            .agg(monthly_count=('source','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std')\n",
    "                                )\n",
    "out_df = out_df.reset_index(level=[\"source\"])\n",
    "out_df = out_df.reset_index(level=[\"tran_timestamp\"])\n",
    "\n",
    "# rename some columns\n",
    "out_df = out_df.rename(columns={\"source\": \"id\",\n",
    "                                                  \"monthly_count\": \"monthly_out_count\",\n",
    "                                                  \"monthly_total_amount\": \"monthly_out_total_amount\",\n",
    "                                                  \"monthly_mean_amount\": \"monthly_out_mean_amount\",\n",
    "                                                  \"monthly_std_amount\": \"monthly_out_std_amount\"}, errors=\"raise\")\n",
    "\n",
    "out_df.tran_timestamp = out_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccff24",
   "metadata": {},
   "source": [
    "##### Incoming transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'target'])\\\n",
    "                            .agg(monthly_count=('target','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std'))\n",
    "\n",
    "in_df = in_df.reset_index(level=[\"target\"])\n",
    "in_df = in_df.reset_index(level=[\"tran_timestamp\"])\n",
    "in_df.columns  = [\"tran_timestamp\", \"id\", \"monthly_in_count\", \"monthly_in_total_amount\", \"monthly_in_mean_amount\", \"monthly_in_std_amount\"]\n",
    "in_df.tran_timestamp = in_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "in_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b7b1d",
   "metadata": {},
   "source": [
    "##### Now lets join incoming and outgoing transcations datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_df = in_df.merge(out_df, on=['tran_timestamp', 'id'], how=\"outer\")\n",
    "in_out_df =  in_out_df.fillna(0)\n",
    "in_out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8d635",
   "metadata": {},
   "source": [
    "#### Assign labels to transactions that were identified as suspicius activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0265b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels = transactions_df[[\"source\",\"target\",\"tran_id\",\"tran_timestamp\"]].merge(alert_transactions[[\"is_sar\", \"tran_id\"]], on=[\"tran_id\"], how=\"left\")\n",
    "transaction_labels.is_sar = transaction_labels.is_sar.map({True: 1, np.nan: 0})\n",
    "transaction_labels.sort_values('tran_id',inplace = True)\n",
    "transaction_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1872",
   "metadata": {},
   "source": [
    "#### Now lets prepare profile (party) dataset and assign lables whether they have been reported for suspicius activity or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "party.columns = [\"id\",\"type\"]\n",
    "party.type = party.type.map({\"Individual\": 0, \"Organization\": 1})\n",
    "\n",
    "party.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]\n",
    "\n",
    "alert_sources = alert_transactions[[\"source\", \"tran_timestamp\"]]\n",
    "alert_sources.columns = [\"id\", \"tran_timestamp\"]\n",
    "alert_sources.head()\n",
    "alert_targets = alert_transactions[[\"target\", \"tran_timestamp\"]]\n",
    "alert_targets.columns = [\"id\", \"tran_timestamp\"]\n",
    "\n",
    "sar_party = pd.concat([alert_sources, alert_targets], ignore_index=True)\n",
    "\n",
    "sar_party.sort_values([\"id\", \"tran_timestamp\"], ascending = [False, True])\n",
    "\n",
    "# find a 1st occurence of sar per id\n",
    "sar_party = sar_party.iloc[[sar_party.id.eq(id).idxmax() for id in sar_party['id'].value_counts().index]]\n",
    "sar_party = sar_party.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'id']).agg(monthly_count=('id','count'))\n",
    "sar_party = sar_party.reset_index(level=[\"id\"])\n",
    "sar_party = sar_party.reset_index(level=[\"tran_timestamp\"])\n",
    "sar_party.drop([\"monthly_count\"], axis=1, inplace=True)\n",
    "\n",
    "sar_party[\"is_sar\"] = sar_party[\"is_sar\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e785c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels = party.merge(sar_party, on=[\"id\"], how=\"left\")\n",
    "party_labels.is_sar = party_labels.is_sar.map({1.0: 1, np.nan: 0})\n",
    "max_time_stamp = datetime.datetime.utcfromtimestamp(int(max(transaction_labels.tran_timestamp.values))/1e9)\n",
    "party_labels = party_labels.fillna(max_time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47922dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f5b51-6ad2-4017-8773-fd7ddf9d3161",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Graph representational learning using Neo4j Graph Database</span>\n",
    "\n",
    "Financial transactions can be represented as a dynamic network graph. Using the technique of graph representation \n",
    "allows for representing transactions with a broader context. In this example, you will perform node \n",
    "representation learning. \n",
    "\n",
    "Neo4j graph architecture and database are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638dcd1f-be78-433b-b65a-1ec0e0878fbc",
   "metadata": {},
   "source": [
    "#### Neo4j setup\n",
    "Before executing the next cells, the Neo4j database must be installed and initialized:\n",
    "- Install Neo4j Desktop from https://neo4j.com/download/\n",
    "- Create a new database project and server\n",
    "- Install the APOC and GDS plugins\n",
    "- BOLT protocol should be set (if already is not) in the the [neo4j.conf](https://neo4j.com/docs/operations-manual/current/configuration/neo4j-conf/) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "import math\n",
    "\n",
    "def convertToNumber(s):\n",
    "    return int.from_bytes(s.encode(), 'little')\n",
    "\n",
    "def convertFromNumber(n):\n",
    "    return n.to_bytes(math.ceil(n.bit_length() / 8), 'little').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j database\n",
    "gds = GraphDataScience('bolt://localhost:7687', auth=('neo4j', 'hopsworks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe259910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_neo4j(df):\n",
    "    \"\"\"\n",
    "    Extract visited nodes and convert them to positive integers (Neo4j requirement).\n",
    "    Return processed nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract unique nodes visited\n",
    "    unique_ids = pd.concat([df['source'], df['target']]).unique()\n",
    "    nodes = pd.DataFrame(unique_ids, columns=['nodeId'])\n",
    "    \n",
    "    nodes = nodes[['nodeId']]\n",
    "    \n",
    "    # Convert node ID to a positive integer (Neo4j requirements)\n",
    "    nodes['nodeId'] = [convertToNumber(nodeId) for nodeId in nodes['nodeId']]\n",
    "    nodes['nodeId'] = nodes['nodeId'].astype(int)\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def get_relationships_neo4j(df):\n",
    "    \"\"\"\n",
    "    Extract edges and convert their nodes to positive integers (Neo4j requirement).\n",
    "    Returned processed edges (relationships).\n",
    "    \"\"\"\n",
    "    \n",
    "    relationships = df[['source', 'target', 'tran_id']]\n",
    "    relationships = relationships.rename(columns={\"source\": \"sourceNodeId\",\n",
    "                                                  \"target\": \"targetNodeId\"},\n",
    "                                         errors=\"raise\")\n",
    "\n",
    "    # Convert source node ID to a positive integer (Neo4j requirements)\n",
    "    relationships['sourceNodeId'] = [convertToNumber(sourceNodeId) for sourceNodeId in relationships['sourceNodeId']]\n",
    "    relationships['sourceNodeId'] = relationships['sourceNodeId'].astype(int)\n",
    "\n",
    "    # Convert target node ID to a positive integer (Neo4j requirements)\n",
    "    relationships['targetNodeId'] = [convertToNumber(targetNodeId) for targetNodeId in relationships['targetNodeId']]\n",
    "    relationships['targetNodeId'] = relationships['targetNodeId'].astype(int)\n",
    "\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pupulate_graph(input_df):\n",
    "    \"\"\"\n",
    "    Build Neo4j graph and return node embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Neo4j node and relationships formatting\n",
    "    nodes = get_nodes_neo4j(input_df)\n",
    "    relationships = get_relationships_neo4j(input_df)\n",
    "    \n",
    "    # Build Graph\n",
    "    G = gds.graph.construct(\"transactions-graph\", nodes, relationships)\n",
    "\n",
    "    # Check if the number of nodes is correctly stored\n",
    "    assert G.node_count() == len(nodes)\n",
    "\n",
    "    # Compute embeddings\n",
    "    graph_embdeddings_df = gds.node2vec.stream(G)\n",
    "\n",
    "    # Delete graph for next partition\n",
    "    G.drop()\n",
    "\n",
    "    # Convert integer node ID back to the original ID\n",
    "    graph_embdeddings_df['nodeId'] = [convertFromNumber(nodeId) for nodeId in graph_embdeddings_df['nodeId']]\n",
    "\n",
    "    return {\"id\": graph_embdeddings_df.nodeId.to_numpy(), \"graph_embeddings\": graph_embdeddings_df.embedding.to_numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288f063",
   "metadata": {},
   "source": [
    "#### Compute graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f95f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "transaction_graphs_by_month = transaction_labels.groupby(pd.Grouper(key='tran_timestamp', freq='M')).progress_apply(lambda x: pupulate_graph(x))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embeddings data frame\n",
    "\n",
    "timestamps = transaction_graphs_by_month.index.values\n",
    "graph_embeddings = transaction_graphs_by_month.tolist()\n",
    "\n",
    "graph_embdeddings_df = pd.DataFrame()\n",
    "for timestamp, graph_embedding in zip(timestamps, graph_embeddings):\n",
    "    df_tmp = pd.DataFrame(graph_embedding)\n",
    "    df_tmp[\"tran_timestamp\"] = timestamp\n",
    "    graph_embdeddings_df = pd.concat([graph_embdeddings_df, df_tmp])    \n",
    "graph_embdeddings_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89521da4",
   "metadata": {},
   "source": [
    "#### Convert date time to unix epoc milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "graph_embdeddings_df.tran_timestamp = graph_embdeddings_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)\n",
    "\n",
    "transaction_labels['month'] = pd.to_datetime(transaction_labels['tran_timestamp'], unit='ms').dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e4a4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üëÆüèº‚Äç‚ôÄÔ∏è Data Validation \n",
    "\n",
    "Before you define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. You do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value you can decide whether to stop it to `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section you will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation with Greate Expectations in Hopsworks\n",
    "You can use GE library for validation in Hopsworks features store. \n",
    "\n",
    "##  <img src=\"../../images/icon102.png\" width=\"18px\"></img> Hopsworks feature store\n",
    "\n",
    "The Hopsworks feature feature store library is Apache V2 licensed and available [here](https://github.com/logicalclocks/feature-store-api). The library is currently available for Python and JVM languages such as Scala and Java.\n",
    "In this notebook, we are going to cover Python part.\n",
    "\n",
    "You can find the complete documentation of the library here: \n",
    "\n",
    "The first step is to establish a connection with your Hopsworks feature store instance and retrieve the object that represents the feature store you'll be working with. \n",
    "\n",
    "> By default `connection.get_feature_store()` returns the feature store of the project we are working with. However, it accepts also a project name as parameter to select a different feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0128f-01c8-4e18-b72b-d1e13744c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914eda6f",
   "metadata": {},
   "source": [
    "### üî¨ Expectations suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Expectation Suite - no use of HSFS\n",
    "import great_expectations as ge\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "expectation_suite = ge.core.ExpectationSuite(expectation_suite_name=\"aml_project_validations\")\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite.add_expectation(\n",
    "  ge.core.ExpectationConfiguration(\n",
    "  expectation_type=\"expect_column_max_to_be_between\",\n",
    "  kwargs={\"column\": \"monthly_in_count\", \"min_value\": 0, \"max_value\": 10000000}) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8420d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Register Feature Groups </span>\n",
    "\n",
    "### Feature Groups\n",
    "\n",
    "A `Feature Groups` is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The `Feature Group` lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them.\n",
    "\n",
    "Generally, the features in a feature group are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, `feature groups` provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in.\n",
    "\n",
    "> It is important to note that `feature groups` are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173138f",
   "metadata": {},
   "source": [
    "#### Transactions monthly aggregates feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    partition_key = [\"tran_timestamp\"],   \n",
    "    description = \"transactions monthly aggregates features\",\n",
    "    event_time = 'tran_timestamp',\n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False},\n",
    "    expectation_suite=expectation_suite\n",
    ")   \n",
    "\n",
    "transactions_fg.insert(in_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a740c",
   "metadata": {},
   "source": [
    "#### Party feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7668d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_fg = fs.get_or_create_feature_group(\n",
    "    name = \"party_labels\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    description = \"party fg with labels\",\n",
    "    event_time = 'tran_timestamp',        \n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "party_fg.insert(party_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b9b89",
   "metadata": {},
   "source": [
    "#### Graph embeddings feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53667d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase Hopsworks feature size to store embeddings\n",
    "\n",
    "from hsfs import engine\n",
    "features = engine.get_instance().parse_schema_feature_group(graph_embdeddings_df)\n",
    "for f in features:\n",
    "    if f.type == \"array<double>\" or f.type == \"array<float>\":\n",
    "        f.online_type = \"VARBINARY(20000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9dcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embeddings_fg = fs.get_or_create_feature_group(name=\"graph_embeddings\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions graph\",\n",
    "                                       event_time = 'tran_timestamp',     \n",
    "                                       online_enabled=True,\n",
    "                                       features=features,\n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}\n",
    "                                       )\n",
    "graph_embeddings_fg.insert(graph_embdeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b191ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a602429",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëì Exploration </span>\n",
    "\n",
    "### Feature groups are now accessible and searchable in the UI\n",
    "![fg-overview](images/fg_explore.gif)\n",
    "\n",
    "## üìä Statistics\n",
    "We can explore feature statistics in the feature groups. If statistics was not enabled when feature group was created then this can be done by:\n",
    "\n",
    "```python\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_fg\", \n",
    "    version = 1)\n",
    "\n",
    "transactions_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "transactions_fg.update_statistics_config()\n",
    "transactions_fg.compute_statistics()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c01da5",
   "metadata": {},
   "source": [
    "![fg-stats](images/freature_group_stats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec01fc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 02 </span>\n",
    "    \n",
    "In the following notebook you will use feature groups to create feature viewa and training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
