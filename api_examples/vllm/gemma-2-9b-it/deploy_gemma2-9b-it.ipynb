{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guide for Gemma 2 9B IT on Hopsworks\n",
    "\n",
    "For details about this Large Language Model (LLM) visit the model page in the HuggingFace repository ‚û°Ô∏è [link](https://huggingface.co/google/gemma-2-9b-it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Download Gemma 2 9B IT using the huggingface_hub library\n",
    "\n",
    "First, we download the Gemma 2 model files (e.g., weights, configuration files) directly from the HuggingFace repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place your HuggingFace token in the HF_TOKEN environment variable\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"<INSERT_YOUR_HF_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "gemma_local_dir = snapshot_download(model_id, ignore_patterns=\"original/*\", local_dir=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Register Gemma 2 9B IT into Hopsworks Model Registry\n",
    "\n",
    "Once the model files are downloaded from the HuggingFace repository, we can register the models files into the Hopsworks Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following instantiates a Hopsworks LLM model, not yet saved in the Model Registry\n",
    "\n",
    "gemma = mr.llm.create_model(\n",
    "    name=\"gemma2_9b_it\",\n",
    "    description=\"Gemma 2 9B IT model (via HF)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Gemma model pointing to the local model files\n",
    "\n",
    "gemma.save(gemma_local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Deploy Gemma 2 9B IT\n",
    "\n",
    "After registering the LLM model into the Model Registry, we can create a deployment that serves it using the vLLM engine with a user-provided configuration (yaml) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a reference to the Gemma model if not obtained yet\n",
    "\n",
    "gemma = mr.get_model(\"gemma2_9b_it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload vllm engine config file for the deployments\n",
    "\n",
    "ds_api = project.get_dataset_api()\n",
    "\n",
    "path_to_config_file = f\"/Projects/{project.name}/\" + ds_api.upload(\"gemma_vllmconfig.yaml\", \"Resources\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_depl = gemma.deploy(\n",
    "    name=\"gemma2v1\",\n",
    "    description=\"Gemma 2 9B IT from HuggingFace\",\n",
    "    config_file=path_to_config_file,\n",
    "    resources={\"num_instances\": 1, \"requests\": {\"cores\": 1, \"memory\": 1024*12, \"gpus\": 1}, \"limits\": {\"cores\": 2, \"memory\": 1024*16, \"gpus\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one of the deployments created above\n",
    "\n",
    "ms = project.get_model_serving()\n",
    "gemma_depl = ms.get_deployment(\"gemma2v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_depl.start(await_running=60*15) # wait for 15 minutes maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma_depl.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_depl.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Prompting Gemma 2 9B IT\n",
    "\n",
    "Once the Gemma deployment is up and running, we can start sending user prompts to the LLM. You can either use an OpenAI API-compatible client (e.g., openai library) or any other http client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nopenai_v1_uri = gemma_depl.get_openai_url()\ncompletions_url = openai_v1_uri + \"/completions\" \nchat_completions_url = openai_v1_uri + \"/chat/completions\"\n\n# Resolve API key for request authentication\nif \"SERVING_API_KEY\" in os.environ:\n    # if running inside Hopsworks\n    api_key_value = os.environ[\"SERVING_API_KEY\"]\nelse:\n    # Create an API KEY using the Hopsworks UI and place the value below\n    api_key_value = \"<API_KEY>\"\n    \n# Prepare request headers\nheaders = {\n    'Content-Type': 'application/json',\n    'Authorization': 'ApiKey ' + api_key_value,\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü® Using httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chat Completion for a user message\n",
    "#\n",
    "\n",
    "user_message = \"Who is the best French painter. Answer with detailed explanations.\"\n",
    "\n",
    "completion_request = {\n",
    "    \"model\": gemma_depl.name,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Completion request: \", completion_request, end=\"\\n\")\n",
    "\n",
    "response = httpx.post(chat_completions_url, headers=headers, json=completion_request, timeout=45.0)\n",
    "print(response)\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chat Completion for list of messages\n",
    "#\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! How are you doing today?\"\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing well! How can I help you?\",\n",
    "}, {\n",
    "    \"role\": \"user\",\n",
    "     \"content\": \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\"\n",
    "}]\n",
    "\n",
    "\n",
    "completion_request = {\n",
    "    \"model\": gemma_depl.name,\n",
    "    \"messages\": messages\n",
    "}\n",
    "\n",
    "print(\"Completion request: \", completion_request, end=\"\\n\")\n",
    "\n",
    "response = httpx.post(chat_completions_url, headers=headers, json=completion_request, timeout=45.0)\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü® Using OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=openai_v1_uri,\n",
    "    api_key=\"X\",\n",
    "    default_headers=headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chat Completion for a user message\n",
    "#\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=gemma_depl.name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who is the best French painter. Answer with a short explanations.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Chat Completion for list of messages\n",
    "#\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=gemma_depl.name,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi! How are you doing today?\"\n",
    "    }, {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well! How can I help you?\",\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "         \"content\": \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}