{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë®üèª‚Äçüè´ Build Index </span>\n",
    "\n",
    "In this notebook we will build an index for our candidate embeddings. Here we will use OpenSearch, which is natively supported by Hopsworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üéØ Compute Candidate Embeddings </span>\n",
    "\n",
    "We start by computing candidate embeddings for all items in the training data.\n",
    "\n",
    "First, we load our candidate model. Recall that we uploaded it to the Hopsworks Model Registry in the previous notebook. If you don't have the model locally you can download it from the Model Registry using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mr.get_model(\n",
    "    name=\"candidate_model\",\n",
    "    version=1,\n",
    ")\n",
    "model_path = model.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the model saved locally you can simply replace `model_path` with the path to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_model = tf.saved_model.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the embeddings of all candidate items that were used to train the retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\n",
    "    name=\"retrieval\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, y_train, y_val, y_test = feature_view.train_validation_test_split(\n",
    "    validation_size=0.1, \n",
    "    test_size=0.1,\n",
    "    description='Retrieval dataset splits',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"article_id\"] = train_df[\"article_id\"].astype(str)\n",
    "val_df[\"article_id\"] = val_df[\"article_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of input features for the candidate model.\n",
    "model_schema = model.model_schema['input_schema']['columnar_schema']\n",
    "candidate_features = [feat['name'] for feat in model_schema]\n",
    "\n",
    "# Get list of unique candidate items.\n",
    "item_df = train_df[candidate_features]\n",
    "item_df.drop_duplicates(subset=\"article_id\", inplace=True)\n",
    "\n",
    "item_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    {col: item_df[col] for col in item_df})\n",
    "\n",
    "# Compute embeddings for all candidate items.\n",
    "candidate_embeddings = item_ds.batch(2048).map(\n",
    "    lambda x: (x[\"article_id\"], candidate_model(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Strictly speaking, we haven't actually computed the candidate embeddings yet, as the dataset functions are lazily evaluated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Index Embeddings </span>\n",
    "\n",
    "Next we index these embeddings. We start by connecting to our project's OpenSearch client using the *hopsworks* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "opensearch_api = project.get_opensearch_api()\n",
    "client = OpenSearch(**opensearch_api.get_default_py_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an index called `candidate_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = opensearch_api.get_project_index(\"candidate_index\")\n",
    "\n",
    "emb_dim = 16 # candidate_model.layers[-1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the HNSW (Hierarchical Navigable Small World) data structure, which can be thought of as a skip list for graphs.\n",
    "\n",
    "See the [OpenSearch documentation](https://opensearch.org/docs/latest/search-plugins/knn/knn-index) for more detailed information about parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To delete the indices\n",
    "# response = client.indices.delete(\n",
    "#     index = index_name\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dimensionality of candidate embeddings.\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"knn\": True,\n",
    "        \"knn.algo_param.ef_search\": 100,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"my_vector1\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": emb_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.indices.create(index_name, body=index_body)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally insert our candidate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy.helpers import bulk\n",
    "\n",
    "actions = []\n",
    "for batch in candidate_embeddings:\n",
    "    item_id_list, embedding_list = batch\n",
    "    item_id_list = item_id_list.numpy().astype(int)\n",
    "    embedding_list = embedding_list.numpy()\n",
    "\n",
    "    for item_id, embedding in zip(item_id_list, embedding_list):\n",
    "        actions.append({\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": item_id,\n",
    "            \"_source\": {\n",
    "                \"my_vector1\": embedding,\n",
    "            }\n",
    "        })\n",
    "\n",
    "# Bulk insertion.\n",
    "bulk(client, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that it works we can retrieve the neighbors of a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.random.rand(emb_dim)\n",
    "\n",
    "query = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"my_vector1\": {\n",
    "        \"vector\": embedding,\n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27\">‚è©Ô∏è Next Steps </span>\n",
    "\n",
    "At this point we have a recommender system that is able to generate a set of candidate items for a customer. However, many of these could be poor, as the candidate model was trained with only a few subset of the features. In the next notebook, we'll create a ranking dataset to train a *ranking model* to do more fine-grained predictions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}