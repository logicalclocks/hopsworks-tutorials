{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb60450",
   "metadata": {},
   "source": [
    "## Feature Groups\n",
    "\n",
    "In this series of tutorials, we will work with data related to credit card transactions. The end goal is to train and serve a model on the Hopsworks platform that can predict whether a credit card transaction is fraudulent or not.\n",
    "\n",
    "In this particular notebook you will learn how to:\n",
    "- Connect to the Hopsworks feature store.\n",
    "- Create feature groups and upload them to the feature store.\n",
    "\n",
    "![tutorial-flow](images/online_offline_fs.png)\n",
    "\n",
    "First of all we will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b143e",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data we will use comes from three different CSV files:\n",
    "\n",
    "- `credit_cards.csv`: credit card information such as expiration date and provider.\n",
    "- `transactions.csv`: transaction information such as timestamp, location, and the amount. Importantly, the binary `fraud_label` variable tells us whether a transaction was fraudulent or not.\n",
    "- `profiles.csv`: credit card user information such as birthdate and city of residence.\n",
    "\n",
    "We can conceptualize these CSV files as originating from separate data sources. All three files have a credit card number column `cc_num` in common, which we can use for joins.\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed97fd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>provider</th>\n",
       "      <th>expires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4796807885357879</td>\n",
       "      <td>visa</td>\n",
       "      <td>05/23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4529266636192966</td>\n",
       "      <td>visa</td>\n",
       "      <td>03/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4922690008243953</td>\n",
       "      <td>visa</td>\n",
       "      <td>02/27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cc_num provider expires\n",
       "0  4796807885357879     visa   05/23\n",
       "1  4529266636192966     visa   03/22\n",
       "2  4922690008243953     visa   02/27"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pandas\n",
    "                                            \n",
    "credit_cards_df = pandas.read_csv(\"/Projects/{}/Jupyter/hopsworks-tutorials/data/credit_cards.csv\".format(hdfs.project_name()))\n",
    "credit_cards_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b97772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>mail</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>cc_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Catherine Zimmerman</td>\n",
       "      <td>F</td>\n",
       "      <td>valenciajason@hotmail.com</td>\n",
       "      <td>1988-09-20</td>\n",
       "      <td>Bryn Mawr-Skyway</td>\n",
       "      <td>US</td>\n",
       "      <td>4796807885357879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael Williams</td>\n",
       "      <td>M</td>\n",
       "      <td>brettkennedy@yahoo.com</td>\n",
       "      <td>1977-03-01</td>\n",
       "      <td>Gates-North Gates</td>\n",
       "      <td>US</td>\n",
       "      <td>4529266636192966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica Krueger</td>\n",
       "      <td>F</td>\n",
       "      <td>marthacruz@hotmail.com</td>\n",
       "      <td>1947-09-10</td>\n",
       "      <td>Greenfield</td>\n",
       "      <td>US</td>\n",
       "      <td>4922690008243953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name sex                       mail  birthdate  \\\n",
       "0  Catherine Zimmerman   F  valenciajason@hotmail.com 1988-09-20   \n",
       "1     Michael Williams   M     brettkennedy@yahoo.com 1977-03-01   \n",
       "2      Jessica Krueger   F     marthacruz@hotmail.com 1947-09-10   \n",
       "\n",
       "                City Country            cc_num  \n",
       "0   Bryn Mawr-Skyway      US  4796807885357879  \n",
       "1  Gates-North Gates      US  4529266636192966  \n",
       "2         Greenfield      US  4922690008243953  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_df = pandas.read_csv(\"/Projects/{}/Jupyter/hopsworks-tutorials/data/profiles.csv\".format(hdfs.project_name()), parse_dates=[\"birthdate\"])\n",
    "profiles_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb4efde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>fraud_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11df919988c134d97bbff2678eb68e22</td>\n",
       "      <td>2022-01-01 00:00:24</td>\n",
       "      <td>4473593503484549</td>\n",
       "      <td>Health/Beauty</td>\n",
       "      <td>62.95</td>\n",
       "      <td>42.30865</td>\n",
       "      <td>-83.48216</td>\n",
       "      <td>Canton</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd0b2d6d4266ccd3bf05bc2ea91cf180</td>\n",
       "      <td>2022-01-01 00:00:56</td>\n",
       "      <td>4272465718946864</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>85.45</td>\n",
       "      <td>33.52253</td>\n",
       "      <td>-117.70755</td>\n",
       "      <td>Laguna Niguel</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e627f5d9a9739833bd52d2da51761fc3</td>\n",
       "      <td>2022-01-01 00:02:32</td>\n",
       "      <td>4104216579248948</td>\n",
       "      <td>Domestic Transport</td>\n",
       "      <td>21.63</td>\n",
       "      <td>37.60876</td>\n",
       "      <td>-77.37331</td>\n",
       "      <td>Mechanicsville</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid            datetime            cc_num  \\\n",
       "0  11df919988c134d97bbff2678eb68e22 2022-01-01 00:00:24  4473593503484549   \n",
       "1  dd0b2d6d4266ccd3bf05bc2ea91cf180 2022-01-01 00:00:56  4272465718946864   \n",
       "2  e627f5d9a9739833bd52d2da51761fc3 2022-01-01 00:02:32  4104216579248948   \n",
       "\n",
       "             category  amount  latitude  longitude            city country  \\\n",
       "0       Health/Beauty   62.95  42.30865  -83.48216          Canton      US   \n",
       "1             Grocery   85.45  33.52253 -117.70755   Laguna Niguel      US   \n",
       "2  Domestic Transport   21.63  37.60876  -77.37331  Mechanicsville      US   \n",
       "\n",
       "   fraud_label  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_df = pandas.read_csv(\"/Projects/{}/Jupyter/hopsworks-tutorials/data/transactions.csv\".format(hdfs.project_name()),parse_dates=[\"datetime\"])\n",
    "trans_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eee538",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Fraudulent transactions can differ from regular ones in many different ways. Typical red flags would for instance be a large transaction volume/frequency in the span of a few hours. It could also be the case that elderly people in particular are targeted by fraudsters. To facilitate model learning we will create additional features based on these patterns. In particular, we will create two types of features:\n",
    "1. Features that aggregate data from different data sources. This could for instance be the age of a customer at the time of a transaction, which combines the `birthdate` feature from `profiles.csv` with the `datetime` feature from `transactions.csv`.\n",
    "2. Features that aggregate data from multiple time steps. An example of this could be the transaction frequency of a credit card in the span of a few hours, which is computed using a window function.\n",
    "\n",
    "Let's start with the first category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a421361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_at_transaction</th>\n",
       "      <th>days_until_card_expires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97.513297</td>\n",
       "      <td>1460.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.752919</td>\n",
       "      <td>1733.999352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.899681</td>\n",
       "      <td>242.998241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.526088</td>\n",
       "      <td>150.997639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.005059</td>\n",
       "      <td>515.997280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_at_transaction  days_until_card_expires\n",
       "0           97.513297              1460.999722\n",
       "1           33.752919              1733.999352\n",
       "2           80.899681               242.998241\n",
       "3           53.526088               150.997639\n",
       "4           46.005059               515.997280"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute age at transaction.\n",
    "age_df = trans_df.merge(profiles_df, on=\"cc_num\", how=\"left\")\n",
    "trans_df[\"age_at_transaction\"] = (age_df[\"datetime\"] - age_df[\"birthdate\"]) / np.timedelta64(1, \"Y\")\n",
    "\n",
    "# Compute days until card expires.\n",
    "card_expiry_df = trans_df.merge(credit_cards_df, on=\"cc_num\", how=\"left\")\n",
    "card_expiry_df[\"expires\"] = pd.to_datetime(card_expiry_df[\"expires\"], format=\"%m/%y\")\n",
    "trans_df[\"days_until_card_expires\"] = (card_expiry_df[\"expires\"] - card_expiry_df[\"datetime\"]) / np.timedelta64(1, \"D\")\n",
    "\n",
    "trans_df[[\"age_at_transaction\", \"days_until_card_expires\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75945534",
   "metadata": {},
   "source": [
    "Next, we create features that for each credit card aggregate data from multiple time steps.\n",
    "\n",
    "We start by computing the distance between consecutive transactions, which we will call `loc_delta`.\n",
    "Here we use the [Haversine distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html?highlight=haversine#sklearn.metrics.pairwise.haversine_distances) to quantify the distance between two longitude and latitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445ae3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians\n",
    "\n",
    "# Do some simple preprocessing.\n",
    "trans_df.sort_values(\"datetime\", inplace=True)\n",
    "trans_df[[\"longitude\", \"latitude\"]] = trans_df[[\"longitude\", \"latitude\"]].applymap(radians)\n",
    "\n",
    "def haversine(long, lat):\n",
    "    \"\"\"Compute Haversine distance between each consecutive coordinate in (long, lat).\"\"\"\n",
    "\n",
    "    long_shifted = long.shift()\n",
    "    lat_shifted = lat.shift()\n",
    "    long_diff = long_shifted - long\n",
    "    lat_diff = lat_shifted - lat\n",
    "\n",
    "    a = np.sin(lat_diff/2.0)**2\n",
    "    b = np.cos(lat) * np.cos(lat_shifted) * np.sin(long_diff/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a + b))\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "trans_df[\"loc_delta\"] = trans_df.groupby(\"cc_num\")\\\n",
    "    .apply(lambda x : haversine(x[\"longitude\"], x[\"latitude\"]))\\\n",
    "    .reset_index(level=0, drop=True)\\\n",
    "    .fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688033f3",
   "metadata": {},
   "source": [
    "Next we compute windowed aggregates. Here we will use 4-hour windows, but feel free to experiment with different window lengths by setting `window_len` below to a value of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddda2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = \"4h\"\n",
    "cc_group = trans_df[[\"cc_num\", \"amount\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\")\n",
    "\n",
    "# Moving average of transaction volume.\n",
    "df_4h_mavg = pd.DataFrame(cc_group.mean())\n",
    "df_4h_mavg.columns = [\"trans_volume_mavg\", \"datetime\"]\n",
    "df_4h_mavg = df_4h_mavg.reset_index(level=[\"cc_num\"])\n",
    "df_4h_mavg = df_4h_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_mavg = df_4h_mavg.sort_index()\n",
    "\n",
    "# Moving standard deviation of transaction volume.\n",
    "df_4h_std = pd.DataFrame(cc_group.mean())\n",
    "df_4h_std.columns = [\"trans_volume_mstd\", \"datetime\"]\n",
    "df_4h_std = df_4h_std.reset_index(level=[\"cc_num\"])\n",
    "df_4h_std = df_4h_std.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_std = df_4h_std.fillna(0)\n",
    "df_4h_std = df_4h_std.sort_index()\n",
    "window_aggs_df = df_4h_std.merge(df_4h_mavg,left_index=True, right_index=True)\n",
    "\n",
    "# Moving average of transaction frequency.\n",
    "df_4h_count = pd.DataFrame(cc_group.mean())\n",
    "df_4h_count.columns = [\"trans_freq\", \"datetime\"]\n",
    "df_4h_count = df_4h_count.reset_index(level=[\"cc_num\"])\n",
    "df_4h_count = df_4h_count.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_count = df_4h_count.sort_index()\n",
    "window_aggs_df = window_aggs_df.merge(df_4h_count,left_index=True, right_index=True)\n",
    "\n",
    "# Moving average of location difference between consecutive transactions.\n",
    "cc_group = trans_df[[\"cc_num\", \"loc_delta\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\").mean()\n",
    "df_4h_loc_delta_mavg = pd.DataFrame(cc_group)\n",
    "df_4h_loc_delta_mavg.columns = [\"loc_delta_mavg\", \"datetime\"]\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.reset_index(level=[\"cc_num\"])\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.sort_index()\n",
    "window_aggs_df = window_aggs_df.merge(df_4h_loc_delta_mavg,left_index=True, right_index=True)\n",
    "\n",
    "window_aggs_df = window_aggs_df.merge(trans_df[[\"cc_num\", \"datetime\"]].sort_index(),left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c79319",
   "metadata": {},
   "source": [
    "## TODO (Davit): There is an issue with date time objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c9021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.datetime = trans_df.datetime.values.astype(np.int64) // 10 ** 6\n",
    "window_aggs_df.datetime = window_aggs_df.datetime.values.astype(np.int64) // 10 ** 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a1401",
   "metadata": {},
   "source": [
    "## TODO (Davit): There is an issue with long type PK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4de73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_aggs_df.datetime = window_aggs_df.datetime.values.astype(np.int64) // 10 ** 6\n",
    "window_aggs_df['cc_num'] = window_aggs_df['cc_num'].astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fffc9b7d",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-11-fc9632f8a8ae> in <module>\n",
    "      6     stream=True,\n",
    "      7 )\n",
    "----> 8 window_aggs_fg.save(window_aggs_df)\n",
    "\n",
    "/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/feature_group.py in save(self, features, write_options)\n",
    "    859 \n",
    "    860         # fg_job is used only if the python engine is used\n",
    "--> 861         fg_job = self._feature_group_engine.save(self, feature_dataframe, write_options)\n",
    "    862         self._code_engine.save_code(self)\n",
    "    863         if self.statistics_config.enabled and engine.get_type() == \"spark\":\n",
    "\n",
    "/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/core/feature_group_engine.py in save(self, feature_group, feature_dataframe, write_options)\n",
    "     63         online_write_options = self.get_kafka_config(write_options)\n",
    "     64 \n",
    "---> 65         return engine.get_instance().save_dataframe(\n",
    "     66             feature_group,\n",
    "     67             feature_dataframe,\n",
    "\n",
    "/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/engine/python.py in save_dataframe(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\n",
    "    373     ):\n",
    "    374         if feature_group.stream:\n",
    "--> 375             return self._write_dataframe_kafka(\n",
    "    376                 feature_group, dataframe, offline_write_options\n",
    "    377             )\n",
    "\n",
    "/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/engine/python.py in _write_dataframe_kafka(self, feature_group, dataframe, offline_write_options)\n",
    "    671             # encode feature row\n",
    "    672             with BytesIO() as outf:\n",
    "--> 673                 writer(row, outf)\n",
    "    674                 encoded_row = outf.getvalue()\n",
    "    675 \n",
    "\n",
    "/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/engine/python.py in <lambda>(record, outf)\n",
    "    721             schema = json.loads(writer_schema)\n",
    "    722             parsed_schema = parse_schema(schema)\n",
    "--> 723             return lambda record, outf: schemaless_writer(outf, parsed_schema, record)\n",
    "    724 \n",
    "    725         parsed_schema = avro.schema.parse(writer_schema)\n",
    "\n",
    "fastavro/_write.pyx in fastavro._write.schemaless_writer()\n",
    "\n",
    "fastavro/_write.pyx in fastavro._write.write_data()\n",
    "\n",
    "fastavro/_write.pyx in fastavro._write.write_record()\n",
    "\n",
    "fastavro/_write.pyx in fastavro._write.write_data()\n",
    "\n",
    "fastavro/_write.pyx in fastavro._write.write_union()\n",
    "\n",
    "ValueError: 4473593503484549.0 (type <class 'float'>) do not match ['null', 'long'] on field cc_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4a1f6",
   "metadata": {},
   "source": [
    "#### Creating Feature Groups\n",
    "\n",
    "A [feature group](https://docs.hopsworks.ai/feature-store-api/latest/generated/feature_group/) can be seen as a collection of conceptually related features. In our case, we will create a feature group for the transaction data and a feature group for the windowed aggregations on the transaction data. Both will have `tid` as primary key, which will allow us to join them when creating a dataset in the next tutorial.\n",
    "\n",
    "Feature groups can also be used to define a namespace for features. For instance, in a real-life setting we would likely want to experiment with different window lengths. In that case, we can create feature groups with identical schema for each window length. \n",
    "\n",
    "Before we can create a feature group we need to connect to our feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9711f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d35f8",
   "metadata": {},
   "source": [
    "## TODO explain streaming fg and event_time enabled FG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1c6e9",
   "metadata": {},
   "source": [
    "To create a feature group we need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group.\n",
    "\n",
    "Here we have also set `online_enabled=True`, which enables low latency access to the data. A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group).\n",
    "\n",
    "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent we populate it with its associated data using the `save` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade10767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/p/119/jobs/named/transactions_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating feature group `transactions`, incremented version to `1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsfs.core.job.Job at 0x7fb274cc54c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_fg = fs.create_feature_group(\n",
    "    name=\"transactions\",\n",
    "    description=\"Transaction data.\",\n",
    "    primary_key=['cc_num'],\n",
    "    event_time=['datetime'],\n",
    "    online_enabled=True,\n",
    "    stream=True,\n",
    ")\n",
    "trans_fg.save(trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305dfdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/p/119/jobs/named/transactions_4h_aggs_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating feature group `transactions_4h_aggs`, incremented version to `1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsfs.core.job.Job at 0x7fb2742f4fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_aggs_fg = fs.create_feature_group(\n",
    "    name=f\"transactions_{window_len}_aggs\",\n",
    "    description=f\"Aggregate transaction data over {window_len} windows.\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    event_time=['datetime'],    \n",
    "    online_enabled=True,    \n",
    "    stream=True,\n",
    ")\n",
    "window_aggs_fg.save(window_aggs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47dae0",
   "metadata": {},
   "source": [
    "You should now be able to inspect the feature groups in the Hopsworks UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee16ec",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook we will use our feature groups to create a dataset we can train a model on."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}