{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration, Feature Engineering and Experimentation\n",
    "\n",
    "Now we're to the fun part - the Data Science. Now that the data engineers have cleaned and loaded the data to the `trips` table, we can begin our model development. For this, we will leverage Snowpark to do the **feature preparation and exploratory analysis**.   This dataset is initially ~100 million rows and is likely too large to fit into memory on our local machine or even a reasonable sized single VM in the cloud. The Snowpark Python client-side Dataframe API \n",
    "allows us to push-down most of the computation for preparation and feature engineering to Snowpark. For security and goverance reasons we can read data into memory for model training and inference but no intermediate data products can be stored outside of Snowflake.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: Data in `trips` table.   \n",
    "Output: Feature engineering logic.  Train function.  Predict function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we will rewind in time and assume that it is January 1, 2020.  With the bulk ingest we have 92M records from 2013 to January 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.WARN)\n",
    "#logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict['trips_table_name']='TRIPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(state_dict['trips_table_name']).select(F.min('STARTTIME'), F.max('STARTTIME')).show()\n",
    "session.table(state_dict['trips_table_name']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too large to fit in memory on my local system.  Lets summarize trips to daily resolution and inspect the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf = session.table(state_dict['trips_table_name'])\n",
    "snowdf.with_column('DATE', F.to_date('STARTTIME')).group_by('DATE').count().sort('DATE').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we aggregate the data at the day level we have a small enough dataset to fit in memory.  But we may want to provide a more granular time series (ie. hour or minute-level) or perhaps our data will grow considerably over time.  In either case we can't rely on in-memory computation and will want to push-down as much computation as possible to Snowflake.  \n",
    "  \n",
    "For exploration purposes we can see a good daily and annual seasonality in the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = snowdf.with_column(\"date\", F.to_date(\"STARTTIME\")).group_by(\"date\").count().sort(\"date\").to_pandas()\n",
    "df.head()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(x='DATE', y='COUNT', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not be able to get a good model that can predict across ALL stations.  Lets start with just the busiest station(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf.filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "      .group_by('START_STATION_ID') \\\n",
    "      .count() \\\n",
    "      .sort('COUNT', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we will build with the busiest station \"Central Park S & 6 Ave\" which is STATION_ID=519.  Later we will see how Snowpark Python allows us to tap into the powerful horizontally, scalable compute engine to parallelize this across all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stations = snowdf.filter(F.col('START_STATION_ID').is_not_null()) \\\n",
    "                                                   .groupBy('START_STATION_ID') \\\n",
    "                                                   .count() \\\n",
    "                                                   .sort('COUNT', ascending=False) \\\n",
    "                                                   .to_pandas()['START_STATION_ID'].values.tolist()\n",
    "top_stations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowdf.filter(F.col('START_STATION_ID') == top_stations[0]) \\\n",
    "      .withColumn('DATE', \n",
    "                  F.call_builtin('DATE_TRUNC', ('DAY', F.col('STARTTIME')))) \\\n",
    "      .groupBy('DATE') \\\n",
    "      .count() \\\n",
    "      .sort('DATE').to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.lineplot(x='DATE', y='COUNT', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what looks like daily, weekly and annual periodicity as well as a slight upward trend over years. To start building the features for a regression model we might start with lag features. We can use autocorrelation to confirm our initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rc(\"figure\", figsize=(10, 7))\n",
    "plot_acf(df['COUNT'], lags=400)\n",
    "plt.xlabel('Lags', fontsize=12)\n",
    "plt.ylabel('Autocorrelation', fontsize=12)\n",
    "plt.title('Autocorrelation of Trip Count Seasonality', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see the strong annual seasonality.  Lets look closer at the daily and weekly lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(10, 7))\n",
    "plot_acf(df['COUNT'], lags=[1, 7, 30, 60, 90, 365])\n",
    "plt.xlabel('Lags', fontsize=12)\n",
    "plt.ylabel('Autocorrelation', fontsize=12)\n",
    "plt.title('Autocorrelation of Trip Count Seasonality', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, lags on 1, 7, 90 and 365 days have the strongest positive correlation.\n",
    "\n",
    "Snowpark has many [functions](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.functions.html) for things like transformation, statistical analysis, etc. We will use the `lag()` window function to generate lag features very quickly on this very large dataset.\n",
    "  \n",
    "NOTE: In addition to the rich set of Snowpark functions, users can also tap into the wealth of [Snowflake built-in functions](https://docs.snowflake.com/en/sql-reference/functions-all.html) using the `call_builtin()` function. \n",
    "\n",
    "Here we will generate our features in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(snowdf):\n",
    "    \n",
    "    snowdf = snowdf.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                           F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                   .group_by(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                   .count()\n",
    "    \n",
    "    #Impute missing values for lag columns using mean of the previous period.\n",
    "    mean_1 = round(snowdf.sort('DATE').limit(1).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_7 = round(snowdf.sort('DATE').limit(7).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_90 = round(snowdf.sort('DATE').limit(90).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_365 = round(snowdf.sort('DATE').limit(365).select(F.mean('COUNT')).collect()[0][0])\n",
    "\n",
    "    date_win = snp.Window.order_by('DATE')\n",
    "\n",
    "    snowdf = snowdf.with_column('LAG_1', F.lag('COUNT', offset=1, default_value=mean_1) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_7', F.lag('COUNT', offset=7, default_value=mean_7) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_90', F.lag('COUNT', offset=90, default_value=mean_90) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .with_column('LAG_365', F.lag('COUNT', offset=365, default_value=mean_365) \\\n",
    "                                         .over(date_win)) \\\n",
    "                   .na.drop()\n",
    "    return snowdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function then becomes our feature generation logic and creates essentially a DAG of aggregations and transformations which gets executed _lazily_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = generate_features(snowdf.filter(F.col('START_STATION_ID') == top_stations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call something this `show()` ALL of the processing gets pushed down into Snowflake compute for highly optimized execution.  In the end the function provides a running list of transformations to build our feature pipeline for inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "We might start a baseline model with ARIMA.  In this case we get an MSE of something like 56983.  See notebook 02_Data_Science_ARIMA_Baseline.ipynb for details.\n",
    "\n",
    "Lets try pytorch tabnet to see if we can at least get this level of predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Development and Experimentation\n",
    "We will use [pytorch_tabnet](https://github.com/dreamquark-ai/tabnet) from the 2019 [tabnet paper](https://arxiv.org/pdf/1908.07442.pdf) by Arik, S. O. and Pfister, T.  Tabnet is a powerful deep learning framework for attentive, interpretable learning on tabular data.  Rather than substantial focus on hyper-parameter optimization  we will start with an initial set of hyper-parameters and focus on iterating over input features for now.  \n",
    "  \n",
    "Rather than a random split of training/validation data we will split the training dataset using a `cutpoint`.  For example we will save the final 365 days of the dataset as validation and train with the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, cutpoint=365, cat_idxs=[], cat_dims=[]):    \n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    import pandas as pd\n",
    "\n",
    "    X_valid = X[-cutpoint:]\n",
    "    y_valid = y[-cutpoint:]\n",
    "    X_train = X[:-cutpoint]\n",
    "    y_train = y[:-cutpoint]\n",
    "\n",
    "    max_epochs = 100\n",
    "    \n",
    "    batch_df = pd.DataFrame(range(2,65,2), columns=['batch_size'])\n",
    "    batch_df['batch_remainder'] = len(X_train)%batch_df['batch_size']\n",
    "    optimal_batch_size=int(batch_df['batch_size'].where(batch_df['batch_remainder']==batch_df['batch_remainder'].min()).max())\n",
    "    \n",
    "    print('Selected batch size '+str(optimal_batch_size)+' for input data size: '+str(len(X_train)))\n",
    "    \n",
    "    regression_model = TabNetRegressor(cat_idxs=cat_idxs, cat_dims=cat_dims)\n",
    "\n",
    "    regression_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=10,\n",
    "        batch_size=optimal_batch_size, \n",
    "        virtual_batch_size=optimal_batch_size/2,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "    \n",
    "    return regression_model\n",
    "\n",
    "def predict(model, X):\n",
    "    y_hat = model.predict(X).reshape(-1)\n",
    "    return y_hat\n",
    "\n",
    "def forecast(df, model, lag_values:list, steps:int):\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "    forecast_df=df.copy()\n",
    "    for i in range(steps):\n",
    "        station_id = forecast_df['STATION_ID'][-1:].values[0]\n",
    "        future_date = forecast_df['DATE'][-1:].values[0]+timedelta(days=1)\n",
    "        lags=[forecast_df['COUNT'].shift(i-1)[-1:].values[0] for i in lag_values]\n",
    "        pred=round(model.predict(np.array([lags]))[0][0])\n",
    "        row=[*[station_id, future_date, pred], *lags, pred]\n",
    "        forecast_df.loc[len(forecast_df)]=row\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "def plot(df, x_lab:str, y_true_lab:str, y_pred_lab:str):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    df = pd.melt(df, id_vars=[x_lab], value_vars=[y_true_lab, y_pred_lab])\n",
    "    ax = sns.lineplot(x=x_lab, y='value', hue='variable', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a pandas dataframe using to_pandas() which will generate the features as a pyarrow dataset and efficiently read them into memory in pandas locally.  \n",
    "  \n",
    "Let's train our first model to get a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_predict(df, cat_idxs=[], cat_dims=[], lag_values=[], forecast_steps=None):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    target = ['COUNT']\n",
    "    feature_columns = [feature.replace('\\\"', '') for feature in df.columns]\n",
    "    feature_columns.remove(target[0])\n",
    "    feature_columns.remove('DATE')\n",
    "    feature_columns.remove('STATION_ID')\n",
    "\n",
    "    model = train(df[feature_columns].astype(float).values, \n",
    "                  df[target].values,\n",
    "                  cat_idxs=cat_idxs, \n",
    "                  cat_dims=cat_dims)\n",
    "    df['Y_PRED'] = predict(model, df[feature_columns].astype(float).values).astype('int')\n",
    "    \n",
    "    if isinstance(forecast_steps, int):\n",
    "        df = forecast(df, model, lag_values=lag_values, steps=forecast_steps)\n",
    "\n",
    "    explain_df = pd.DataFrame(model.explain(df[feature_columns].astype(float).values)[0], \n",
    "                         columns = feature_columns).add_prefix('EXPL_').round(2)\n",
    "    df = pd.concat([df.set_index('DATE').reset_index(), explain_df], axis=1)\n",
    "    \n",
    "    MSE = mean_squared_error(y_pred=df['Y_PRED'], y_true=df[target])\n",
    "    display(\"Error for training dataset is: \"+str(MSE))\n",
    "    return df.reset_index(drop=True), model, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, lag_values=[1,7,90,365], forecast_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df[-1095:], 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we add US holidays as a feature?\n",
    "Pandas has great support for holiday lists.  We can generate a pandas dataframe and then upload it as a temporary table to create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "cal = USFederalHolidayCalendar()\n",
    "\n",
    "#generate a feature of 20 years worth of US holiday days.\n",
    "start_date = datetime.strptime('2013-01-01', '%Y-%m-%d')\n",
    "end_date = start_date+timedelta(days=365*20)\n",
    "\n",
    "holiday_df = pd.DataFrame(cal.holidays(start=start_date, end=end_date), columns=['DATE'])\n",
    "holiday_df['DATE'] = holiday_df['DATE'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-05-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2032-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2032-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2032-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2032-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2032-12-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE\n",
       "0    2013-01-01\n",
       "1    2013-01-21\n",
       "2    2013-02-18\n",
       "3    2013-05-27\n",
       "4    2013-07-04\n",
       "..          ...\n",
       "207  2032-09-06\n",
       "208  2032-10-11\n",
       "209  2032-11-11\n",
       "210  2032-11-25\n",
       "211  2032-12-24\n",
       "\n",
       "[212 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holiday_df(session):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    from datetime import timedelta, datetime\n",
    "\n",
    "    cal = USFederalHolidayCalendar()\n",
    "\n",
    "    #generate a feature of 20 years worth of US holiday days.\n",
    "    start_date = datetime.strptime('2013-01-01', '%Y-%m-%d')\n",
    "    end_date = start_date+timedelta(days=365*20)\n",
    "\n",
    "    holiday_df = pd.DataFrame(cal.holidays(start=start_date, end=end_date), columns=['DATE'])\n",
    "    holiday_df['DATE'] = holiday_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    session.create_dataframe(holiday_df) \\\n",
    "           .with_column(\"HOLIDAY\", F.lit(1))\\\n",
    "           .write\\\n",
    "           .save_as_table(\"HOLIDAYS\", mode=\"overwrite\", create_temp_table=True)\n",
    "    \n",
    "    return session.table(\"HOLIDAYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = generate_holiday_df(session)\n",
    "holiday_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create an in-memory instance of the holiday dataframe.  In production we probably want to materialize this feature as a table or view.  We'll see later how that is easy to do but for now we have a function to generate it and join it to our training dataframe as a one-hot feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we join there is a possibility of losing data from our feature set if the join column doesn't cover the same dates.  So its good to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = train_snowdf.join(holiday_df, 'DATE', join_type='left') \\\n",
    "                           .na.fill({'HOLIDAY':0}) \\\n",
    "                           .sort('DATE', ascending=True)\n",
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train we need to specify this new holiday feature as a categorical feature.  Its already encoded (by definition) so nothing to add there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-1], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhancing Model Accuracy With Weather Data\n",
    "\n",
    "Let's see how we can make our model even better with weather data. Its likely that weather and the amount of precipitation on the day will be an important signal for our model. \n",
    "  \n",
    "We might start by downloading weather data or setting up an API.  But this is a lot of work when we want to build inference pipelines later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_weather_df(session):\n",
    "#     from snowflake.snowpark import Window\n",
    "#     from snowflake.snowpark import functions as F \n",
    "#     import pandas as pd\n",
    "\n",
    "#     tempdf = pd.read_csv('./include/weather.csv')\n",
    "#     tempdf['DATE']=pd.to_datetime(tempdf['dt_iso'].str.replace(' UTC', ''), \n",
    "#                                  format='%Y-%m-%d %H:%M:%S %z', \n",
    "#                                  utc=True).dt.tz_convert('America/New_York').dt.date\n",
    "#     tempdf.columns=tempdf.columns.str.upper()\n",
    "        \n",
    "#     session.create_dataframe(tempdf[['DATE','RAIN_1H', 'TEMP']]) \\\n",
    "#            .group_by('DATE').agg([F.round(F.mean('RAIN_1H'), 2).alias('PRECIP'),\n",
    "#                                  F.round(F.mean('TEMP')-F.lit(273.15), 2).alias('TEMP')])\\\n",
    "#            .fillna({'PRECIP':0, 'TEMP':0})\\\n",
    "#            .write\\\n",
    "#            .save_as_table(\"WEATHER\", mode=\"overwrite\", create_temp_table=True)\n",
    "\n",
    "#     return session.table('WEATHER')\n",
    "\n",
    "# weather_df = generate_weather_df(session)\n",
    "# weather_df.show()\n",
    "# weather_df.select(F.min('DATE'), F.max('DATE')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Use the [Snowflake Data Marketplace](https://www.snowflake.com/data-marketplace/?_sft_dataset-category=weather)  \n",
    "The Snowflake marketplace allows us to very easily access weather data (among many other types of data) and join it with the trips data.  Not only is this faster and more scalable than building from a csv file or an API, it is extremely easy to setup an MLops pipeline to keep it fresh.  \n",
    "  \n",
    "For this demo we will use a dataset from [Weather Source](https://app.snowflake.com/marketplace/listing/GZSOZ1LLE9).  \n",
    "  \n",
    "Weather Source is a leading provider of global weather and climate data and our OnPoint Product Suite provides businesses with the necessary weather and climate data to quickly generate meaningful and actionable insights for a wide range of use cases across industries.\n",
    "\n",
    "**See notebook 02_Data_Marketplace.ipynb for details on accessing this data**.\n",
    "\n",
    "Alternatively, run the following to subscribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_listing_id = state_dict['weather_listing_id']\n",
    "weather_database_name = state_dict['weather_database_name']\n",
    "\n",
    "session.sql(\"CREATE OR REPLACE DATABASE \"+weather_database_name+\" FROM SHARE \"+weather_listing_id).collect()\n",
    "\n",
    "#The default database and schema may be reset after creating a new database\n",
    "session.use_database(state_dict['connection_parameters']['database'])\n",
    "session.use_schema(state_dict['connection_parameters']['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_weather_df(session, weather_table_name):\n",
    "\n",
    "    weather_df = session.table(weather_table_name)\\\n",
    "                       .filter(F.col('POSTAL_CODE') == '10007')\\\n",
    "                       .select(F.col('DATE_VALID_STD').alias('DATE'), \n",
    "                               F.col('TOT_PRECIPITATION_MM').alias('PRECIP'), \n",
    "                               F.round(F.col('AVG_TEMPERATURE_FEELSLIKE_2M_C'), 2).alias('TEMP'))\\\n",
    "                       .sort('DATE', ascending=True)\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_table_name = weather_database_name+'.ONPOINT_ID.HISTORY_DAY'\n",
    "\n",
    "weather_df = generate_weather_df(session, weather_table_name)\n",
    "weather_df.show()\n",
    "weather_df.select(F.min('DATE'), F.max('DATE')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add weather features by joining the weather dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf = train_snowdf.join(weather_df, 'DATE', 'inner').sort('DATE', ascending=True)\n",
    "train_snowdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check the date range after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snowdf.select(F.min('DATE'), F.max('DATE')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-3], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the feature importance\n",
    "One reason we chose tabnet for this analysis is the built-in abilities for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([model.feature_importances_], columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict['holiday_table_name'] = 'HOLIDAYS'\n",
    "state_dict['weather_database_name'] = 'WEATHER_NYC'\n",
    "state_dict['weather_table_name'] = state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY'\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets consolidate and upate our feature functions and save them for the ML engineering team to operationalize.\n",
    "Based on the feature importance above we will go with 1 and 7 day lags only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/feature_engineering.py\n",
    "\n",
    "def generate_holiday_df(session, holiday_table_name:str):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    import pandas as pd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    from datetime import timedelta, datetime\n",
    "\n",
    "    cal = USFederalHolidayCalendar()\n",
    "\n",
    "    #generate a feature of 20 years worth of US holiday days.\n",
    "    start_date = datetime.strptime('2013-01-01', '%Y-%m-%d')\n",
    "    end_date = start_date+timedelta(days=365*20)\n",
    "\n",
    "    holiday_df = pd.DataFrame(cal.holidays(start=start_date, end=end_date), columns=['DATE'])\n",
    "    holiday_df['DATE'] = holiday_df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    session.create_dataframe(holiday_df) \\\n",
    "           .with_column(\"HOLIDAY\", F.lit(1))\\\n",
    "           .write\\\n",
    "           .save_as_table(holiday_table_name, mode=\"overwrite\", create_temp_table=True)\n",
    "    \n",
    "    return session.table(holiday_table_name)\n",
    "\n",
    "def generate_weather_df(session, weather_table_name):\n",
    "    from snowflake.snowpark import functions as F \n",
    "    return session.table(weather_table_name)\\\n",
    "                  .filter(F.col('POSTAL_CODE') == '10007')\\\n",
    "                  .select(F.col('DATE_VALID_STD').alias('DATE'), \n",
    "                          F.col('TOT_PRECIPITATION_MM').alias('PRECIP'), \n",
    "                          F.round(F.col('AVG_TEMPERATURE_FEELSLIKE_2M_C'), 2).alias('TEMP'))\\\n",
    "                  .sort('DATE', ascending=True)\n",
    "\n",
    "def generate_features(session, input_df, holiday_table_name, weather_table_name):\n",
    "    import snowflake.snowpark as snp\n",
    "    from snowflake.snowpark import functions as F \n",
    "    \n",
    "    #start_date, end_date = input_df.select(F.min('STARTTIME'), F.max('STARTTIME')).collect()[0][0:2]\n",
    "    \n",
    "    #check if features are already materialized (or in a temp table)\n",
    "    holiday_df = session.table(holiday_table_name)\n",
    "    try: \n",
    "        _ = holiday_df.columns\n",
    "    except:\n",
    "        holiday_df = generate_holiday_df(session, holiday_table_name)\n",
    "        \n",
    "    weather_df = session.table(weather_table_name)[['DATE','TEMP']]\n",
    "    try: \n",
    "        _ = weather_df.columns\n",
    "    except:\n",
    "        weather_df = generate_weather_df(session, weather_table_name)[['DATE','PRECIP','TEMP']]\n",
    "\n",
    "    feature_df = input_df.select(F.to_date(F.col('STARTTIME')).alias('DATE'),\n",
    "                                 F.col('START_STATION_ID').alias('STATION_ID'))\\\n",
    "                         .replace({'NULL': None}, subset=['STATION_ID'])\\\n",
    "                         .group_by(F.col('STATION_ID'), F.col('DATE'))\\\n",
    "                         .count()\n",
    "\n",
    "    #Impute missing values for lag columns using mean of the previous period.\n",
    "    mean_1 = round(feature_df.sort('DATE').limit(1).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_7 = round(feature_df.sort('DATE').limit(7).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_90 = round(feature_df.sort('DATE').limit(90).select(F.mean('COUNT')).collect()[0][0])\n",
    "    mean_365 = round(feature_df.sort('DATE').limit(365).select(F.mean('COUNT')).collect()[0][0])\n",
    "\n",
    "    date_win = snp.Window.order_by('DATE')\n",
    "\n",
    "    feature_df = feature_df.with_column('LAG_1', F.lag('COUNT', offset=1, default_value=mean_1) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .with_column('LAG_7', F.lag('COUNT', offset=7, default_value=mean_7) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .with_column('LAG_90', F.lag('COUNT', offset=90, default_value=mean_90) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .with_column('LAG_365', F.lag('COUNT', offset=365, default_value=mean_365) \\\n",
    "                                         .over(date_win)) \\\n",
    "                           .join(holiday_df, 'DATE', join_type='left').na.fill({'HOLIDAY':0}) \\\n",
    "                           .join(weather_df, 'DATE', 'inner') \\\n",
    "                          .na.drop() \n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train and evaluate the model with the final feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "from dags.feature_engineering import generate_features\n",
    "from snowflake.snowpark import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "session, state_dict = snowpark_connect()\n",
    "test_station_id = '519'\n",
    "\n",
    "snowdf = session.table(state_dict['trips_table_name']).filter(F.col('START_STATION_ID') == test_station_id)\n",
    "\n",
    "train_snowdf = generate_features(session=session, \n",
    "                                 input_df=snowdf, \n",
    "                                 holiday_table_name=state_dict['holiday_table_name'], \n",
    "                                 weather_table_name=state_dict['weather_table_name'])\n",
    "\n",
    "df = train_snowdf.sort('DATE', ascending=True).to_pandas()\n",
    "df, model, feature_columns = train_predict(df, cat_idxs=[-3], cat_dims=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df, 'DATE', 'COUNT', 'Y_PRED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([model.feature_importances_], columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
