{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Training Dataset and Modeling</span>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/churn/2_training_dataset_and_modeling.ipynb)\n",
    "\n",
    "This is the second part of the quick start series of tutorials about predicting customers that are at risk of churning with the Hopsworks Feature Store.\n",
    "\n",
    "This notebook explains how to read from a feature group and create training dataset within the feature store.\n",
    "\n",
    "You will train the model using XGBoost model, although it could just as well be trained with other machine learning frameworks such as Scikit-learn, PySpark, TensorFlow, and PyTorch. You will also perform some of the exploration that can be done in Hopsworks, notably the search functions and the lineage.\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Select the features you want to train the model on.\n",
    "2. Preprocess of features.\n",
    "3. Create a dataset split for training and validation data.\n",
    "4. Load the training data.\n",
    "5. Train the model.\n",
    "6. Explore feature groups and views via the UI.\n",
    "\n",
    "![tutorial-flow](../images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You will start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature groups\n",
    "customer_info_fg = fs.get_feature_group(\n",
    "    name=\"customer_info\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "demography_fg = fs.get_feature_group(\n",
    "    name=\"customer_demography_info\",\n",
    "    version=1\n",
    ")\n",
    "\n",
    "subscriptions_fg = fs.get_feature_group(\n",
    "    name=\"customer_subscription_info\",\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "query = customer_info_fg.select_except([\"customerid\"]).join(demography_fg.select_except([\"customerid\"])).join(subscriptions_fg.select_all())\n",
    "\n",
    "# uncomment this if you would like to view query result\n",
    "# query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you created three feature groups in the previous notebook. If you had created multiple feature groups with identical schema and wanted to include them in the join you would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">ü§ñ Transformation Functions </span>\n",
    "\n",
    "You will preprocess the data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you will simply define a mapping between features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "numerical_features = [\"tenure\", \"monthlycharges\", \"totalcharges\"]\n",
    "categorical_features = [\n",
    "    \"multiplelines\", \"internetservice\", \"onlinesecurity\", \"onlinebackup\",\n",
    "    \"deviceprotection\", \"techsupport\", \"streamingmovies\", \"streamingtv\",\n",
    "    \"phoneservice\", \"paperlessbilling\", \"contract\", \"paymentmethod\", \"gender\", \n",
    "    \"dependents\", \"partner\"\n",
    "]\n",
    "\n",
    "# Map features to transformations\n",
    "transformation_functions = {}\n",
    "for feature in numerical_features:\n",
    "    transformation_functions[feature] = min_max_scaler\n",
    "\n",
    "for feature in categorical_features:\n",
    "    transformation_functions[feature] = label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View you may use `fs.get_or_create_feature_view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "        name = 'churn_feature_view',\n",
    "        version = 1,\n",
    "        labels=[\"churn\"],\n",
    "        transformation_functions=transformation_functions,\n",
    "        query=query   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Dataset with train/test splits can be created using `fs.create_train_test_split()` method.\n",
    "Dataset with train/valid/test splits can be created using `fs.create_train_validation_test_split()` method.\n",
    "\n",
    "**You can use event time filters like `train_start`, `train_end`, `valid_start`, `valid_end`... Values can be either in unix, string and datetime format.** \n",
    "\n",
    "**Or, use `validation_size` and `test_size` parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_version, td_job = feature_view.create_train_validation_test_split(\n",
    "    description='churn_training_dataset_random_splitted',\n",
    "    data_format='csv',\n",
    "    validation_size=0.2,\n",
    "    test_size=0.1,\n",
    "    write_options={'wait_for_job': True},\n",
    "    coalesce=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view and training dataset are now visible in the UI\n",
    "\n",
    "![fv-overview](../churn/images/churn_tutofv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load Training Data </span>\n",
    "\n",
    "Now you need to fetch the training dataset that you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_split(\n",
    "    training_dataset_version=td_version\n",
    ")\n",
    "\n",
    "X_train.drop('customerid', axis=1, inplace=True)\n",
    "X_val.drop('customerid', axis=1, inplace=True)\n",
    "X_test.drop('customerid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution is skewed, which is good news for the company considering that customers at risk of churning make up smaller part of customer base. However, as a data scientist should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, you will use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (curn) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next you will train a model and set the bigger class weight for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "classifier = xgb.XGBClassifier(scale_pos_weight=3)\n",
    "\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> üë®üèª‚Äç‚öñÔ∏è Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üìù Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, classifier.predict(X_test)).astype(int)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_matrix, ['Non Churn', 'Churn'],\n",
    "                     ['Non Churn', 'Churn'])\n",
    "\n",
    "figure_cm = plt.figure(figsize = (10,7))\n",
    "figure_cm = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='.10g')\n",
    "\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üóÑ Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"churn_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "pkl_file_name = model_dir + '/churnmodel.pkl'\n",
    "\n",
    "joblib.dump(classifier, pkl_file_name)\n",
    "\n",
    "figure_cm.figure.savefig(model_dir + '/confusion_matrix.png')\n",
    "\n",
    "model = mr.python.create_model(\n",
    "    name=\"churnmodel\",\n",
    "    description = \"Churn Model\",\n",
    "    input_example = X_train.sample(),\n",
    "    model_schema = model_schema\n",
    ")\n",
    "\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'>üöÄ Fetch and test the model</span>\n",
    "\n",
    "Finally you can start making predictions with your model! To identify customers at risk of churn lets retrieve your churn prediction model from Hopsworks model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_model = mr.get_model(\n",
    "    name=\"churnmodel\",\n",
    "    version=1\n",
    ")\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_xgboost_model = joblib.load(saved_model_dir + \"/churnmodel.pkl\")\n",
    "retrieved_xgboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üîÆ  Use trained model to identify customers at risk of churn </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_preds(predictions):\n",
    "    return ['Churn' if pred == 1 else 'Not Churn' for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.init_batch_scoring(td_version)\n",
    "\n",
    "batch_data = feature_view.get_batch_data()\n",
    "\n",
    "batch_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the all for all customer data and then visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data.drop('customerid',axis = 1, inplace = True)\n",
    "\n",
    "predictions = retrieved_xgboost_model.predict(batch_data)\n",
    "predictions = transform_preds(predictions)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üë®üèª‚Äçüé® Prediction Visualisation</span>\n",
    "\n",
    "Now you got your predictions but you also would like to exlain predictions to make informed decisions. Lets visualise them and explain important features that influences on the risk of churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "\n",
    "# Recall that you applied transformation functions, such as min max scaler and laber encoder. \n",
    "# Now you want to transform them back to human readable format.\n",
    "df_all = batch_data.copy()\n",
    "td_transformation_functions = feature_view._batch_scoring_server._transformation_functions\n",
    "for feature_name in td_transformation_functions:\n",
    "    td_transformation_function = td_transformation_functions[feature_name]\n",
    "    sig, foobar_locals = inspect.signature(td_transformation_function.transformation_fn), locals()\n",
    "    param_dict = dict([(param.name, param.default) for param in sig.parameters.values() if param.default != inspect._empty])\n",
    "    if td_transformation_function.name == \"label_encoder\":\n",
    "        rev_dict = {v: k for k, v in param_dict[\"value_to_index\"].items()}\n",
    "        df_all[feature_name] = df_all[feature_name].map(lambda x: rev_dict[x])\n",
    "    if td_transformation_function.name == \"min_max_scaler\":\n",
    "        df_all[feature_name] = df_all[feature_name].map(lambda x: x*(param_dict[\"max_value\"]-param_dict[\"min_value\"])+param_dict[\"min_value\"])\n",
    "\n",
    "            \n",
    "df_all = df_all\n",
    "df_all['Churn'] = predictions\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_imp = plot_importance(classifier, max_num_features=10, importance_type='weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'internetservice',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Churn rate according to internet service subscribtion', fontsize = 20)\n",
    "plt.xlabel(\"internetservice\", fontsize = 13)\n",
    "plt.ylabel('Number of customers', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise couple of more imporant features such as `streamingtv` and `streamingmovies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'streamingtv',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Churn rate according to internet streaming tv subscribtion', fontsize = 20)\n",
    "plt.xlabel(\"streamingtv\", fontsize = 13)\n",
    "plt.ylabel('Number of customers', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'streamingtv',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Churn rate according to streaming movies service subscribtion', fontsize = 20)\n",
    "plt.xlabel(\"streamingmovies\", fontsize = 13)\n",
    "plt.ylabel('Number of customers', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'gender',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Churn rate according to Gender', fontsize = 20)\n",
    "plt.xlabel(\"Gender\", fontsize = 13)\n",
    "plt.ylabel('Count', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.histplot(\n",
    "    data = df_all,\n",
    "    x = 'totalcharges',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Amount of each Payment Method', fontsize = 20)\n",
    "plt.xlabel(\"Charge Value\", fontsize = 13)\n",
    "plt.ylabel('Count', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'paymentmethod',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Amount of each Payment Method', fontsize = 20)\n",
    "plt.xlabel(\"Payment Method\", fontsize = 13)\n",
    "plt.ylabel('Total Amount', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (13,6))\n",
    "\n",
    "sns.countplot(\n",
    "    data = df_all,\n",
    "    x = 'partner',\n",
    "    hue = 'Churn'\n",
    ")\n",
    "\n",
    "plt.title('Affect of having a partner on Churn/Not', fontsize = 20)\n",
    "plt.xlabel(\"Have a partner\", fontsize = 13)\n",
    "plt.ylabel('Count', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üßëüèª‚Äçüî¨ StreamLit App </span>\n",
    "\n",
    "If you want to use an **interactive dashboards** - you can use a StreamLit App.\n",
    "\n",
    "Use the following commands in terminal to run a Streamlit App:\n",
    "\n",
    "> `cd {%path_to_hopsworks_tutorials%}/`  </br>\n",
    "> `conda activate ./miniconda/envs/hopsworks` </br>\n",
    "> `python -m streamlit run churn/streamlit_app.py`</br>\n",
    "\n",
    "**‚ö†Ô∏è** If you are running on Colab, you will need to follow a different procedure. As highlighted in this [notebook](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/Create_streamlit_app.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> üëì  Exploration</span>\n",
    "In the Hopsworks feature store, the metadata allows for multiple levels of explorations and review. Here we will show a few of those capacities. \n",
    "\n",
    "### <span style=\"color:#ff5f27;\">üîé <b>Search</b></span> \n",
    "Using the search function in the ui, you can query any aspect of the feature groups, feature_view and training data that was previously created.\n",
    "\n",
    "### <span style=\"color:#ff5f27;\">üìä <b>Statistics</b> </span>\n",
    "We can also enable statistics in one or all the feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_info_fg = fs.get_feature_group(\"customer_info\", version = 1)\n",
    "customer_info_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "customer_info_fg.update_statistics_config()\n",
    "customer_info_fg.compute_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fg-statistics](../churn/images/churn_statistics.gif)\n",
    "\n",
    "\n",
    "### <span style=\"color:#ff5f27;\">‚õìÔ∏è <b> Lineage </b> </span>\n",
    "In all the feature groups and feature view you can look at the relation between each abstractions; what feature group created which training dataset and that is used in which model.\n",
    "This allows for a clear undestanding of the pipeline in relation to each element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:#ff5f27;\">ü•≥ <b> Next Steps  </b> </span>\n",
    "Congratulations you've now completed the churn risk prediction tutorial for Managed Hopsworks.\n",
    "\n",
    "Check out our other tutorials on ‚û° https://github.com/logicalclocks/hopsworks-tutorials\n",
    "\n",
    "Or documentation at ‚û° https://docs.hopsworks.ai"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
