{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store Quickstart** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Iris Classification</span>\n",
    "\n",
    "**Note**: you may get an error when installing hopsworks on Colab, and it is safe to ignore it.\n",
    "\n",
    "This is the quickstart tutorial about Hopsworks Feature Store. Here you will work with data related to iris flower classification.\n",
    "\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into next sections:\n",
    "1. Import libraries and connect to Hopsworks Feature Store\n",
    "2. Load the Iris flower dataset\n",
    "3. Create a feature group and upload to the feature store\n",
    "4. Create a feature view from the feature group\n",
    "5. Create a training dataset\n",
    "6. Train a model using XGBoost\n",
    "7. Save the trained model to Hopsworks\n",
    "8. Launch a serving instance.\n",
    "9. Model deployment in Hopsworks\n",
    "10. Send a prediction request to the served model\n",
    "11. Try out your Model Interactively with a Gradio UI \n",
    "\n",
    "![tutorial-flow](../images/03_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U hopsworks --quiet\n",
    "!pip install -U xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRtpj-psbpG8"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVCqQYDhbpG_"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRmFM7vcbpHA"
   },
   "outputs": [],
   "source": [
    "# Read the Iris dataset\n",
    "iris_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv\")\n",
    "\n",
    "# Display the first 3 rows\n",
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR8HeEs6bpHB"
   },
   "outputs": [],
   "source": [
    "# Display concise summary information\n",
    "# This includes the data types, non-null counts, and memory usage of each column\n",
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H3XTfhMbpHB"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Creating Feature Groups </span>\n",
    "\n",
    "Lets save a feature group (hive table) called `iris` that contains the iris features and the corresponding numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4By1zTHIbpHC"
   },
   "outputs": [],
   "source": [
    "# Get or create the 'iris' feature group\n",
    "iris_fg = fs.get_or_create_feature_group(\n",
    "    name=\"iris\",\n",
    "    version=1,\n",
    "    primary_key=[\n",
    "        \"sepal_length\", \"sepal_width\",\n",
    "        \"petal_length\", \"petal_width\",\n",
    "    ],\n",
    "    description=\"Iris flower dataset\",\n",
    ")\n",
    "# Insert data info the feature group\n",
    "iris_fg.insert(\n",
    "    iris_df, \n",
    "    write_options={\"wait_for_job\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "Feature views are used to read features for training and inference.\n",
    "If the feature view already exists, get it. If not, create the feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query object\n",
    "query = iris_fg.select_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create the 'iris' feature view\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name=\"iris\",\n",
    "    version=1,\n",
    "    description=\"Read from Iris flower dataset\",\n",
    "    query=query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read query and display first 3 rows.\n",
    "query_df = query.read()\n",
    "query_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `feature_view.train_test_split()` method.\n",
    "\n",
    "* `X_train` is the train set features\n",
    "* `X_test` is the test set features\n",
    "* `Y_train` is the train set labels\n",
    "* `Y_test` is the test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train-test split\n",
    "X_train, X_test, _, _ = feature_view.train_test_split(\n",
    "    description='iris tutorial',\n",
    "    test_size=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable 'variety' from the training dataset (X_train)\n",
    "y_train = X_train.pop('variety').to_frame()\n",
    "\n",
    "# Extract the target variable 'variety' from the testing dataset (X_test)\n",
    "y_test = X_test.pop('variety').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 rows of the X_train\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 elements of the y_train\n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üß¨ Modeling</span>\n",
    "\n",
    "Train the XGBoost Classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LabelEncoder from scikit-learn\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training target variable 'variety' to numeric labels\n",
    "y_train_encoded = le.fit_transform(y_train['variety'])\n",
    "\n",
    "# Transform the testing target variable 'variety' to numeric labels using the previously fitted LabelEncoder\n",
    "y_test_encoded = le.transform(y_test['variety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBoost classifier\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Train the classifier using the training features (X_train) and encoded target variable (y_train_encoded)\n",
    "classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìê Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the importance of features using the XGBoost classifier\n",
    "plot_importance(\n",
    "    classifier, \n",
    "    max_num_features=10, \n",
    "    importance_type='weight',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8EC4_SvbpHE"
   },
   "outputs": [],
   "source": [
    "# Use the trained XGBoost classifier to make predictions on the testing features\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score using the true testing labels (y_test) and predicted labels (y_pred)\n",
    "# The 'macro' average calculates the F1 score for each class and then takes the unweighted mean\n",
    "f1 = f1_score(\n",
    "    y_test_encoded, \n",
    "    y_pred, \n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "# Create a dictionary containing the calculated F1 score\n",
    "metrics = {\n",
    "    \"f1_score\": f1,\n",
    "}\n",
    "\n",
    "# Print the dictionary containing the F1 score\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the true labels of the first 5 examples in the testing set\n",
    "y_test_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predicted labels for the first 5 examples in the testing set\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the confusion matrix using the true labels (y_test) and predicted labels (y_pred)\n",
    "results = confusion_matrix(\n",
    "    y_test_encoded, \n",
    "    y_pred,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the confusion matrix with labels for rows and columns\n",
    "df_cm = pd.DataFrame(\n",
    "    results, \n",
    "    ['True Setosa', 'True Versicolor', 'True Virginica'],\n",
    "    ['Pred Setosa', 'Pred Versicolor', 'Pred Virginica'],\n",
    ")\n",
    "\n",
    "# Create a heatmap using seaborn with annotations\n",
    "cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "# Get the figure from the heatmap and display it\n",
    "fig = cm.get_figure()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/3.0/user_guides/mlops/registry/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Create input and output schemas based on training features (X_train) and target variable (y_train)\n",
    "input_schema = Schema(X_train)\n",
    "output_schema = Schema(y_train_encoded)\n",
    "\n",
    "# Create a model schema using the input and output schemas\n",
    "model_schema = ModelSchema(\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema,\n",
    ")\n",
    "\n",
    "# Convert the model schema to a dictionary\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "Save the following objects as pickle files locally to a directory that will be uploaded later to the model registry:\n",
    "\n",
    " * the model object, `classifier` saved as `iris_xgboost_model.pkl`\n",
    " * the label encoder object, `le` saved as `iris_encoder.pkl`, so that we can reconstruct categorical names \n",
    "    from the encoded predictions (numbers) \n",
    "    \n",
    "The model input schema is the same set of features as in the `X_train` DataFrame.\n",
    "\n",
    "The model output schema is the same label as in the `y_train_encoded` array.\n",
    "\n",
    "Finally, lazily create the model that will be register, including all files (artifacts) in the directory (containing the pickled label encoder object and the pickled model object), the model's input/output schema, and a sample input row (`input_example`). The model registry is the `mr` object, and for our Scikit-Learn model, we create a model of type Python with `mr.python.create_model()`. For TensorFlow, there is `mr.tensorflow.create_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'iris_model' directory exists; if not, create it\n",
    "model_dir = \"iris_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Save the trained XGBoost classifier to a file in the 'iris_model' directory\n",
    "joblib.dump(classifier, model_dir + '/xgboost_iris_model.pkl')\n",
    "\n",
    "# Save the LabelEncoder to a file in the 'iris_model' directory\n",
    "joblib.dump(le, model_dir + '/iris_encoder.pkl')\n",
    "\n",
    "# Save the confusion matrix plot as an image file in the 'iris_model' directory\n",
    "fig.savefig(model_dir + \"/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Create a Python model in the model registry\n",
    "iris_model = mr.python.create_model(\n",
    "    name=\"xgboost_iris_model\", \n",
    "    metrics=metrics,\n",
    "    model_schema=model_schema,\n",
    "    input_example=X_train.sample(), \n",
    "    description=\"Iris Flower Predictor\",\n",
    ")\n",
    "\n",
    "# Save the model to the 'iris_model' directory\n",
    "iris_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#ff5f27\"> üöÄ Model Deployment</a>\n",
    "\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üìé Predictor script for Python models</span>\n",
    "\n",
    "\n",
    "Scikit-learn and XGBoost models are deployed as Python models, in which case you need to provide a **Predict** class that implements the **predict** method. The **predict()** method invokes the model on the inputs and returns the prediction as a list.\n",
    "\n",
    "The **init()** method is run when the predictor is loaded into memory, loading the model from the local directory it is materialized to, *ARTIFACT_FILES_PATH*.\n",
    "\n",
    "The directive `%%writefile` writes out the cell before to the given Python file. We will use the `predict_example.py` file to create a deployment for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k14k_uqbpHF"
   },
   "outputs": [],
   "source": [
    "%%writefile predict_example.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # NOTE: env var ARTIFACT_FILES_PATH has the local path to the model artifact files      \n",
    "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/xgboost_iris_model.pkl\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request from a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEEHKFzdbpHG"
   },
   "outputs": [],
   "source": [
    "# Get the dataset API for the project\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "# Upload the file \"predict_example.py\" to the \"Models\" dataset\n",
    "# If a file with the same name already exists, overwrite it\n",
    "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\", overwrite=True)\n",
    "\n",
    "# Construct the full path to the uploaded predictor script\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model using the predictor script located at 'predictor_script_path'\n",
    "deployment = iris_model.deploy(\n",
    "    name=\"irisdeployed\",\n",
    "    script_file=predictor_script_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of the deployed model\n",
    "print(\"Deployment: \" + deployment.name)\n",
    "\n",
    "# Retrieve and print detailed information about the deployment\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment is warming up...\")\n",
    "time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h4qsnUlbpHG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the deployment and wait for it to be in a running state for up to 180 seconds\n",
    "deployment.start(await_running=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and print detailed information about the current state of the deployment\n",
    "deployment.get_state().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To troubleshoot you can use `get_logs()` method\n",
    "deployment.get_logs(component='predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0iRFs0FbpHH"
   },
   "source": [
    "### <span style='color:#ff5f27'>üîÆ Predicting using deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the deployed model to make predictions on the provided input example\n",
    "predict = deployment.predict(\n",
    "    inputs=iris_model.input_example,\n",
    ")\n",
    "# or deployment.predict({ \"instances\": [iris_model.input_example] })\n",
    "\n",
    "print(le.inverse_transform([predict[\"predictions\"][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSFCgRWcbpHH"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëæ Try out your Model Interactively </span> \n",
    "\n",
    "\n",
    "We will build a user interface with Gradio to allow you to enter the 4 feature values (sepal length/width and petal length/width), producing a prediction of the type of iris flower.\n",
    "\n",
    "First, we have to install the gradio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdMNftbQbpHI"
   },
   "outputs": [],
   "source": [
    "!pip install gradio --quiet\n",
    "!pip install typing-extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Gradio\n",
    "\n",
    "Start the Gradio UI. Users enter the 4 feature values and a prediction is returned. We use the label encoder object to transform the number returned to the categorical value (stringified name of the Iris Flower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3DyKEOLbpHI"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def iris(sl, sw, pl, pw):\n",
    "    list_inputs = [sl, sw, pl, pw]\n",
    "    res = deployment.predict(inputs=[list_inputs])\n",
    "    return le.inverse_transform([res[\"predictions\"][0]])[0]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=iris,\n",
    "    title=\"Iris Flower Predictive Analytics\",\n",
    "    description=\"Experiment with sepal/petal lengths/widths to predict which flower it is.\",\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.Number(label=\"sepal length (cm)\"),\n",
    "        gr.Number(label=\"sepal width (cm)\"),\n",
    "        gr.Number(label=\"petal length (cm)\"),\n",
    "        gr.Number(label=\"petal width (cm)\"),\n",
    "    ],\n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "August 2022 iris_sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3502d2fd240c5a16c1ad36292676af1ecc0e366e100e74e6e336a5116ced841"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
