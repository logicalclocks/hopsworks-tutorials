{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üß¨ Train Retrieval Model </span>\n",
    "\n",
    "In this notebook, you will train a retrieval model that will be able to quickly generate a small subset of candidate items from a large collection of items. Your model will be based on the *two-tower architecture*, which embeds queries and candidates (keys) into a shared low-dimensional vector space. Here, a query consists of features of a customer and a transaction (e.g. timestamp of the purchase), whereas a candidate consists of features of a particular item. All queries will have a user ID and all candidates will have an item ID, and the model will be trained such that the embedding of a user will be close to all the embeddings of items the user has previously bought.\n",
    "\n",
    "After training the model you will save and upload its components to the Hopsworks Model Registry.\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup, Normalization\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üî™ Feature Selection </span>\n",
    "\n",
    "First, you'll load the feature groups you created in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_feature_group(\n",
    "    name=\"transactions\",\n",
    "    version=1,\n",
    ")\n",
    "customers_fg = fs.get_feature_group(\n",
    "    name=\"customers\",\n",
    "    version=1,\n",
    ")\n",
    "articles_fg = fs.get_feature_group(\n",
    "    name=\"articles\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to join these three data sources to make the data compatible with out retrieval model. Recall that each row in the `transactions` feature group relates information about which customer bought which item. You'll join this feature group with the `customers` and `articles` feature groups to inject customer and item features into each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = trans_fg.select([\"customer_id\", \"article_id\", \"t_dat\", \"price\", \"month_sin\", \"month_cos\"])\\\n",
    "    .join(customers_fg.select([\"age\", \"club_member_status\", \"age_group\"]), on=\"customer_id\")\\\n",
    "    .join(articles_fg.select([\"garment_group_name\", \"index_group_name\"]), on=\"article_id\")\n",
    "\n",
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='retrieval',\n",
    "    query=selected_features,\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and explore data in the feature view you can retrieve batch data using the `get_batch_data()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üèãÔ∏è Training Dataset </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, _, _, _ = feature_view.train_validation_test_split(\n",
    "    validation_size=0.1, \n",
    "    test_size=0.1,\n",
    "    description='Retrieval dataset splits',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train your retrieval model with a subset of features.\n",
    "\n",
    "For the query embedding you will use:\n",
    "- `customer_id`: ID of the customer.\n",
    "- `age`: age of the customer at the time of purchase.\n",
    "- `month_sin`, `month_cos`: time of year the purchase was made.\n",
    "\n",
    "For the candidate embedding you will use:\n",
    "- `article_id`: ID of the item.\n",
    "- `garment_group_name`: type of garment.\n",
    "- `index_group_name`: menswear/ladieswear etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features = [\"customer_id\", \"age\", \"month_sin\", \"month_cos\"]\n",
    "candidate_features = [\"article_id\", \"garment_group_name\", \"index_group_name\"]\n",
    "\n",
    "def df_to_ds(df):\n",
    "    return tf.data.Dataset.from_tensor_slices({col: df[col] for col in df})\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "train_ds = df_to_ds(train_df).batch(BATCH_SIZE).cache().shuffle(BATCH_SIZE*10)\n",
    "val_ds = df_to_ds(val_df).batch(BATCH_SIZE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a list of user and item IDs when you initialize your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve unique customer IDs and article IDs from the training dataset\n",
    "user_id_list = train_df[\"customer_id\"].unique().tolist()\n",
    "item_id_list = train_df[\"article_id\"].unique().tolist()\n",
    "\n",
    "# Retrieve unique garment group names and index group names from the training dataset\n",
    "garment_group_list = train_df[\"garment_group_name\"].unique().tolist()\n",
    "index_group_list = train_df[\"index_group_name\"].unique().tolist()\n",
    "\n",
    "# Print the number of transactions, number of users, number of items, and unique garment group names\n",
    "print(f\"Number of transactions: {len(train_df):,}\")\n",
    "print(f\"Number of users: {len(user_id_list):,}\")\n",
    "print(f\"Number of items: {len(item_id_list):,}\")\n",
    "print(garment_group_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üè∞ Two Tower Model </span>\n",
    "\n",
    "The two tower model consist of two models:\n",
    "- Query model: Generates a query representation given user and transaction features.\n",
    "- Candidate model: Generates an item representation given item features.\n",
    "\n",
    "**Both models produce embeddings that live in the same embedding space**. You let this space be low-dimensional to prevent overfitting on the training data. (Otherwise, the model might simply memorize previous purchases, which makes it recommend items customers already have bought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start with creating the query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTower(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            StringLookup(\n",
    "                vocabulary=user_id_list,\n",
    "                mask_token=None\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                # You add an additional embedding to account for unknown tokens.\n",
    "                len(user_id_list) + 1,\n",
    "                EMB_DIM\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.normalized_age = Normalization(axis=None)\n",
    "\n",
    "        self.fnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(EMB_DIM, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(EMB_DIM)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        concatenated_inputs = tf.concat([\n",
    "            self.user_embedding(inputs[\"customer_id\"]),\n",
    "            tf.reshape(self.normalized_age(inputs[\"age\"]), (-1,1)),\n",
    "            tf.reshape(inputs[\"month_sin\"], (-1,1)),\n",
    "            tf.reshape(inputs[\"month_cos\"], (-1,1))\n",
    "        ], axis=1)\n",
    "\n",
    "        outputs = self.fnn(concatenated_inputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "query_model = QueryTower()\n",
    "\n",
    "query_model.normalized_age.adapt(train_ds.map(lambda x : x[\"age\"]))\n",
    "\n",
    "# Initialize model with inputs.\n",
    "query_df = train_df[query_features]\n",
    "query_ds = df_to_ds(query_df).batch(1)\n",
    "query_model(next(iter(query_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidate model is very similar to the query model. A difference is that it has two categorical features as input, which you one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTower(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.item_embedding = tf.keras.Sequential([\n",
    "            StringLookup(\n",
    "                vocabulary=item_id_list,\n",
    "                mask_token=None\n",
    "            ),\n",
    "            tf.keras.layers.Embedding(\n",
    "                # You add an additional embedding to account for unknown tokens.\n",
    "                len(item_id_list) + 1,\n",
    "                EMB_DIM\n",
    "            )\n",
    "        ])\n",
    "        # Converts strings into integer indices (scikit-learn LabelEncoder analog)\n",
    "        self.garment_group_tokenizer = StringLookup(\n",
    "            vocabulary=garment_group_list, \n",
    "            mask_token=None,\n",
    "        )\n",
    "        self.index_group_tokenizer = StringLookup(\n",
    "            vocabulary=index_group_list, \n",
    "            mask_token=None,\n",
    "        )\n",
    "\n",
    "        self.fnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(EMB_DIM, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(EMB_DIM)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        garment_group_embedding = tf.one_hot(\n",
    "            self.garment_group_tokenizer(inputs[\"garment_group_name\"]),\n",
    "            len(garment_group_list),\n",
    "        )\n",
    "\n",
    "        index_group_embedding = tf.one_hot(\n",
    "            self.index_group_tokenizer(inputs[\"index_group_name\"]),\n",
    "            len(index_group_list),\n",
    "        )\n",
    "\n",
    "        concatenated_inputs = tf.concat([\n",
    "            self.item_embedding(inputs[\"article_id\"]),\n",
    "            garment_group_embedding,\n",
    "            index_group_embedding,\n",
    "        ], axis=1)\n",
    "\n",
    "        outputs = self.fnn(concatenated_inputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "item_model = ItemTower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the two tower model using the *top-100 accuracy*. That is, for each transaction in the validation data you will generate the associated query embedding and retrieve the set of the 100 items that are closest to this query in the embedding space. The top-100 accuracy measures how often the item that was actually bought is part of this subset. To evaluate this, you create a dataset of all unique items in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = train_df[candidate_features]\n",
    "item_df.drop_duplicates(subset=\"article_id\", inplace=True)\n",
    "item_ds = df_to_ds(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, you can finally create your two tower model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(tf.keras.Model):\n",
    "    def __init__(self, query_model, item_model):\n",
    "        super().__init__()\n",
    "        self.query_model = query_model\n",
    "        self.item_model = item_model\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=item_ds.batch(BATCH_SIZE).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_step(self, batch) -> tf.Tensor:\n",
    "        # Set up a gradient tape to record gradients.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Loss computation.\n",
    "            user_embeddings = self.query_model(batch)\n",
    "            item_embeddings = self.item_model(batch)\n",
    "            loss = self.task(\n",
    "                user_embeddings, \n",
    "                item_embeddings,\n",
    "                compute_metrics=False,\n",
    "            )\n",
    "\n",
    "            # Handle regularization losses as well.\n",
    "            regularization_loss = sum(self.losses)\n",
    "\n",
    "            total_loss = loss + regularization_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"regularization_loss\": regularization_loss,\n",
    "            \"total_loss\": total_loss\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch) -> tf.Tensor:\n",
    "        # Loss computation.\n",
    "        user_embeddings = self.query_model(batch)\n",
    "        item_embeddings = self.item_model(batch)\n",
    "\n",
    "        loss = self.task(\n",
    "            user_embeddings, \n",
    "            item_embeddings,\n",
    "            compute_metrics=False,\n",
    "        )\n",
    "\n",
    "        # Handle regularization losses as well.\n",
    "        regularization_loss = sum(self.losses)\n",
    "\n",
    "        total_loss = loss + regularization_loss\n",
    "\n",
    "        metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "        metrics[\"loss\"] = loss\n",
    "        metrics[\"regularization_loss\"] = regularization_loss\n",
    "        metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27\">üèÉüèª‚Äç‚ôÇÔ∏è Model Training </span>\n",
    "\n",
    "You'll train our model using the AdamW optimizer, which applies weight regularization during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TwoTowerModel with the specified query_model and item_model\n",
    "model = TwoTowerModel(query_model, item_model)\n",
    "\n",
    "# Define an optimizer using AdamW with a learning rate of 0.01\n",
    "optimizer = tfa.optimizers.AdamW(0.001, learning_rate=0.01)\n",
    "\n",
    "# Compile the model using the specified optimizer\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üóÑÔ∏è Upload Model to Model Registry </span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "Let's connect to the model registry using the [HSML library](https://docs.hopsworks.ai/machine-learning-api/latest) from Hopsworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModelModule(tf.Module):\n",
    "    def __init__(self, query_model):\n",
    "        self.query_model = query_model\n",
    "\n",
    "    @tf.function()\n",
    "    def compute_emb(self, instances):\n",
    "        query_emb = self.query_model(instances)\n",
    "        return {\n",
    "            \"customer_id\": instances[\"customer_id\"],\n",
    "            \"month_sin\": instances[\"month_sin\"],\n",
    "            \"month_cos\": instances[\"month_cos\"],\n",
    "            \"query_emb\": query_emb,\n",
    "        }\n",
    "\n",
    "# wrap query_model:   query_model -> query_model_module\n",
    "query_model = QueryModelModule(model.query_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input specifications for the instances\n",
    "instances_spec = {\n",
    "    'customer_id': tf.TensorSpec(shape=(None,), dtype=tf.string, name='customer_id'),  # Specification for customer IDs\n",
    "    'month_sin': tf.TensorSpec(shape=(None,), dtype=tf.float64, name='month_sin'),     # Specification for sine of month\n",
    "    'month_cos': tf.TensorSpec(shape=(None,), dtype=tf.float64, name='month_cos'),     # Specification for cosine of month\n",
    "    'age': tf.TensorSpec(shape=(None,), dtype=tf.float64, name='age'),                 # Specification for age\n",
    "}\n",
    "\n",
    "# Get the concrete function for the query_model's compute_emb function using the specified input signatures\n",
    "signatures = query_model.compute_emb.get_concrete_function(instances_spec)\n",
    "\n",
    "# Save the query_model along with the concrete function signatures\n",
    "tf.saved_model.save(\n",
    "    query_model,           # The model to save\n",
    "    \"query_model\",         # Path to save the model\n",
    "    signatures=signatures, # Concrete function signatures to include\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to save our models locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(\n",
    "    model.item_model,    # The model to save\n",
    "    \"candidate_model\",   # Path to save the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model. A schema can either be manually specified or inferred from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Infer input schema from data.\n",
    "query_model_input_schema = Schema(query_df)\n",
    "\n",
    "# Manually specify output schema.\n",
    "query_model_output_schema = Schema([{\n",
    "    \"name\": \"query_embedding\",\n",
    "    \"type\": \"float32\",\n",
    "    \"shape\": [EMB_DIM],\n",
    "}])\n",
    "\n",
    "query_model_schema = ModelSchema(\n",
    "    input_schema=query_model_input_schema,\n",
    "    output_schema=query_model_output_schema,\n",
    ")\n",
    "\n",
    "query_model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the schema in place, you can finally register your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a query example from the query DataFrame\n",
    "query_example = query_df.sample().to_dict(\"records\")\n",
    "\n",
    "# Create a tensorflow model for the query_model in the Model Registry \n",
    "mr_query_model = mr.tensorflow.create_model(\n",
    "    name=\"query_model\",                                           # Name of the model\n",
    "    description=\"Model that generates query embeddings from user and transaction features\",  # Description of the model\n",
    "    input_example=query_example,                                  # Example input for the model\n",
    "    model_schema=query_model_schema,                              # Schema of the model\n",
    ")\n",
    "\n",
    "# Save the query_model to the Model Registry\n",
    "mr_query_model.save(\"query_model\")                                # Path to save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have also saved an input example from the training data, which can be helpful for test purposes.\n",
    "\n",
    "Let's repeat the process with the candidate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema for the candidate_model based on item_df\n",
    "candidate_model_input_schema = Schema(item_df)\n",
    "\n",
    "# Define the output schema for the candidate_model, specifying the shape and type of the output\n",
    "candidate_model_output_schema = Schema([{\n",
    "    \"name\": \"candidate_embedding\",   # Name of the output feature\n",
    "    \"type\": \"float32\",               # Data type of the output feature\n",
    "    \"shape\": [EMB_DIM],              # Shape of the output feature\n",
    "}])\n",
    "\n",
    "# Combine the input and output schemas to create the overall model schema for the candidate_model\n",
    "candidate_model_schema = ModelSchema(\n",
    "    input_schema=candidate_model_input_schema,    # Input schema for the model\n",
    "    output_schema=candidate_model_output_schema,  # Output schema for the model\n",
    ")\n",
    "\n",
    "# Sample a candidate example from the item DataFrame\n",
    "candidate_example = item_df.sample().to_dict(\"records\")\n",
    "\n",
    "# Create a tensorflow model for the candidate_model in the Model Registry\n",
    "mr_candidate_model = mr.tensorflow.create_model(\n",
    "    name=\"candidate_model\",                                        # Name of the model\n",
    "    description=\"Model that generates candidate embeddings from item features\",  # Description of the model\n",
    "    input_example=candidate_example,                              # Example input for the model\n",
    "    model_schema=candidate_model_schema,                          # Schema of the model\n",
    ")\n",
    "\n",
    "# Save the candidate_model to the Model Registry\n",
    "mr_candidate_model.save(\"candidate_model\")                        # Path to save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27\">‚è©Ô∏è Next Steps </span>\n",
    "\n",
    "Retrieving the top-k closest candidate embeddings in a brute-force way (computing the distances between the query embedding and all candidate embeddings) is too expensive in a practical setting. In the next notebook, you will compute embeddings and create a feature view which will allow you to retrieve candidates with very low latency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
