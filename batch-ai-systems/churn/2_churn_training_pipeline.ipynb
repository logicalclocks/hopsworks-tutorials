{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Training Pipeline</span>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/churn/2_churn_training_pipeline.ipynb)\n",
    "\n",
    "This is the second part of the quick start series of tutorials about predicting customers that are at risk of churning with the Hopsworks Feature Store.\n",
    "\n",
    "This notebook explains how to read from a feature group and create training dataset within the feature store.\n",
    "\n",
    "You will train the model using XGBoost model, although it could just as well be trained with other machine learning frameworks such as Scikit-learn, PySpark, TensorFlow, and PyTorch. You will also perform some of the exploration that can be done in Hopsworks, notably the search functions and the lineage.\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Select the features you want to train the model on.\n",
    "2. Preprocess of features.\n",
    "3. Create a dataset split for training and validation data.\n",
    "4. Load the training data.\n",
    "5. Train the model.\n",
    "6. Explore feature groups and views via the UI.\n",
    "\n",
    "![tutorial-flow](../images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'hopsworks[python]' --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You will start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "customer_info_fg = fs.get_feature_group(\n",
    "    name=\"customer_info\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "demography_fg = fs.get_feature_group(\n",
    "    name=\"customer_demography_info\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "subscriptions_fg = fs.get_feature_group(\n",
    "    name=\"customer_subscription_info\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = customer_info_fg.select_features() \\\n",
    "    .join(demography_fg.select_features()) \\\n",
    "    .join(subscriptions_fg.select_all(include_event_time=False))\n",
    "\n",
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you created three feature groups in the previous notebook. If you had created multiple feature groups with identical schema and wanted to include them in the join you would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü§ñ Transformation Functions </span>\n",
    "\n",
    "You will preprocess the data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you will simply define a mapping between features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformation functions from Hopsworks.\n",
    "from hopsworks.hsfs.builtin_transformations import label_encoder, min_max_scaler\n",
    "\n",
    "\n",
    "# Define lists of numerical and categorical features\n",
    "numerical_features = [\"tenure\", \"monthlycharges\", \"totalcharges\"]\n",
    "categorical_features = [\n",
    "    \"multiplelines\", \"internetservice\", \"onlinesecurity\", \"onlinebackup\",\n",
    "    \"deviceprotection\", \"techsupport\", \"streamingmovies\", \"streamingtv\",\n",
    "    \"phoneservice\", \"paperlessbilling\", \"contract\", \"paymentmethod\", \"gender\", \n",
    "    \"dependents\", \"partner\",\n",
    "]\n",
    "\n",
    "# Map features to their corresponding transformation functions\n",
    "transformation_functions = []\n",
    "\n",
    "# For numerical features, use the min_max_scaler transformation\n",
    "for feature in numerical_features:\n",
    "    transformation_functions.append(min_max_scaler(feature))\n",
    "\n",
    "# For categorical features, use the label_encoder transformation\n",
    "for feature in categorical_features:\n",
    "    transformation_functions.append(label_encoder(feature)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View you may use `fs.get_or_create_feature_view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create the 'churn_feature_view'\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='churn_feature_view',\n",
    "    version=1,\n",
    "    labels=[\"churn\"],\n",
    "    transformation_functions=transformation_functions,\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature view is now visible in the UI.\n",
    "\n",
    "![fv-overview](../churn/images/churn_tutofv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üèãÔ∏è Training Dataset </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n",
    "    validation_size=0.2,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Drop the 'customerid' column from the training set\n",
    "X_train.drop('customerid', axis=1, inplace=True)\n",
    "\n",
    "# Drop the 'customerid' column from the validation set\n",
    "X_val.drop('customerid', axis=1, inplace=True)\n",
    "\n",
    "# Drop the 'customerid' column from the test set\n",
    "X_test.drop('customerid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the normalized value counts of the target variable\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution is skewed, which is good news for the company considering that customers at risk of churning make up smaller part of customer base. However, as a data scientist should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, you will use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (curn) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next you will train a model and set the bigger class weight for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBClassifier with a specified scale_pos_weight\n",
    "model = xgb.XGBClassifier(scale_pos_weight=3)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üë®üèª‚Äç‚öñÔ∏è Model Evaluation</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for both training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute all metrics\n",
    "metrics = {\n",
    "    # Training metrics\n",
    "    \"train_accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "    \"train_precision\": precision_score(y_train, y_train_pred),\n",
    "    \"train_recall\": recall_score(y_train, y_train_pred),\n",
    "    \"train_f1\": f1_score(y_train, y_train_pred),\n",
    "    \n",
    "    # Test metrics\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "    \"test_precision\": precision_score(y_test, y_test_pred),\n",
    "    \"test_recall\": recall_score(y_test, y_test_pred),\n",
    "    \"test_f1\": f1_score(y_test, y_test_pred),\n",
    "}\n",
    "\n",
    "metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:#ff5f27;\">üóÑ Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model directory\n",
    "model_dir = \"churn_model\"\n",
    "images_dir = os.path.join(model_dir, \"images\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(images_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier as json file\n",
    "model.save_model(os.path.join(model_dir, \"model.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(\n",
    "    y_test, \n",
    "    model.predict(X_test)\n",
    ").astype(int)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "df_cm = pd.DataFrame(\n",
    "    conf_matrix, \n",
    "    ['Non Churn', 'Churn'],\n",
    "    ['Non Churn', 'Churn']\n",
    ")\n",
    "\n",
    "# Create and save the confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(\n",
    "    df_cm, \n",
    "    annot=True,\n",
    "    fmt='d',  # Use integer format for confusion matrix values\n",
    "    cmap='RdPu',  # Use a color palette good for binary classification\n",
    "    annot_kws={'size': 14},\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Save the plot with high quality settings\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(images_dir, \"confusion_matrix.png\"), \n",
    "    dpi=300, \n",
    "    bbox_inches='tight'\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model in the model registry\n",
    "model = mr.python.create_model(\n",
    "    name=\"churnmodel\",\n",
    "    description=\"Churn Model\",\n",
    "    input_example=X_train.sample(),\n",
    "    feature_view=feature_view,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03 </span>\n",
    "\n",
    "In the following notebook you will use your model for batch inference.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/churn/3_churn_batch_inference.ipynb)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
