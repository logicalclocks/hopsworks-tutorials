{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**Your Python Jupyter notebook should be configured for >8GB of memory.**\n",
    "\n",
    "In this series of tutorials, we will build a recommender system for fashion items. It will consist of two models: a *retrieval model* and a *ranking model*. The idea is that the retrieval model should be able to quickly generate a small subset of candidate items from a large collection of items. This comes at the cost of granularity, which is why we also train a ranking model that can afford to use more features than the retrieval model.\n",
    "\n",
    "### Data\n",
    "\n",
    "We will use data from the [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations) Kaggle competition.\n",
    "\n",
    "<!-- https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data\n",
    "\n",
    "For this challenge you are given the purchase history of customers across time, along with supporting metadata. Your challenge is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends. Customer who did not make any purchase during that time are excluded from the scoring. -->\n",
    "\n",
    "The full dataset contains images of all products, but here we will simply use the tabular data. We have three data sources:\n",
    "- `articles.csv`: info about fashion items.\n",
    "- `customers.csv`: info about users.\n",
    "- `transactions_train.csv`: info about transactions.\n",
    "\n",
    "You can use the *hopsworks* library to download these files locally, assuming that they are stored in your cluster. In this example, we have saved them to the `Resources` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell and fill in details if you are running external Python\n",
    "import os\n",
    "key=\"\"\n",
    "with open(\"api-key.txt\", \"r\") as f:\n",
    "    key = f.read().rstrip()\n",
    "os.environ['HOPSWORKS_PROJECT']=\"hm\"\n",
    "os.environ['HOPSWORKS_HOST']=\"35.240.81.237\"\n",
    "os.environ['HOPSWORKS_API_KEY']=key   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path=\"https://repo.hops.works/dev/jdowling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(path + \"articles.csv\")\n",
    "articles_df[\"article_id\"] = articles_df[\"article_id\"].astype(str)\n",
    "articles_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = pd.read_csv(path + \"customers.csv\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = pd.read_csv(path + \"transactions_train.csv\", parse_dates=[\"t_dat\"])\n",
    "trans_df[\"article_id\"] = trans_df[\"article_id\"].astype(str)\n",
    "trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(trans_df):,} transactions in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a large dataset. For the sake of the tutorial, we will use a small subset of this dataset, which we generate by sampling 25'000 customers and using their transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = 25_000\n",
    "\n",
    "# Consider only customers with age defined.\n",
    "customers_df.dropna(inplace=True, subset=[\"age\"])\n",
    "customer_subset_df = customers_df.sample(N_USERS, random_state=27)\n",
    "trans_df = trans_df.merge(customer_subset_df[\"customer_id\"])\n",
    "\n",
    "print(f\"Subset has {len(trans_df):,} transactions in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Next, we do some feature engineering.\n",
    "\n",
    "The time of the year a purchase was made should be a strong predictor, as seasonality plays a big factor in fashion purchases. Here, we will use the month of the purchase as a feature. Since this is a cyclical feature (January is as close to December as it is to February), we'll map each month to the unit circle using sine and cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO - this is a transformation. We are applying it before we write to the FG.\n",
    "# We should instead apply it as a transformation fn to the feature-view\n",
    "\n",
    "# Map month to range [0,11].\n",
    "month = trans_df[\"t_dat\"].apply(lambda x : x.month - 1)\n",
    "C = 2*np.pi/12\n",
    "\n",
    "# Map month to the unit circle.\n",
    "trans_df[\"month_sin\"] = np.sin(month*C)\n",
    "trans_df[\"month_cos\"] = np.cos(month*C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.dropna(axis=1, inplace=True)\n",
    "articles_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert python datetime object to unix epoch milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.t_dat = trans_df.t_dat.values.astype(np.int64) // 10 ** 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Groups\n",
    "\n",
    "A [feature group](https://docs.hopsworks.ai/feature-store-api/latest/generated/feature_group/) can be seen as a collection of conceptually related features.\n",
    "\n",
    "Before we can create a feature group we need to connect to our feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a feature group we need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg = fs.create_feature_group(\n",
    "    name=\"customers\",\n",
    "    version=1,\n",
    "    description=\"Customer data.\",\n",
    "    primary_key=[\"customer_id\"],\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have also set `online_enabled=True`, which enables low latency access to the data. A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group).\n",
    "\n",
    "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent we populate it with its associated data using the `save` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_fg.insert(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for the rest of the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_fg = fs.create_feature_group(\n",
    "    name=\"articles\",\n",
    "    version=1,\n",
    "    description=\"Fashion item data.\",\n",
    "    primary_key=[\"article_id\"],\n",
    "    online_enabled=True\n",
    ")\n",
    "articles_fg.insert(articles_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.create_feature_group(\n",
    "    name=\"transactions\",\n",
    "    version=1,\n",
    "    description=\"Transaction data.\",\n",
    "    primary_key=[\"customer_id\", \"article_id\"], \n",
    "    online_enabled=True,\n",
    "    event_time=[\"t_dat\"]\n",
    ")\n",
    "trans_fg.insert(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to inspect the feature groups in the Hopsworks UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook we'll create a dataset that we can train a retrieval model on."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
