{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4aad33d",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Maggy\n",
    "\n",
    "### **Note: currently this notebook needs to be run with a PySpark kernel to work properly!*\n",
    "\n",
    "In this notebook, we'll use the [Maggy](https://maggy.ai/master/) library from Hopsworks to run experiments with hyperparameter tuning. In particular we will:\n",
    "\n",
    "- Load a training dataset from the feature store.\n",
    "- Train models on the dataset using different hyperparameters.\n",
    "\n",
    "![tutorial-flow](images/maggy_hp.png)\n",
    "\n",
    "We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c799999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>0</td><td>application_1653087438552_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1653087438552_0010/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_1653087438552_0010_01_000001/fraud_streaming__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1961792",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\"transactions_view\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3452bdd",
   "metadata": {},
   "source": [
    "## TODO: (Davit): explain about label here or in the previous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ed7bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraud_label']"
     ]
    }
   ],
   "source": [
    "feature_view.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c9dc9",
   "metadata": {},
   "source": [
    "### Load Training Data\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook. Since we're running this notebook in a PySpark Kernel we'll get Spark Dataframes, which we'll need to convert back to Pandas Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e9d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserWarning: Training dataset splits were defined but no `train_split` (the name of the split that is going to be used for training) was provided. Setting this property to `train`. The statistics of this split will be used for transformation functions."
     ]
    }
   ],
   "source": [
    "_, train_df = feature_view.get_training_dataset_splits({'train': 80}, start_time=None, end_time=None, version = 1)\n",
    "_, val_df = feature_view.get_training_dataset_splits({'validation': 20}, start_time=None, end_time=None, version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f40777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        _c0|     _c1|                 _c2|                 _c3|                 _c4|                 _c5|                 _c6|                 _c7|                 _c8|                 _c9|\n",
      "+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|fraud_label|category|              amount|  age_at_transaction|days_until_card_e...|           loc_delta|   trans_volume_mstd|   trans_volume_mavg|          trans_freq|      loc_delta_mavg|\n",
      "|          0|       5|1.067794613710282...|0.006711374929193003|  0.7216365405497585|0.028996255497714032|8.709211442635827E-4|8.709211442635827E-4|8.709211442635827E-4| 8.31250223954274E-5|\n",
      "|          0|       5|1.101163195388729E-5|  0.5485951330448104|   0.130666027415632| 0.04235874508677107|0.003717598646835...|0.003717598646835...|0.003717598646835...| 0.14249053722073857|\n",
      "|          0|       5|1.167900358745621...|  0.3389873672507989|   0.425474525738079| 0.19669734970088537|0.033030935719679515|0.033030935719679515|0.033030935719679515|1.195637752322052...|\n",
      "|          0|       5|1.334743267137853E-5|  0.9603606560144765|  0.8171175237305549| 0.09628056702296035|4.204446903341434E-4|4.204446903341434E-4|4.204446903341434E-4| 0.21053838121164067|\n",
      "|          0|       5|1.401480430494745...|  0.5383528924728148|  0.8549000296774498| 0.47532472365722517|0.005993005268572...|0.005993005268572...|0.005993005268572...|5.884240063408851...|\n",
      "|          0|       5|2.202326390777458E-5|  0.3295601308959226| 0.15840648626796516|0.050723351998720294|0.002802964602227...|0.002802964602227...|0.002802964602227...|8.565943343761031E-5|\n",
      "|          0|       5|4.171072709805791...|  0.9113290954104182|  0.6362407377080843| 0.08831717630373255|7.698142068260864E-4|7.698142068260864E-4|7.698142068260864E-4|  0.3121778099801709|\n",
      "|          0|       5|4.771707180017825...|  0.6395259086738042| 0.32960200767365616| 0.14642536860890035|0.001577668647539...|0.001577668647539...|0.001577668647539...|1.236009440439180...|\n",
      "|          0|       5|4.938550088410057E-5| 0.05434449657702083| 0.19311078720426988| 0.16067151722625178|0.001167234545070...|0.001167234545070...|0.001167234545070...| 0.42576532942485873|\n",
      "|          0|       5|5.639290303657429...|  0.8222840248809471|  0.3974009598856944|4.348605511639803...| 0.00319938388168553| 0.00319938388168553| 0.00319938388168553|6.721483330343674E-5|\n",
      "|          0|       5|1.017741741192613...| 0.03752159892631234| 0.11168055406576421| 0.16691503402477786|0.002758250643096...|0.002758250643096...|0.002758250643096...|4.072344377792043...|\n",
      "|          0|       5|1.064457755542438E-4| 0.39018750022336884|  0.7022547228582442| 0.21800283819380267|  0.0034586581073916|  0.0034586581073916|  0.0034586581073916|1.481517323565552...|\n",
      "|          0|       5|1.111173769892262...|  0.8850134081335863| 0.05347921002643857| 0.11287493953617274|0.044889478104675376|0.044889478104675376|0.044889478104675376|  9.3853025447379E-5|\n",
      "|          0|       5|1.301374685459407E-4| 0.36185475287796265|  0.3503875233456295|3.366495201692102E-5| 0.02556537297615111| 0.02556537297615111| 0.02556537297615111|1.224103660953819...|\n",
      "|          0|       5|1.424838437669658...|   0.516056861556752| 0.39085673879516797| 0.07617571437279308|1.638399547254479...|1.638399547254479...|1.638399547254479...|6.524018867261329E-5|\n",
      "|          0|       5|1.741839963614898...| 0.46652882735678664|  0.7917489095707084| 0.09553821599495267|0.006628343911743988|0.006628343911743988|0.006628343911743988| 0.05437668657881247|\n",
      "|          0|       5|1.741839963614898...|  0.5056323569167376| 0.05633095987798804| 0.18262896217207442|0.002195321918816135|0.002195321918816135|0.002195321918816135|  0.4870790750439103|\n",
      "|          0|       5|1.868640573992994...|  0.4722023625042293|  0.8438428213093011|0.008500564801137107|0.004065633418278733|0.004065633418278733|0.004065633418278733| 0.16919721548830896|\n",
      "|          0|       5|2.415885313519514...|  0.5274815460411597| 0.37920426577663524| 0.16120049632249805|0.020833701778581206|0.020833701778581206|0.020833701778581206|6.542873117189791E-5|\n",
      "+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e229b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.toPandas()\n",
    "X_val = val_df.toPandas()\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e12462",
   "metadata": {},
   "source": [
    "We will train a model to predict `fraud_label` given the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a2987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "\"None of [Index(['fraud_label'], dtype='object')] are in the [columns]\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/core/frame.py\", line 5226, in pop\n",
      "    return super().pop(item=item)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/core/generic.py\", line 870, in pop\n",
      "    result = self[item]\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/core/frame.py\", line 3464, in __getitem__\n",
      "    indexer = self.loc._get_listlike_indexer(key, axis=1)[1]\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1314, in _get_listlike_indexer\n",
      "    self._validate_read_indexer(keyarr, indexer, axis)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1374, in _validate_read_indexer\n",
      "    raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "KeyError: \"None of [Index(['fraud_label'], dtype='object')] are in the [columns]\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = feature_view.label\n",
    "\n",
    "y_train = X_train.pop(target)\n",
    "y_val = X_val.pop(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152b188",
   "metadata": {},
   "source": [
    "Let's check the distribution of our target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374e44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'y_train' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'y_train' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec93b6f",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus we should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, we'll use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097b7cf",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "In the following example, we'll use a simple logistic regression model and do a hyperparameter search over class weights. Since our dataset is unbalanced we will evaluate each hyperparameter configuration using the *F1-score* rather than *accuracy*.\n",
    "\n",
    "First, we define a training function that will return an evaluation score given a hyperparameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b6c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def training_function(pos_class_weight):\n",
    "    clf = LogisticRegression(class_weight={0: 1.0 - pos_class_weight, 1: pos_class_weight}, solver='liblinear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_val)\n",
    "    score = f1_score(y_val, preds)\n",
    "    return {'metric': score} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33679700",
   "metadata": {},
   "source": [
    "Note that this code assumes that the `X_train`, `y_train` etc variables already exist in the namespace.\n",
    "\n",
    "Let's test the code to see that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3d1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'y_train' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 6, in training_function\n",
      "NameError: name 'y_train' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = training_function(0.5)\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d0122",
   "metadata": {},
   "source": [
    "Now let's see if we can find a value for `class_weight` that gives us a better score.\n",
    "\n",
    "To do this we'll define a search space, which represents the set of possible values we want to consider for our hyperparameters. We'll also need to define datatypes for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30181a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: pos_class_weight"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "\n",
    "sp = Searchspace(pos_class_weight=('DOUBLE', [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da951a4f",
   "metadata": {},
   "source": [
    "Next we'll define a configuration for our hyperparameter search. Some important parameters are:\n",
    "- `num_trials`: Number of models to train. You should set this based on how much time you are willing to spend. We'll just do five trials here to showcase the functionality.\n",
    "- `optimizer`: Strategy used to determine the next parameter value to try. We will just use grid search, but you can read about alternatives [here](https://maggy.ai/master/hpo/strategies/).\n",
    "- `direction`: Should be set to `max` if the output of `train_fn` should be maximized, otherwise `min`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b78dea",
   "metadata": {},
   "source": [
    "Now we can run the `lagom` method, which tries to find the best value. Lagom is a Swedish word that means \"just right\". The function is \"lagom\" in the way it uses your resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0340233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ecc88b68ab4781934200d3b3249031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=2.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 1628) (hopsworks0.logicalclocks.com executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 418, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 932, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/maggy/core/trialexecutor.py\", line 149, in _wrapper_fun\n",
      "    retval = train_fn(**parameters)\n",
      "  File \"<stdin>\", line 6, in training_function\n",
      "NameError: name 'y_train' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 418, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 932, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/maggy/core/trialexecutor.py\", line 149, in _wrapper_fun\n",
      "    retval = train_fn(**parameters)\n",
      "  File \"<stdin>\", line 6, in training_function\n",
      "NameError: name 'y_train' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/maggy/experiment.py\", line 224, in lagom\n",
      "    nodeRDD.foreachPartition(\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 937, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1235, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1224, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1078, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 949, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 1628) (hopsworks0.logicalclocks.com executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 418, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 932, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/maggy/core/trialexecutor.py\", line 149, in _wrapper_fun\n",
      "    retval = train_fn(**parameters)\n",
      "  File \"<stdin>\", line 6, in training_function\n",
      "NameError: name 'y_train' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2916, in pipeline_func\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 418, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 932, in func\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/maggy/core/trialexecutor.py\", line 149, in _wrapper_fun\n",
      "    retval = train_fn(**parameters)\n",
      "  File \"<stdin>\", line 6, in training_function\n",
      "NameError: name 'y_train' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "result = experiment.lagom(train_fn=training_function, \n",
    "                          searchspace=sp,\n",
    "                          optimizer='randomsearch', \n",
    "                          direction='max',\n",
    "                          num_trials=2,\n",
    "                          name='fraud_lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14e5f7",
   "metadata": {},
   "source": [
    "The function returns a dict with results from our experiment. Of special interest is of course the `best_config` dict, which contains the best hyperparameters found. Let's save this dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be922114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'result' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'result' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84c32f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'result' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'result' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"best_params.pickle\", \"wb\") as f:\n",
    "    pickle.dump(result[\"best_hp\"], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942af85",
   "metadata": {},
   "source": [
    "You can also upload this file to your cluster using the *hopsworks* library. To do this you would run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0bb4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Resources/best_params.pickle\n",
      "Uploading: 0.000%|          | 0/0 elapsed<00:00 remaining<?"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "hopsworks_conn = hopsworks.connection()\n",
    "project = hopsworks_conn.get_project()\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"best_params.pickle\", \"Resources\")\n",
    "print(uploaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c1967",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll look at how to register a model to the [Hopsworks Model Registry](https://docs.hopsworks.ai/machine-learning-api/latest), which enables us to version control our models and easily create APIs for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}