{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5be995-f5a8-45f4-9499-ff1f17fd6c02",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Load, Engineer & Connect</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This is the first part of the quick start series of tutorials about Hopsworks Feature Store. As part of this first module, we will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store**  for batch data with a goal of training and deploying a model that can predict fraudulent transactions.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 4 sections:** \n",
    "1. Loading the data and do feature engineeing,\n",
    "2. Connect to the Hopsworks feature store,\n",
    "3. Create feature groups and upload them to the feature store.\n",
    "4. Explore feature groups from the UI.\n",
    "\n",
    "![tutorial-flow](../../images/01_featuregroups.png)\n",
    "\n",
    "First of all we will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddd437",
   "metadata": {},
   "source": [
    "### üìù Import librararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5413d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb424a0b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n",
    "\n",
    "The data we will use comes from three different CSV files:\n",
    "\n",
    "- `transactions.csv`: transaction information such as timestamp, location, and the amount. \n",
    "- `alert_transactions.csv`: Suspicious Activity Report (SAR) transactions.\n",
    "- `party.csv`: User profile information.\n",
    "\n",
    "In a production system, these CSV files would originate from separate data sources or tables, and probably separate data pipelines. **All three files have a customer id column `id` in common, which we can use for joins.**\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58521b",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\", parse_dates = ['tran_timestamp'])\n",
    "transactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5e6e9",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Alert Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/alert_transactions.csv\")\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001931a6",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Party dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd533f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "party = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\")\n",
    "party.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb4301",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n",
    "\n",
    "#### To investigate patterns of suspicious activities you will make time window aggregates such monthly frequency, total, mean and standard deviation of amount of incoming and outgoing transasactions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename some columns\n",
    "transactions_df = transactions_df.rename(columns={\"src\": \"source\",\n",
    "                                                  \"dst\": \"target\"}, errors=\"raise\")\n",
    "\n",
    "# select interested columns\n",
    "transactions_df = transactions_df[[\"source\", \"target\", \"tran_timestamp\", \"tran_id\", \"base_amt\"]]\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1be7c",
   "metadata": {},
   "source": [
    "##### Outgoing transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c57b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'source'])\\\n",
    "                            .agg(monthly_count=('source','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std')\n",
    "                                )\n",
    "out_df = out_df.reset_index(level=[\"source\"])\n",
    "out_df = out_df.reset_index(level=[\"tran_timestamp\"])\n",
    "\n",
    "# rename some columns\n",
    "out_df = out_df.rename(columns={\"source\": \"id\",\n",
    "                                                  \"monthly_count\": \"monthly_out_count\",\n",
    "                                                  \"monthly_total_amount\": \"monthly_out_total_amount\",\n",
    "                                                  \"monthly_mean_amount\": \"monthly_out_mean_amount\",\n",
    "                                                  \"monthly_std_amount\": \"monthly_out_std_amount\"}, errors=\"raise\")\n",
    "\n",
    "out_df.tran_timestamp = out_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18609fe",
   "metadata": {},
   "source": [
    "##### Incoming transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a81af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = transactions_df.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'target'])\\\n",
    "                            .agg(monthly_count=('target','count'), \n",
    "                                 monthly_total_amount=('base_amt','sum'),\n",
    "                                 monthly_mean_amount=('base_amt','mean'),\n",
    "                                 monthly_std_amount=('base_amt','std'))\n",
    "\n",
    "in_df = in_df.reset_index(level=[\"target\"])\n",
    "in_df = in_df.reset_index(level=[\"tran_timestamp\"])\n",
    "in_df.columns  = [\"tran_timestamp\", \"id\", \"monthly_in_count\", \"monthly_in_total_amount\", \"monthly_in_mean_amount\", \"monthly_in_std_amount\"]\n",
    "in_df.tran_timestamp = in_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "in_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eb16c",
   "metadata": {},
   "source": [
    "##### Now lets join incoming and outgoing transcations datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77334cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_df = in_df.merge(out_df, on=['tran_timestamp', 'id'], how=\"outer\")\n",
    "in_out_df =  in_out_df.fillna(0)\n",
    "in_out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c9c78",
   "metadata": {},
   "source": [
    "#### Assign labels to transactions that were identified as suspicius activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels = transactions_df[[\"source\",\"target\",\"tran_id\",\"tran_timestamp\"]].merge(alert_transactions[[\"is_sar\", \"tran_id\"]], on=[\"tran_id\"], how=\"left\")\n",
    "transaction_labels.is_sar = transaction_labels.is_sar.map({True: 1, np.nan: 0})\n",
    "transaction_labels.sort_values('tran_id',inplace = True)\n",
    "transaction_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09454a6f",
   "metadata": {},
   "source": [
    "#### Now lets prepare profile (party) dataset and assign lables whether they have been reported for suspicius activity or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85391a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "party.columns = [\"id\",\"type\"]\n",
    "party.type = party.type.map({\"Individual\": 0, \"Organization\": 1})\n",
    "\n",
    "party.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a73230",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = transaction_labels[transaction_labels.is_sar ==1]\n",
    "\n",
    "alert_sources = alert_transactions[[\"source\", \"tran_timestamp\"]]\n",
    "alert_sources.columns = [\"id\", \"tran_timestamp\"]\n",
    "alert_sources.head()\n",
    "alert_targets = alert_transactions[[\"target\", \"tran_timestamp\"]]\n",
    "alert_targets.columns = [\"id\", \"tran_timestamp\"]\n",
    "\n",
    "sar_party = pd.concat([alert_sources, alert_targets], ignore_index=True)\n",
    "\n",
    "sar_party.sort_values([\"id\", \"tran_timestamp\"], ascending = [False, True])\n",
    "\n",
    "# find a 1st occurence of sar per id\n",
    "sar_party = sar_party.iloc[[sar_party.id.eq(id).idxmax() for id in sar_party['id'].value_counts().index]]\n",
    "sar_party = sar_party.groupby([pd.Grouper(key='tran_timestamp', freq='M'), 'id']).agg(monthly_count=('id','count'))\n",
    "sar_party = sar_party.reset_index(level=[\"id\"])\n",
    "sar_party = sar_party.reset_index(level=[\"tran_timestamp\"])\n",
    "sar_party.drop([\"monthly_count\"], axis=1, inplace=True)\n",
    "\n",
    "sar_party[\"is_sar\"] = sar_party[\"is_sar\"] = 1\n",
    "sar_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37177ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels = party.merge(sar_party, on=[\"id\"], how=\"left\")\n",
    "party_labels.is_sar = party_labels.is_sar.map({1.0: 1, np.nan: 0})\n",
    "max_time_stamp = datetime.datetime.utcfromtimestamp(int(max(transaction_labels.tran_timestamp.values))/1e9)\n",
    "party_labels = party_labels.fillna(max_time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc061c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4772716-71d6-4a56-91ff-7456e3125ec5",
   "metadata": {},
   "source": [
    "# Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979253d3-6c12-4ae1-866a-141302752a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "import math\n",
    "\n",
    "def convertToNumber(s):\n",
    "    return int.from_bytes(s.encode(), 'little')\n",
    "\n",
    "def convertFromNumber(n):\n",
    "    return n.to_bytes(math.ceil(n.bit_length() / 8), 'little').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e993ed-632f-4d20-83ce-82a4c0f597c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = GraphDataScience('bolt://localhost:7687', auth=('neo4j', 'hopsworks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a6dd6-fe0b-49ff-a94d-187470004645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pupulate_graph(input_df):\n",
    "    # Neo4j node formatting ##################\n",
    "    \n",
    "    # Extract unique nodes visited\n",
    "    nodes = party_labels[party_labels.id.isin(input_df.source) | (party_labels.id.isin(input_df.target))]\n",
    "    nodes = nodes[['id']]    \n",
    "    nodes = nodes.rename(columns={\"id\": \"nodeId\"},\n",
    "                         errors=\"raise\")\n",
    "    \n",
    "    # Convert node ID to a positive integer (Neo4j requirements)\n",
    "    nodes['nodeId'] = [convertToNumber(nodeId) for nodeId in nodes['nodeId']]\n",
    "    nodes['nodeId'] = nodes['nodeId'].astype(int)\n",
    "\n",
    "    # Neo4j relationships formatting ##################\n",
    "    \n",
    "    relationships = input_df[['source', 'target', 'tran_id']]\n",
    "    relationships = relationships.rename(columns={\"source\": \"sourceNodeId\",\n",
    "                                                  \"target\": \"targetNodeId\"},\n",
    "                                         errors=\"raise\")\n",
    "\n",
    "    # Convert source node ID to a positive integer (Neo4j requirements)\n",
    "    relationships['sourceNodeId'] = [convertToNumber(sourceNodeId) for sourceNodeId in relationships['sourceNodeId']]\n",
    "    relationships['sourceNodeId'] = relationships['sourceNodeId'].astype(int)\n",
    "\n",
    "    # Convert target node ID to a positive integer (Neo4j requirements)\n",
    "    relationships['targetNodeId'] = [convertToNumber(targetNodeId) for targetNodeId in relationships['targetNodeId']]\n",
    "    relationships['targetNodeId'] = relationships['targetNodeId'].astype(int)\n",
    "\n",
    "    ##################\n",
    "    \n",
    "    # Build Graph\n",
    "    G = gds.graph.construct(\"transactions-graph\", nodes, relationships)\n",
    "\n",
    "    # Check if the number of nodes is correctly stored\n",
    "    assert G.node_count() == len(nodes)\n",
    "\n",
    "    # Compute embeddings\n",
    "    graph_embdeddings_df = gds.node2vec.stream(G)\n",
    "    \n",
    "    G.drop()\n",
    "\n",
    "    # Convert integer node ID back to original ID\n",
    "    graph_embdeddings_df['nodeId'] = [convertFromNumber(nodeId) for nodeId in graph_embdeddings_df['nodeId']]\n",
    "\n",
    "    return {\"id\": graph_embdeddings_df.nodeId.to_numpy(), \"graph_embeddings\": graph_embdeddings_df.embedding.to_numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cad42b-11c9-47e5-823d-b2b1580aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "transaction_graphs_by_month = transaction_labels.groupby(pd.Grouper(key='tran_timestamp', freq='M')).progress_apply(lambda x: construct_graph(x))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b5f9a-8cb5-44a4-8374-9345afe8ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = transaction_graphs_by_month.index.values\n",
    "graph_embeddings = transaction_graphs_by_month.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c4c84-c099-464a-8a85-60b86fb46cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df = pd.DataFrame()\n",
    "for timestamp, graph_embedding in zip(timestamps, graph_embeddings):\n",
    "    df_tmp = pd.DataFrame(graph_embedding)\n",
    "    df_tmp[\"tran_timestamp\"] = timestamp\n",
    "    graph_embdeddings_df = pd.concat([graph_embdeddings_df, df_tmp])    \n",
    "graph_embdeddings_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee21bbb-a957-4132-a978-ba38b5a6affc",
   "metadata": {},
   "source": [
    "#### Convert date time to unix epoc milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44048408-825f-4fe9-8ae2-1944eebc07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "graph_embdeddings_df.tran_timestamp = graph_embdeddings_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)\n",
    "\n",
    "transaction_labels['month'] = pd.to_datetime(transaction_labels['tran_timestamp'], unit='ms').dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10691e48-5683-43b0-aba9-547249afd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique nodes\n",
    "nodes = pd.DataFrame({'id': party_labels['id'].drop_duplicates()})\n",
    "\n",
    "nodes = nodes.rename(columns={\"id\": \"nodeId\"},\n",
    "                     errors=\"raise\")\n",
    "\n",
    "nodes['nodeId'] = [convertToNumber(nodeId) for nodeId in nodes['nodeId']]\n",
    "nodes['nodeId'] = nodes['nodeId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0a22a-6649-4eaf-aa57-92f46298a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41e9b6-d766-47f6-920a-de265cf25310",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834314d-968a-4f44-beb5-124951876833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename transactions into relationships\n",
    "relationships = transaction_labels[['source', 'target', 'tran_id']]\n",
    "\n",
    "relationships = relationships.rename(columns={\"source\": \"sourceNodeId\",\n",
    "                                              \"target\": \"targetNodeId\"},\n",
    "                                     errors=\"raise\")\n",
    "\n",
    "relationships['sourceNodeId'] = [convertToNumber(sourceNodeId) for sourceNodeId in relationships['sourceNodeId']]\n",
    "relationships['sourceNodeId'] = relationships['sourceNodeId'].astype(int)\n",
    "\n",
    "relationships['targetNodeId'] = [convertToNumber(targetNodeId) for targetNodeId in relationships['targetNodeId']]\n",
    "relationships['targetNodeId'] = relationships['targetNodeId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c2e6e-0460-4bf1-8b5f-5adb66c3b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c963f4-5804-47bf-b142-79bcbcd685a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6b479-1051-42d2-8de6-25a7a023c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gds.graph.construct(\"transactions-graph\", nodes, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ad90c-8366-4f41-a4b2-0bcfed4e96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b52d2e8-94b0-4264-a3aa-0d68b03de2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1d661-89e0-4dd9-941a-eb7f822346e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if number of nodes is correctly stored\n",
    "G.node_count() == len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbef4f-2cad-48ab-b0d5-498feb217500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the graph is no longer needed, it should be dropped to free up memory\n",
    "#G.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28424261-1a88-4075-8dc0-85754fac5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df = gds.node2vec.stream(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb55c4-a039-47d3-b9cf-86126edae4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9125a-3a7e-44ad-9fe7-d0fed465bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df['nodeId'] = [convertFromNumber(nodeId) for nodeId in graph_embdeddings_df['nodeId']]\n",
    "\n",
    "graph_embdeddings_df = graph_embdeddings_df.rename(columns={\"nodeId\": \"id\",\n",
    "                                                           \"embedding\": \"graph_embeddings\"},\n",
    "                                                   errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebba909-f216-49c6-b418-ee21d56ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014de9be",
   "metadata": {},
   "source": [
    "#### Compute time evolving graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_graphs_by_month = transaction_labels.groupby(pd.Grouper(key='tran_timestamp', freq='M')).apply(lambda x: construct_graph(x))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = transaction_graphs_by_month.index.values\n",
    "graph_embeddings = transaction_graphs_by_month.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embdeddings_df = pd.DataFrame()\n",
    "for timestamp, graph_embedding in zip(timestamps, graph_embeddings):\n",
    "    df_tmp = pd.DataFrame(graph_embedding)\n",
    "    df_tmp[\"tran_timestamp\"] = timestamp\n",
    "    graph_embdeddings_df = pd.concat([graph_embdeddings_df, df_tmp])    \n",
    "graph_embdeddings_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d131c06",
   "metadata": {},
   "source": [
    "#### Convert date time to unix epoc milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df92022",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "graph_embdeddings_df.tran_timestamp = graph_embdeddings_df.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ed756",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üëÆüèº‚Äç‚ôÄÔ∏è Data Validation \n",
    "\n",
    "Before you define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. You do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value you can decide whether to stop it to `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section you will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation with Greate Expectations in Hopsworks\n",
    "You can use GE library for validation in Hopsworks features store. \n",
    "\n",
    "##  <img src=\"../../images/icon102.png\" width=\"18px\"></img> Hopsworks feature store\n",
    "\n",
    "The Hopsworks feature feature store library is Apache V2 licensed and available [here](https://github.com/logicalclocks/feature-store-api). The library is currently available for Python and JVM languages such as Scala and Java.\n",
    "In this notebook, we are going to cover Python part.\n",
    "\n",
    "You can find the complete documentation of the library here: \n",
    "\n",
    "The first step is to establish a connection with your Hopsworks feature store instance and retrieve the object that represents the feature store you'll be working with. \n",
    "\n",
    "> By default `connection.get_feature_store()` returns the feature store of the project we are working with. However, it accepts also a project name as parameter to select a different feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f38294a",
   "metadata": {},
   "source": [
    "### üî¨ Expectations suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Expectation Suite - no use of HSFS\n",
    "import great_expectations as ge\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "expectation_suite = ge.core.ExpectationSuite(expectation_suite_name=\"aml_project_validations\")\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite.add_expectation(\n",
    "  ge.core.ExpectationConfiguration(\n",
    "  expectation_type=\"expect_column_max_to_be_between\",\n",
    "  kwargs={\"column\": \"monthly_in_count\", \"min_value\": 0, \"max_value\": 10000000}) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5517d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18207eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Register Feature Groups </span>\n",
    "\n",
    "### Feature Groups\n",
    "\n",
    "A `Feature Groups` is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The `Feature Group` lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them.\n",
    "\n",
    "Generally, the features in a feature group are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, `feature groups` provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in.\n",
    "\n",
    "> It is important to note that `feature groups` are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0fcb6",
   "metadata": {},
   "source": [
    "#### Transactions monthly aggregates feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    partition_key = [\"tran_timestamp\"],   \n",
    "    description = \"transactions monthly aggregates features\",\n",
    "    event_time = 'tran_timestamp',\n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False},\n",
    "    expectation_suite=expectation_suite\n",
    ")   \n",
    "\n",
    "transactions_fg.insert(in_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb4fe1",
   "metadata": {},
   "source": [
    "#### Alert Transaction labels feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de49372",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_labels_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transaction_labels_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"tran_id\"],\n",
    "    partition_key = [\"month\"],         \n",
    "    description = \"alert transactions\",\n",
    "    event_time = 'tran_timestamp',    \n",
    "    online_enabled = True,                                                \n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "transaction_labels_fg.insert(transaction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639f1f2",
   "metadata": {},
   "source": [
    "#### Party feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_fg = fs.get_or_create_feature_group(\n",
    "    name = \"party_aml_fg\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    description = \"party fg with labels\",\n",
    "    event_time = 'tran_timestamp',        \n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "party_fg.insert(party_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa61640",
   "metadata": {},
   "source": [
    "#### Graph embeddings feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embeddings_fg = fs.get_or_create_feature_group(name=\"graph_embeddings_aml_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions graph\",\n",
    "                                       event_time = 'tran_timestamp',     \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}\n",
    "                                       )\n",
    "\n",
    "graph_embeddings_fg.insert(graph_embdeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fee7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01883b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëì Exploration </span>\n",
    "\n",
    "### Feature groups are now accessible and searchable in the UI\n",
    "![fg-overview](images/fg_explore.gif)\n",
    "\n",
    "## üìä Statistics\n",
    "We can explore feature statistics in the feature groups. If statistics was not enabled when feature group was created then this can be done by:\n",
    "\n",
    "```python\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly_fg\", \n",
    "    version = 1)\n",
    "\n",
    "transactions_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "transactions_fg.update_statistics_config()\n",
    "transactions_fg.compute_statistics()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6b329",
   "metadata": {},
   "source": [
    "![fg-stats](images/freature_group_stats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf84cc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 02 </span>\n",
    "    \n",
    "In the following notebook you will use feature groups to create feature viewa and training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
