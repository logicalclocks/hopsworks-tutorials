{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c382383",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff5f27\"> üë®üèª‚Äçüè´ Sklearn Transformation Functions Registration</span>\n",
    "\n",
    "In this tutorial you will learn how to **register sklearn.pipeline with transformation functions and classifier** in Hopsworks Model Registry and use it in **training and inference pipelines**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994e496",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üóÑÔ∏è Table of Contents</span>\n",
    "- [üìù Imports](#1)\n",
    "- [‚õ≥Ô∏è Feature Pipeline](#t1)\n",
    "    - [üíΩ Loading Data](#2)\n",
    "    - [üîÆ Connecting to Hopsworks Feature Store](#3)\n",
    "    - [ü™Ñ Creating Feature Groups](#4)\n",
    "- [‚õ≥Ô∏è Training Pipeline](#t2)\n",
    "    - [üñç Feature View Creation](#5)\n",
    "    - [üë©üèª‚Äçüî¨ Data Transformation Pipeline](#6)\n",
    "    - [üß¨ Modeling](#7)\n",
    "    - [üíæ Saving the Model in the Model Registry](#8)\n",
    "- [‚õ≥Ô∏è Inference Pipeline](#t3)\n",
    "    - [üìÆ Retrieving the Model from the Model Registry](#9)\n",
    "    - [üë®üèª‚Äç‚öñÔ∏è Batch Prediction](#10)\n",
    "    - [üë®üèª‚Äç‚öñÔ∏è Serving Feature Vector Prediction](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8743b40",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## <span style='color:#ff5f27'> üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd          # Pandas for data manipulation\n",
    "import numpy as np           # NumPy for numerical operations\n",
    "import os                    # os for interacting with the operating system\n",
    "import joblib                # joblib for saving and loading models\n",
    "\n",
    "# Importing classes and functions from scikit-learn for data preprocessing and modeling\n",
    "from sklearn.compose import ColumnTransformer   # For transforming specific columns in a DataFrame\n",
    "from sklearn.pipeline import Pipeline           # For creating a data processing pipeline\n",
    "from sklearn.impute import SimpleImputer        # For handling missing values\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder   # For feature scaling and one-hot encoding\n",
    "from sklearn.feature_selection import SelectPercentile, chi2    # For feature selection using chi-squared test\n",
    "\n",
    "# Importing XGBoost library for gradient boosting-based machine learning\n",
    "import xgboost as xgb\n",
    "\n",
    "# Importing a metric to evaluate the model's performance\n",
    "from sklearn.metrics import accuracy_score      # For measuring classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7b685",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='t1'></a>\n",
    "# <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Feature Pipeline </span>\n",
    "\n",
    "In this section you will load data, create a Hopsworks feature group and insert your dataset into created feature group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ca429",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading Data </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610b404",
   "metadata": {},
   "source": [
    "To begin with, let's load a dataset which contains air quality measurements for different  cities from 2013-01-01 to 2023-04-11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7aaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_original = pd.read_csv(\"https://repo.hops.works/dev/davit/air_quality/backfill_pm2_5_eu.csv\")\n",
    "df_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be569d79",
   "metadata": {},
   "source": [
    "After loading the data, let's add a new column called 'target' to the DataFrame. \n",
    "\n",
    "The purpose of this column is to serve as the target variable for a binary classification task. In binary classification, we aim to predict one of two possible outcomes (in this case, either 0 or 1) based on the input features (other columns in the DataFrame).\n",
    "\n",
    "To create the **target** column, you will use the `np.random.choice` function from the NumPy library. It randomly assigns either a 0 or a 1 to each row in the DataFrame. \n",
    "\n",
    "This randomness helps simulate a scenario where we don't have the actual target values yet, so we create a placeholder **for demonstration purposes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary target column\n",
    "df_original['target'] = np.random.choice([0, 1], size=len(df_original))\n",
    "df_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd983ab",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>\n",
    "\n",
    "The next step is to login to the Hopsworks platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4826fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f77b7b",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## <span style=\"color:#ff5f27;\">ü™Ñ Creating Feature Groups</span>\n",
    "\n",
    "Now you need to create a Feature Group and insert your dataset.\n",
    "\n",
    "You will use `.get_or_create_feature_group()` method of the feature store object with the next parameters:\n",
    "\n",
    "- name='feature_group_online': This specifies the name of the feature group as 'feature_group_online'.\n",
    "\n",
    "- description='Online Feature Group': A brief description provided for the feature group.\n",
    "\n",
    "- version=1: The version number of the feature group.\n",
    "\n",
    "- primary_key=['city_name', 'date']: The primary key of the feature group. It indicates which columns in the DataFrame uniquely identify each record.\n",
    "\n",
    "- online_enabled=True: This parameter enables the feature group for online serving, meaning it can be used in real-time for making predictions.\n",
    "\n",
    "The `feature_group.insert(df_original)` statement inserts the `df_original` DataFrame into the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = fs.get_or_create_feature_group(\n",
    "    name='feature_group_online',\n",
    "    description='Online Feature Group',\n",
    "    version=1,\n",
    "    primary_key=['city_name', 'date'],\n",
    "    online_enabled=True,\n",
    ")    \n",
    "feature_group.insert(df_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cc0c2",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='t2'></a>\n",
    "# <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Training Pipeline </span>\n",
    "\n",
    "In the **Training Pipeline** you will create a train-test split, define data preprocessing steps and put then into sklearn.pipeline with the XGBClassifier. Then you will register your sklearn.pipeline into Hopsworks Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e3352",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üñç Feature View Creation</span>\n",
    "\n",
    "In this part you will build a Query object and create a feature view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f102889",
   "metadata": {},
   "source": [
    "In Hopsworks Feature Store, a Query object allows you to select specific features from a feature group.\n",
    "\n",
    "`feature_group.select_except(['date'])` selects all columns from the feature group except for the 'date' column. It means that the 'date' column will be excluded from the Query object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35959639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Query object\n",
    "query = feature_group.select_except(['date'])\n",
    "query.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c8a83",
   "metadata": {},
   "source": [
    "After creating the Query object, you will create a feature view.\n",
    "\n",
    "A feature view is a logical representation of data which can be used for real-time serving or batch processing. \n",
    "\n",
    "The `.get_or_create_feature_view()` method has the next parameters:\n",
    "\n",
    "- name='serving_fv': This specifies the name of the feature view.\n",
    "\n",
    "- version=1: The version number of the feature view.\n",
    "\n",
    "- query=query: The Query object created earlier is provided as input to the feature view. It determines the set of required features.\n",
    "\n",
    "- labels=['target']: This parameter defines the target column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='serving_fv',\n",
    "    version=1,\n",
    "    query=query,\n",
    "    labels=['target'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554ced8",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "The next step is to create the train-test split of your data.\n",
    "\n",
    "Let's clarify the next parameters of the `.create_train_test_split()` method:\n",
    "\n",
    "- test_size=0.1: This parameter specifies the size of the test set relative to the entire dataset. In this case, the test set will contain 10% of the data, and the train set will have the remaining 90%.\n",
    "\n",
    "- description='Description of the dataset': A brief description provided for the train-test split dataset, explaining its purpose or any other relevant information.\n",
    "\n",
    "- data_format='csv': This parameter specifies the format in which the train-test split dataset will be stored. Here, it is set to 'csv', meaning the dataset will be saved in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb20081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train-test split dataset\n",
    "td_version, job = feature_view.create_train_test_split(\n",
    "    test_size=0.1,\n",
    "    description='Description of the dataset',\n",
    "    data_format='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502740c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">ü™ù Training Dataset Retrieval</span>\n",
    "\n",
    "To retrieve your train_test_split you can use the `.get_train_test_split()` method of the feature_view object.\n",
    "\n",
    "The parameter `training_dataset_version` specifies the version number of the train-test split dataset to retrieve. \n",
    "\n",
    "`td_version` is the version number that was obtained when the train-test split dataset was created in a previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a938b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the train-test split\n",
    "X_train, X_test, y_train, y_test = feature_view.get_train_test_split(\n",
    "    training_dataset_version=td_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a830bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8342245",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7bee3",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## <span style=\"color:#ff5f27;\">üë©üèª‚Äçüî¨ Data Transformation Pipeline</span>\n",
    "\n",
    "The next step is to build a Data Transformation Pipeline.\n",
    "\n",
    "A pipeline allows you to chain multiple data preprocessing steps and machine learning model into a single object. This is particularly useful for automating and simplifying the workflow in a machine learning project, as it ensures that all the necessary steps are applied consistently to both the training and testing data.\n",
    "\n",
    "Let's define transformation steps for numeric and categorical features and put then into pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pipeline for numeric features\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "# Define a Pipeline for categorical features\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabded2",
   "metadata": {},
   "source": [
    "- `SimpleImputer transformer` is used to handle missing values in numeric features. The strategy chosen is \"median,\" meaning that missing values will be replaced with the median of the existing values in that feature.\n",
    "\n",
    "- `StandardScaler` is used to standardize the numeric features so that they have a mean of 0 and a standard deviation of 1. Standardization is essential to ensure that features are on similar scales, which can be beneficial for certain machine learning algorithms.\n",
    "\n",
    "- `OneHotEncoder` is used to convert categorical features into a binary representation, where each category is transformed into a binary vector (0s and 1s). The **handle_unknown=\"ignore\"** parameter allows the encoder to ignore unknown categories during transformation, avoiding errors when encountering unseen categories during inference.\n",
    "\n",
    "- `SelectPercentile` is used to select the most informative features from the encoded binary vectors based on their chi-squared statistics. The parameter percentile=50 indicates that the top 50% most informative features will be retained, and the rest will be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b633fab",
   "metadata": {},
   "source": [
    "Then you need to use `ColumnTransformer` to combine several transformers into a single transformer. \n",
    "\n",
    "It allows you to specify different preprocessing steps for specific subsets of columns of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, ['pm2_5']),\n",
    "        (\"cat\", categorical_transformer, ['city_name']),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09cf48d",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## <span style=\"color:#ff5f27;\">üß¨ Model Fit</span>\n",
    "\n",
    "Now let's combine the `preprocessor` and `XGBClassifier` into one **pipeline** and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5019a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "xgb_classifier = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", xgb.XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "# Fit the classifier\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"üëÆüèª‚Äç‚ôÇÔ∏è Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bedc3e",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üóÑ Model Registry</span>\n",
    "\n",
    "In Hopsworks, the Model Registry is a crucial component used to manage and version machine learning models. It acts as a centralized repository where trained models can be stored, tracked, and shared among team members.\n",
    "\n",
    "By calling `project.get_model_registry()`, the code retrieves a reference to the Model Registry associated with the current Hopsworks project. This reference allows the user to interact with the Model Registry and perform operations such as registering, versioning, and accessing trained machine learning models.\n",
    "With the Model Registry, data scientists and machine learning engineers can effectively collaborate, track model changes, and easily deploy the best-performing models to production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a73fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbde875",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The next step is to **define input and output schema** of a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train.values)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6cb84",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "### <span style=\"color:#ff5f27;\">üíæ Saving the Model</span>\n",
    "\n",
    "Now you are ready to register your model in the Hopsworks Moder Registry.\n",
    "\n",
    "To begin with, let's create the `xgb_tf_model` model directory and save the trained model in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"xgb_tf_model\"\n",
    "\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "joblib.dump(xgb_classifier, model_dir + '/xgb_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d3c34",
   "metadata": {},
   "source": [
    "To register your model in the Hopsworks model registry you can use `.create_model()` method with the next parameters:\n",
    "\n",
    "- name=\"xgb_model\": The name of the model.\n",
    "\n",
    "- metrics={\"Accuracy\": accuracy}: The model's performance metrics are specified as a dictionary, with \"Accuracy\" as the key and the value being the accuracy score computed earlier in the code. This metric represents the accuracy of the model's predictions on the test data.\n",
    "\n",
    "- description=\"XGB model\": A brief description of the model.\n",
    "\n",
    "- input_example=X_train.sample(): An example input from the training data (X_train) is used to demonstrate the expected format of the model's input data. It is randomly sampled from X_train.\n",
    "\n",
    "- model_schema=model_schema: The model schema, which represents the data input and output structure of the model, is specified using the previously defined model_schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7415dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model in the model registry\n",
    "model = mr.python.create_model(\n",
    "    name=\"xgb_model\",\n",
    "    metrics={\"Accuracy\": accuracy}, \n",
    "    description=\"XGB model\",\n",
    "    input_example=X_train.sample(),\n",
    "    model_schema=model_schema\n",
    ")\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5df30d",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='t3'></a>\n",
    "# <span style=\"color:#ff5f27;\">‚õ≥Ô∏è Inference Pipeline </span>\n",
    "\n",
    "In the **Inference Pipeline** section, you will retrieve your model from Hopsworks Model Registry and utilize this model to make predictions on both Batch Data and Serving Feature Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e238f",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üìÆ Retrieving the Model from Model Registry </span>\n",
    "\n",
    "To retrieve a previously registered machine learning model from the Hopsworks Model Registry you need to use the `.get_model()` method with the next parameters:\n",
    "\n",
    "- name=\"xgb_model\": The name of the model to be retrieved.\n",
    "\n",
    "- version=1: The version number of the model to be retrieved.\n",
    "\n",
    "Then you will download the model from the Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b287444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your model from the model registry\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"xgb_model\",\n",
    "    version=1\n",
    ")\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_xgboost_model = joblib.load(saved_model_dir + \"/xgb_classifier.pkl\")\n",
    "retrieved_xgboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2e23e",
   "metadata": {},
   "source": [
    "<a name='10'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üë®üèª‚Äç‚öñÔ∏è Batch Prediction </span>\n",
    "\n",
    "Batch prediction is a process in which a trained machine learning model is used to make predictions on a large set of data all at once.\n",
    "\n",
    "It's important to note that while batch prediction is efficient for large-scale data processing, it may not be suitable for applications that require immediate, real-time predictions, such as online recommendations or fraud detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcee23e",
   "metadata": {},
   "source": [
    "To retrieve batch data from the feature view you need to use `init_batch_scoring` method of the feature view object.\n",
    "\n",
    "`training_dataset_version` parameter specifies the version number of the training dataset that will be used for scoring.\n",
    "\n",
    "Then you can use the `.get_batch_data()` method to retrieve batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94123860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise feature view to retrieve batch data\n",
    "feature_view.init_batch_scoring(training_dataset_version=td_version)\n",
    "\n",
    "# Retrieve batch data\n",
    "batch_data = feature_view.get_batch_data()\n",
    "batch_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb60042",
   "metadata": {},
   "source": [
    "Now let's use retrieved model to predict batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict batch data using retrieved model\n",
    "predictions_batch = retrieved_xgboost_model.predict(batch_data)\n",
    "predictions_batch[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4631d",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "## <span style=\"color:#ff5f27;\"> üë®üèª‚Äç‚öñÔ∏è Serving Feature Vector Prediction</span>\n",
    "\n",
    "**Serving Feature Vector Prediction** is a process of using a trained machine learning model to make predictions on feature vector(s) in real-time. \n",
    "\n",
    "In contrast to batch prediction, where predictions are made on a large set of data simultaneously, serving feature vector prediction focuses on making predictions on-demand, usually as new data points arrive or as users interact with a system.\n",
    "\n",
    "To begin with, let's create `to_df` function which will transform a feature vector(s) list into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(feature_vector):\n",
    "    \"\"\"\n",
    "    Convert a feature vector or a list of feature vectors into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        feature_vector (a list, or list of lists): \n",
    "            A feature vector or a list of feature vectors. A feature vector is \n",
    "            represented as a list containing two elements: the first \n",
    "            element corresponds to the city name (categorical feature), and the \n",
    "            second element corresponds to the PM2.5 value (numerical feature).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame representing the feature vector(s). \n",
    "        The DataFrame will have two columns: 'city_name' for the city names \n",
    "        and 'pm2_5' for the corresponding PM2.5 values.\n",
    "\n",
    "    Example:\n",
    "        >>> feature_vector = ['New York', 15.3]\n",
    "        >>> to_df(feature_vector)\n",
    "           city_name  pm2_5\n",
    "        0  New York   15.3\n",
    "\n",
    "        >>> multiple_vectors = [['New York', 15.3], ['Los Angeles', 10.7]]\n",
    "        >>> to_df(multiple_vectors)\n",
    "          city_name  pm2_5\n",
    "        0  New York   15.3\n",
    "        1  Los Angeles 10.7\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the input is a list of feature vectors\n",
    "    if isinstance(feature_vector[0], list): \n",
    "        # Separate the city names and PM2.5 values into separate lists\n",
    "        city_names = [vector[0] for vector in feature_vector]\n",
    "        pm2_5_values = [vector[1] for vector in feature_vector]\n",
    "        \n",
    "        # Create a DataFrame with 'city_name' and 'pm2_5' columns from the lists\n",
    "        data = pd.DataFrame(\n",
    "            {\n",
    "                'city_name': city_names,\n",
    "                'pm2_5': pm2_5_values,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Return the DataFrame representing multiple feature vectors\n",
    "        return data\n",
    "\n",
    "    # If only one feature vector is provided, create a DataFrame for it\n",
    "    data = pd.DataFrame(\n",
    "            {\n",
    "                'city_name': [feature_vector[0]],\n",
    "                'pm2_5': [feature_vector[1]],\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Return the DataFrame representing a single feature vector\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69dde7e",
   "metadata": {},
   "source": [
    "The next step is to initialize the feature view for serving and then retrieve a feature vector with specified primary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise feature view to retrieve feature vector\n",
    "feature_view.init_serving(1)\n",
    "\n",
    "# Retrieve a feature vector\n",
    "feature_vector = feature_view.get_feature_vector(\n",
    "    entry = {\n",
    "        \"city_name\": 'Amsterdam',\n",
    "        \"date\": '2013-01-01',\n",
    "    }\n",
    ")\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec97144",
   "metadata": {},
   "source": [
    "Let's apply `to_df` function in order to transform the feature vector into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac308069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform feature vector to pandas dataframe\n",
    "feature_vector_df = to_df(feature_vector)\n",
    "feature_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6455ff0",
   "metadata": {},
   "source": [
    "Now you can use your model to predict the feature vector dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict transformed feature vector using retrieved model\n",
    "prediction_feature_vector = retrieved_xgboost_model.predict(feature_vector_df)\n",
    "prediction_feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0636139",
   "metadata": {},
   "source": [
    "In addition, you can retrieve several feature vectors. Just pass primary keys as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac541e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature vectors from feature store\n",
    "feature_vectors = feature_view.get_feature_vectors(\n",
    "    entry = [\n",
    "        {\"city_name\": 'Amsterdam', \"date\": '2013-01-01'},\n",
    "        {\"city_name\": 'Amsterdam', \"date\": '2014-01-01'},\n",
    "    ]\n",
    ")\n",
    "feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c156e4b",
   "metadata": {},
   "source": [
    "Apply `to_df` function in order to transform feature vectors into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature vectors to pandas dataframe\n",
    "feature_vectors_df = to_df(feature_vectors)\n",
    "feature_vectors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d798d6",
   "metadata": {},
   "source": [
    "Now you can use your model to predict the dataframe which contains feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict transformed feature vectors using retrieved model\n",
    "prediction_feature_vectors = retrieved_xgboost_model.predict(feature_vectors_df)\n",
    "prediction_feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1149b1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
