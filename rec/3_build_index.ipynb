{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index\n",
    "\n",
    "In this notebook we will build an index for our candidate embeddings. Here we will use OpenSearch, which is natively supported by Hopsworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Candidate Embeddings\n",
    "\n",
    "We start by computing candidate embeddings for all items in the training data.\n",
    "\n",
    "First, we load our candidate model. Recall that we uploaded it to the Hopsworks Model Registry in the previous notebook. If you don't have the model locally you can download it from the Model Registry using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "model = mr.get_model(\"candidate_model\")\n",
    "model_path = model.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the model saved locally you can simply replace `model_path` with the path to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "candidate_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the embeddings of all candidate items that were used to train the retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()\n",
    "\n",
    "# Load training dataset.\n",
    "td = fs.get_training_dataset(\"retrieval_fv_1\")\n",
    "train_df = td.read(\"train\")\n",
    "\n",
    "# Get list of input features for the candidate model.\n",
    "model_schema = model.model_schema['input_schema']['columnar_schema']\n",
    "candidate_features = [feat['name'] for feat in model_schema]\n",
    "\n",
    "# Get list of unique candidate items.\n",
    "item_df = train_df[candidate_features]\n",
    "item_df.drop_duplicates(subset=\"article_id\", inplace=True)\n",
    "\n",
    "item_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    {col: item_df[col] for col in item_df})\n",
    "\n",
    "# Compute embeddings for all candidate items.\n",
    "candidate_embeddings = item_ds.batch(2048).map(\n",
    "    lambda x: (x[\"article_id\"], candidate_model(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Strictly speaking, we haven't actually computed the candidate embeddings yet, as the dataset functions are lazily evaluated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Embeddings\n",
    "\n",
    "Next we index these embeddings. We start by connecting to our project's OpenSearch client using the *hopsworks* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "connection = hopsworks.connection()\n",
    "project = connection.get_project()\n",
    "opensearch_api = project.get_opensearch_api()\n",
    "\n",
    "client = OpenSearch(**opensearch_api.get_default_py_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an index called `candidate_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = opensearch_api.get_project_index(\"candidate_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the HNSW (Hierarchical Navigable Small World) data structure, which can be thought of as a skip list for graphs.\n",
    "\n",
    "See the [OpenSearch documentation](https://opensearch.org/docs/latest/search-plugins/knn/knn-index) for more detailed information about parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of candidate embeddings.\n",
    "emb_dim = candidate_model.layers[-1].output.shape[-1]\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"knn\": True,\n",
    "        \"knn.algo_param.ef_search\": 100,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"my_vector1\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": emb_dim,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.indices.create(index_name, body=index_body)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally insert our candidate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy.helpers import bulk\n",
    "\n",
    "actions = []\n",
    "for batch in candidate_embeddings:\n",
    "    item_id_list, embedding_list = batch\n",
    "    item_id_list = item_id_list.numpy().astype(int)\n",
    "    embedding_list = embedding_list.numpy()\n",
    "\n",
    "    for item_id, embedding in zip(item_id_list, embedding_list):\n",
    "        actions.append({\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": item_id,\n",
    "            \"_source\": {\n",
    "                \"my_vector1\": embedding,\n",
    "            }\n",
    "        })\n",
    "\n",
    "# Bulk insertion.\n",
    "bulk(client, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that it works we can retrieve the neighbors of a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO would be more illustrative to select a vector from 'actions'\n",
    "# and join the results with item_df.\n",
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "embedding = np.random.rand(emb_dim)\n",
    "\n",
    "query = {\n",
    "  \"size\": 10,\n",
    "  \"query\": {\n",
    "    \"knn\": {\n",
    "      \"my_vector1\": {\n",
    "        \"vector\": embedding,\n",
    "        \"k\": 10\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "\n",
    "At this point we have a recommender system that is able to generate a set of candidate items for a customer. However, many of these could be poor, as the candidate model was trained with only a few subset of the features. In the next notebook, we'll train a *ranking model* to do more fine-grained predictions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
