{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb83ff8",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Data & Feature views</span>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/advanced_tutorials/air_quality/3_feature_views_and_training_dataset.ipynb)\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This notebook explains how to read from a feature group and create training dataset within the feature store</span>\n",
    "\n",
    "## 🗒️ This notebook is divided into the following sections:\n",
    "\n",
    "1. Fetch Feature Groups\n",
    "2. Define Transformation functions\n",
    "4. Create Feature Views\n",
    "5. Create Training Dataset with training, validation and test splits\n",
    "\n",
    "![part2](../../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3bcd1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> 📡 Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ad779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/14502\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735a083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_fg = fs.get_or_create_feature_group(\n",
    "    name = 'air_quality',\n",
    "    version = 1\n",
    ")\n",
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name = 'weather',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54079736-1cce-4284-9434-27a18285142a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hopsworks==3.2.0rc0\n",
      "  Downloading hopsworks-3.2.0rc0.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hsfs[python]~=3.2.0rc0\n",
      "  Downloading hsfs-3.2.0rc0.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hsml~=3.2.0rc0\n",
      "  Downloading hsml-3.2.0rc0.tar.gz (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyhumps==1.6.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (1.6.1)\n",
      "Requirement already satisfied: requests in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (2.28.2)\n",
      "Requirement already satisfied: furl in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (2.1.3)\n",
      "Requirement already satisfied: boto3 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (1.26.98)\n",
      "Requirement already satisfied: pyjks in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (20.0.0)\n",
      "Requirement already satisfied: mock in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (5.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hopsworks==3.2.0rc0) (4.65.0)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.5.3)\n",
      "Requirement already satisfied: numpy in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.23.5)\n",
      "Collecting avro==1.11.0\n",
      "  Downloading avro-1.11.0.tar.gz (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.0.7)\n",
      "Requirement already satisfied: PyMySQL[rsa] in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.0.2)\n",
      "Requirement already satisfied: great_expectations==0.14.12 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.14.12)\n",
      "Requirement already satisfied: markupsafe<2.1.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.0.1)\n",
      "Requirement already satisfied: tzlocal in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.3)\n",
      "Requirement already satisfied: pyhopshive[thrift] in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.6.4.1.dev0)\n",
      "Requirement already satisfied: pyarrow in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (11.0.0)\n",
      "Requirement already satisfied: confluent-kafka==1.8.2 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.8.2)\n",
      "Requirement already satisfied: fastavro<=1.7.3,>=1.4.11 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.4.11)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (8.0.5)\n",
      "Requirement already satisfied: jinja2<3.1.0,>=2.10 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2021.3 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2022.7)\n",
      "Requirement already satisfied: nbformat>=5.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (5.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.26.15)\n",
      "Requirement already satisfied: packaging in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (23.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (6.1.0)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.17.17)\n",
      "Requirement already satisfied: Click>=7.1.2 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (8.1.3)\n",
      "Requirement already satisfied: jsonpatch>=1.22 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.32)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.8.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.10.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.2.0)\n",
      "Requirement already satisfied: colorama>=0.4.3 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.4.6)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.17.3)\n",
      "Requirement already satisfied: mistune<2.0.0,>=0.8.4 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.8.4)\n",
      "Requirement already satisfied: cryptography>=3.2 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (39.0.2)\n",
      "Requirement already satisfied: altair<5,>=4.0.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from requests->hopsworks==3.2.0rc0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from requests->hopsworks==3.2.0rc0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from requests->hopsworks==3.2.0rc0) (3.1.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from boto3->hopsworks==3.2.0rc0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from boto3->hopsworks==3.2.0rc0) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.98 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from boto3->hopsworks==3.2.0rc0) (1.29.98)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from furl->hopsworks==3.2.0rc0) (1.0.1)\n",
      "Requirement already satisfied: six>=1.8.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from furl->hopsworks==3.2.0rc0) (1.16.0)\n",
      "Requirement already satisfied: pycryptodomex in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyjks->hopsworks==3.2.0rc0) (3.17)\n",
      "Requirement already satisfied: javaobj-py3 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyjks->hopsworks==3.2.0rc0) (0.4.3)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyjks->hopsworks==3.2.0rc0) (0.2.8)\n",
      "Requirement already satisfied: twofish in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyjks->hopsworks==3.2.0rc0) (0.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyjks->hopsworks==3.2.0rc0) (0.4.8)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from tzlocal->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.1.0.post0)\n",
      "Requirement already satisfied: backports.zoneinfo in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from tzlocal->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.2.1)\n",
      "Requirement already satisfied: future in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyhopshive[thrift]->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.18.3)\n",
      "Requirement already satisfied: thrift>=0.10.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pyhopshive[thrift]->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from sqlalchemy->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.0.2)\n",
      "Requirement already satisfied: entrypoints in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.4)\n",
      "Requirement already satisfied: toolz in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from altair<5,>=4.0.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from cryptography>=3.2->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from importlib-metadata>=1.7.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (3.15.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (8.11.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (5.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (3.0.6)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.0.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jsonpatch>=1.22->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (5.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (22.2.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jsonschema>=2.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.19.3)\n",
      "Requirement already satisfied: fastjsonschema in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from nbformat>=5.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.16.3)\n",
      "Requirement already satisfied: jupyter-core in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from nbformat>=5.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (5.3.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ruamel.yaml<0.17.18,>=0.16->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.2.7)\n",
      "Requirement already satisfied: tzdata in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pytz-deprecation-shim->tzlocal->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2022.7)\n",
      "Requirement already satisfied: pycparser in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.21)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.14.0)\n",
      "Requirement already satisfied: pickleshare in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.6.2)\n",
      "Requirement already satisfied: decorator in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (5.1.1)\n",
      "Requirement already satisfied: appnope in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.1.3)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.1.6)\n",
      "Requirement already satisfied: backcall in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (3.0.38)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.18.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jupyter-core->nbformat>=5.0->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (3.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/romankah/mambaforge/envs/venv38_new/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.5.1->great_expectations==0.14.12->hsfs[python]~=3.2.0rc0->hopsworks==3.2.0rc0) (2.2.1)\n",
      "Building wheels for collected packages: hopsworks, avro, hsml, hsfs\n",
      "  Building wheel for hopsworks (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hopsworks: filename=hopsworks-3.2.0rc0-py3-none-any.whl size=75010 sha256=d4dde5140bf19933fc96afc23b06f4a489d395737b13adcc3ea99aa97b8a9aad\n",
      "  Stored in directory: /Users/romankah/Library/Caches/pip/wheels/18/a0/f8/5e7a4a5bf98685940066f3810d078b213eaf7bc95d8710b769\n",
      "  Building wheel for avro (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro: filename=avro-1.11.0-py2.py3-none-any.whl size=115908 sha256=61438da0910ef672c7847f82f82ce1fa959ffd277fb6b3e19d5971b1779cf130\n",
      "  Stored in directory: /Users/romankah/Library/Caches/pip/wheels/9a/a5/9b/d100e4bd3ef9697b2f955616260c77cb136f8cd2fc89533c63\n",
      "  Building wheel for hsml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hsml: filename=hsml-3.2.0rc0-py3-none-any.whl size=104952 sha256=9ac77e967579be6aa03abd6c810fc3ee27895c7b029267b722402db08d63c31f\n",
      "  Stored in directory: /Users/romankah/Library/Caches/pip/wheels/c7/4b/a0/80667b9ddeca326f53e4d96950a1c44e6c2942de3cefb3bdb9\n",
      "  Building wheel for hsfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hsfs: filename=hsfs-3.2.0rc0-py3-none-any.whl size=214740 sha256=f1f3e91ce7a309fb9fdc3ab3b53a86ddadfdc0700d5228391f377782338c9e04\n",
      "  Stored in directory: /Users/romankah/Library/Caches/pip/wheels/3e/7b/e3/fb540651a6b1d49702ebcb5093da25002ed8dc969b9fb22198\n",
      "Successfully built hopsworks avro hsml hsfs\n",
      "Installing collected packages: avro, hsml, hsfs, hopsworks\n",
      "  Attempting uninstall: avro\n",
      "    Found existing installation: avro 1.10.2\n",
      "    Uninstalling avro-1.10.2:\n",
      "      Successfully uninstalled avro-1.10.2\n",
      "  Attempting uninstall: hsml\n",
      "    Found existing installation: hsml 3.0.3\n",
      "    Uninstalling hsml-3.0.3:\n",
      "      Successfully uninstalled hsml-3.0.3\n",
      "  Attempting uninstall: hsfs\n",
      "    Found existing installation: hsfs 3.0.5\n",
      "    Uninstalling hsfs-3.0.5:\n",
      "      Successfully uninstalled hsfs-3.0.5\n",
      "Successfully installed avro-1.11.0 hopsworks-3.2.0rc0 hsfs-3.2.0rc0 hsml-3.2.0rc0\n"
     ]
    }
   ],
   "source": [
    "!pip install hopsworks==3.2.0rc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c1f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = air_quality_fg.select_all().join(weather_fg.select_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8946c204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 21:34:55,329 INFO: USE `test5_featurestore`\n",
      "2023-04-17 21:34:56,050 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`city_name` `city_name`, `fg1`.`date` `date`, `fg1`.`pm2_5` `pm2_5`, `fg1`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `fg1`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `fg1`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `fg1`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `fg1`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `fg1`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `fg1`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `fg1`.`mean_7_days` `mean_7_days`, `fg1`.`mean_14_days` `mean_14_days`, `fg1`.`mean_28_days` `mean_28_days`, `fg1`.`std_7_days` `std_7_days`, `fg1`.`exp_mean_7_days` `exp_mean_7_days`, `fg1`.`exp_std_7_days` `exp_std_7_days`, `fg1`.`std_14_days` `std_14_days`, `fg1`.`exp_mean_14_days` `exp_mean_14_days`, `fg1`.`exp_std_14_days` `exp_std_14_days`, `fg1`.`std_28_days` `std_28_days`, `fg1`.`exp_mean_28_days` `exp_mean_28_days`, `fg1`.`exp_std_28_days` `exp_std_28_days`, `fg1`.`year` `year`, `fg1`.`day_of_month` `day_of_month`, `fg1`.`month` `month`, `fg1`.`day_of_week` `day_of_week`, `fg1`.`is_weekend` `is_weekend`, `fg1`.`sin_day_of_year` `sin_day_of_year`, `fg1`.`cos_day_of_year` `cos_day_of_year`, `fg1`.`sin_day_of_week` `sin_day_of_week`, `fg1`.`cos_day_of_week` `cos_day_of_week`, `fg1`.`unix_time` `unix_time`, `fg1`.`city_name` `join_pk_city_name`, `fg1`.`unix_time` `join_pk_unix_time`, `fg1`.`unix_time` `join_evt_unix_time`, `fg0`.`date` `date`, `fg0`.`temperature_max` `temperature_max`, `fg0`.`temperature_min` `temperature_min`, `fg0`.`precipitation_sum` `precipitation_sum`, `fg0`.`rain_sum` `rain_sum`, `fg0`.`snowfall_sum` `snowfall_sum`, `fg0`.`precipitation_hours` `precipitation_hours`, `fg0`.`wind_speed_max` `wind_speed_max`, `fg0`.`wind_gusts_max` `wind_gusts_max`, `fg0`.`wind_direction_dominant` `wind_direction_dominant`, RANK() OVER (PARTITION BY `fg0`.`city_name`, `fg0`.`unix_time`, `fg1`.`unix_time` ORDER BY `fg0`.`unix_time` DESC) pit_rank_hopsworks\n",
      "FROM `test5_featurestore`.`air_quality_1` `fg1`\n",
      "INNER JOIN `test5_featurestore`.`weather_1` `fg0` ON `fg1`.`city_name` = `fg0`.`city_name` AND `fg1`.`unix_time` = `fg0`.`unix_time` AND `fg1`.`unix_time` >= `fg0`.`unix_time`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`city_name` `city_name`, `right_fg0`.`date` `date`, `right_fg0`.`pm2_5` `pm2_5`, `right_fg0`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `right_fg0`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `right_fg0`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `right_fg0`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `right_fg0`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `right_fg0`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `right_fg0`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `right_fg0`.`mean_7_days` `mean_7_days`, `right_fg0`.`mean_14_days` `mean_14_days`, `right_fg0`.`mean_28_days` `mean_28_days`, `right_fg0`.`std_7_days` `std_7_days`, `right_fg0`.`exp_mean_7_days` `exp_mean_7_days`, `right_fg0`.`exp_std_7_days` `exp_std_7_days`, `right_fg0`.`std_14_days` `std_14_days`, `right_fg0`.`exp_mean_14_days` `exp_mean_14_days`, `right_fg0`.`exp_std_14_days` `exp_std_14_days`, `right_fg0`.`std_28_days` `std_28_days`, `right_fg0`.`exp_mean_28_days` `exp_mean_28_days`, `right_fg0`.`exp_std_28_days` `exp_std_28_days`, `right_fg0`.`year` `year`, `right_fg0`.`day_of_month` `day_of_month`, `right_fg0`.`month` `month`, `right_fg0`.`day_of_week` `day_of_week`, `right_fg0`.`is_weekend` `is_weekend`, `right_fg0`.`sin_day_of_year` `sin_day_of_year`, `right_fg0`.`cos_day_of_year` `cos_day_of_year`, `right_fg0`.`sin_day_of_week` `sin_day_of_week`, `right_fg0`.`cos_day_of_week` `cos_day_of_week`, `right_fg0`.`unix_time` `unix_time`, `right_fg0`.`date` `date`, `right_fg0`.`temperature_max` `temperature_max`, `right_fg0`.`temperature_min` `temperature_min`, `right_fg0`.`precipitation_sum` `precipitation_sum`, `right_fg0`.`rain_sum` `rain_sum`, `right_fg0`.`snowfall_sum` `snowfall_sum`, `right_fg0`.`precipitation_hours` `precipitation_hours`, `right_fg0`.`wind_speed_max` `wind_speed_max`, `right_fg0`.`wind_gusts_max` `wind_gusts_max`, `right_fg0`.`wind_direction_dominant` `wind_direction_dominant`\n",
      "FROM right_fg0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: WITH right_fg0 AS (SELECT *\nFROM (SELECT `fg1`.`city_name` `city_name`, `fg1`.`date` `date`, `fg1`.`pm2_5` `pm2_5`, `fg1`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `fg1`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `fg1`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `fg1`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `fg1`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `fg1`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `fg1`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `fg1`.`mean_7_days` `mean_7_days`, `fg1`.`mean_14_days` `mean_14_days`, `fg1`.`mean_28_days` `mean_28_days`, `fg1`.`std_7_days` `std_7_days`, `fg1`.`exp_mean_7_days` `exp_mean_7_days`, `fg1`.`exp_std_7_days` `exp_std_7_days`, `fg1`.`std_14_days` `std_14_days`, `fg1`.`exp_mean_14_days` `exp_mean_14_days`, `fg1`.`exp_std_14_days` `exp_std_14_days`, `fg1`.`std_28_days` `std_28_days`, `fg1`.`exp_mean_28_days` `exp_mean_28_days`, `fg1`.`exp_std_28_days` `exp_std_28_days`, `fg1`.`year` `year`, `fg1`.`day_of_month` `day_of_month`, `fg1`.`month` `month`, `fg1`.`day_of_week` `day_of_week`, `fg1`.`is_weekend` `is_weekend`, `fg1`.`sin_day_of_year` `sin_day_of_year`, `fg1`.`cos_day_of_year` `cos_day_of_year`, `fg1`.`sin_day_of_week` `sin_day_of_week`, `fg1`.`cos_day_of_week` `cos_day_of_week`, `fg1`.`unix_time` `unix_time`, `fg1`.`city_name` `join_pk_city_name`, `fg1`.`unix_time` `join_pk_unix_time`, `fg1`.`unix_time` `join_evt_unix_time`, `fg0`.`date` `date`, `fg0`.`temperature_max` `temperature_max`, `fg0`.`temperature_min` `temperature_min`, `fg0`.`precipitation_sum` `precipitation_sum`, `fg0`.`rain_sum` `rain_sum`, `fg0`.`snowfall_sum` `snowfall_sum`, `fg0`.`precipitation_hours` `precipitation_hours`, `fg0`.`wind_speed_max` `wind_speed_max`, `fg0`.`wind_gusts_max` `wind_gusts_max`, `fg0`.`wind_direction_dominant` `wind_direction_dominant`, RANK() OVER (PARTITION BY `fg0`.`city_name`, `fg0`.`unix_time`, `fg1`.`unix_time` ORDER BY `fg0`.`unix_time` DESC) pit_rank_hopsworks\nFROM `test5_featurestore`.`air_quality_1` `fg1`\nINNER JOIN `test5_featurestore`.`weather_1` `fg0` ON `fg1`.`city_name` = `fg0`.`city_name` AND `fg1`.`unix_time` = `fg0`.`unix_time` AND `fg1`.`unix_time` >= `fg0`.`unix_time`) NA\nWHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`city_name` `city_name`, `right_fg0`.`date` `date`, `right_fg0`.`pm2_5` `pm2_5`, `right_fg0`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `right_fg0`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `right_fg0`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `right_fg0`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `right_fg0`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `right_fg0`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `right_fg0`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `right_fg0`.`mean_7_days` `mean_7_days`, `right_fg0`.`mean_14_days` `mean_14_days`, `right_fg0`.`mean_28_days` `mean_28_days`, `right_fg0`.`std_7_days` `std_7_days`, `right_fg0`.`exp_mean_7_days` `exp_mean_7_days`, `right_fg0`.`exp_std_7_days` `exp_std_7_days`, `right_fg0`.`std_14_days` `std_14_days`, `right_fg0`.`exp_mean_14_days` `exp_mean_14_days`, `right_fg0`.`exp_std_14_days` `exp_std_14_days`, `right_fg0`.`std_28_days` `std_28_days`, `right_fg0`.`exp_mean_28_days` `exp_mean_28_days`, `right_fg0`.`exp_std_28_days` `exp_std_28_days`, `right_fg0`.`year` `year`, `right_fg0`.`day_of_month` `day_of_month`, `right_fg0`.`month` `month`, `right_fg0`.`day_of_week` `day_of_week`, `right_fg0`.`is_weekend` `is_weekend`, `right_fg0`.`sin_day_of_year` `sin_day_of_year`, `right_fg0`.`cos_day_of_year` `cos_day_of_year`, `right_fg0`.`sin_day_of_week` `sin_day_of_week`, `right_fg0`.`cos_day_of_week` `cos_day_of_week`, `right_fg0`.`unix_time` `unix_time`, `right_fg0`.`date` `date`, `right_fg0`.`temperature_max` `temperature_max`, `right_fg0`.`temperature_min` `temperature_min`, `right_fg0`.`precipitation_sum` `precipitation_sum`, `right_fg0`.`rain_sum` `rain_sum`, `right_fg0`.`snowfall_sum` `snowfall_sum`, `right_fg0`.`precipitation_hours` `precipitation_hours`, `right_fg0`.`wind_speed_max` `wind_speed_max`, `right_fg0`.`wind_gusts_max` `wind_gusts_max`, `right_fg0`.`wind_direction_dominant` `wind_direction_dominant`\nFROM right_fg0)\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.parse.SemanticException:Ambiguous column reference date in na:44:17', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:rewriteRRForSubQ:SemanticAnalyzer.java:11359', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11338', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11201', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genOPTree:SemanticAnalyzer.java:11987', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:genOPTree:CalcitePlanner.java:597', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12066', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10007, errorMessage='Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na'), operationHandle=None)\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pandas/io/sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2018\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pyhive/hive.py:408\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mExecuteStatement(req)\n\u001b[0;32m--> 408\u001b[0m \u001b[43m_check_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operationHandle \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moperationHandle\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pyhive/hive.py:538\u001b[0m, in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mstatusCode \u001b[38;5;241m!=\u001b[39m ttypes\u001b[38;5;241m.\u001b[39mTStatusCode\u001b[38;5;241m.\u001b[39mSUCCESS_STATUS:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(response)\n",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.parse.SemanticException:Ambiguous column reference date in na:44:17', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:rewriteRRForSubQ:SemanticAnalyzer.java:11359', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11338', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11201', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genOPTree:SemanticAnalyzer.java:11987', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:genOPTree:CalcitePlanner.java:597', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12066', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10007, errorMessage='Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na'), operationHandle=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pandas/io/sql.py:2022\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2022\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pyhive/hive.py:285\u001b[0m, in \u001b[0;36mConnection.rollback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrollback\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHive does not have transactions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Hive does not have transactions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/hsfs/constructor/query.py:108\u001b[0m, in \u001b[0;36mread\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m     online: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m     dataframe_type: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     read_options: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     84\u001b[0m ):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the specified query into a DataFrame.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    It is possible to specify the storage (online/offline) to read from and the\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    type of the output DataFrame (Spark, Pandas, Numpy, Python Lists).\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    !!! warning \"External Feature Group Engine Support\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m        **Spark only**\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m        Reading a Query containing an External Feature Group directly into a\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        Pandas Dataframe using Python/Pandas as Engine is not supported,\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        however, you can use the Query API to create Feature Views/Training\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m        Data containing External Feature Groups.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    # Arguments\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m        online: Read from online storage. Defaults to `False`.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m        dataframe_type: DataFrame type to return. Defaults to `\"default\"`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        read_options: Dictionary of read options for Spark in spark engine.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m            Only for python engine: Use key \"hive_config\" to pass a dictionary of hive or tez configurations.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m            For example: `{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}`\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m            Defaults to `{}`.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    # Returns\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m        `DataFrame`: DataFrame depending on the chosen type.\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     sql_query, online_conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_read(online, read_options)\n\u001b[1;32m    111\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/hsfs/engine/python.py:84\u001b[0m, in \u001b[0;36msql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m     81\u001b[0m     self._job_api = job_api.JobApi()\n\u001b[1;32m     82\u001b[0m     self._kafka_api = kafka_api.KafkaApi()\n\u001b[0;32m---> 84\u001b[0m     # cache the sql engine which contains the connection pool\n\u001b[1;32m     85\u001b[0m     self._mysql_online_fs_engine = None\n\u001b[1;32m     87\u001b[0m def sql(\n\u001b[1;32m     88\u001b[0m     self,\n\u001b[1;32m     89\u001b[0m     sql_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     schema=None,\n\u001b[1;32m     95\u001b[0m ):\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/hsfs/engine/python.py:90\u001b[0m, in \u001b[0;36m_sql_offline\u001b[0;34m(self, sql_query, feature_store, dataframe_type)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# cache the sql engine which contains the connection pool\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mysql_online_fs_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     sql_query,\n\u001b[0;32m---> 90\u001b[0m     feature_store,\n\u001b[1;32m     91\u001b[0m     online_conn,\n\u001b[1;32m     92\u001b[0m     dataframe_type,\n\u001b[1;32m     93\u001b[0m     read_options,\n\u001b[1;32m     94\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m ):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_offline(\n\u001b[1;32m     98\u001b[0m             sql_query,\n\u001b[1;32m     99\u001b[0m             feature_store,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m             hive_config\u001b[38;5;241m=\u001b[39mread_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m read_options \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m         )\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pandas/io/sql.py:564\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    561\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pandas/io/sql.py:2078\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2067\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2068\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     dtype: DtypeArg \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2075\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m   2077\u001b[0m     args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2078\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/venv38_new/lib/python3.8/site-packages/pandas/io/sql.py:2027\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m     ex \u001b[38;5;241m=\u001b[39m DatabaseError(\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124munable to rollback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2026\u001b[0m     )\n\u001b[0;32m-> 2027\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql: WITH right_fg0 AS (SELECT *\nFROM (SELECT `fg1`.`city_name` `city_name`, `fg1`.`date` `date`, `fg1`.`pm2_5` `pm2_5`, `fg1`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `fg1`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `fg1`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `fg1`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `fg1`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `fg1`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `fg1`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `fg1`.`mean_7_days` `mean_7_days`, `fg1`.`mean_14_days` `mean_14_days`, `fg1`.`mean_28_days` `mean_28_days`, `fg1`.`std_7_days` `std_7_days`, `fg1`.`exp_mean_7_days` `exp_mean_7_days`, `fg1`.`exp_std_7_days` `exp_std_7_days`, `fg1`.`std_14_days` `std_14_days`, `fg1`.`exp_mean_14_days` `exp_mean_14_days`, `fg1`.`exp_std_14_days` `exp_std_14_days`, `fg1`.`std_28_days` `std_28_days`, `fg1`.`exp_mean_28_days` `exp_mean_28_days`, `fg1`.`exp_std_28_days` `exp_std_28_days`, `fg1`.`year` `year`, `fg1`.`day_of_month` `day_of_month`, `fg1`.`month` `month`, `fg1`.`day_of_week` `day_of_week`, `fg1`.`is_weekend` `is_weekend`, `fg1`.`sin_day_of_year` `sin_day_of_year`, `fg1`.`cos_day_of_year` `cos_day_of_year`, `fg1`.`sin_day_of_week` `sin_day_of_week`, `fg1`.`cos_day_of_week` `cos_day_of_week`, `fg1`.`unix_time` `unix_time`, `fg1`.`city_name` `join_pk_city_name`, `fg1`.`unix_time` `join_pk_unix_time`, `fg1`.`unix_time` `join_evt_unix_time`, `fg0`.`date` `date`, `fg0`.`temperature_max` `temperature_max`, `fg0`.`temperature_min` `temperature_min`, `fg0`.`precipitation_sum` `precipitation_sum`, `fg0`.`rain_sum` `rain_sum`, `fg0`.`snowfall_sum` `snowfall_sum`, `fg0`.`precipitation_hours` `precipitation_hours`, `fg0`.`wind_speed_max` `wind_speed_max`, `fg0`.`wind_gusts_max` `wind_gusts_max`, `fg0`.`wind_direction_dominant` `wind_direction_dominant`, RANK() OVER (PARTITION BY `fg0`.`city_name`, `fg0`.`unix_time`, `fg1`.`unix_time` ORDER BY `fg0`.`unix_time` DESC) pit_rank_hopsworks\nFROM `test5_featurestore`.`air_quality_1` `fg1`\nINNER JOIN `test5_featurestore`.`weather_1` `fg0` ON `fg1`.`city_name` = `fg0`.`city_name` AND `fg1`.`unix_time` = `fg0`.`unix_time` AND `fg1`.`unix_time` >= `fg0`.`unix_time`) NA\nWHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`city_name` `city_name`, `right_fg0`.`date` `date`, `right_fg0`.`pm2_5` `pm2_5`, `right_fg0`.`pm_2_5_previous_1_day` `pm_2_5_previous_1_day`, `right_fg0`.`pm_2_5_previous_2_day` `pm_2_5_previous_2_day`, `right_fg0`.`pm_2_5_previous_3_day` `pm_2_5_previous_3_day`, `right_fg0`.`pm_2_5_previous_4_day` `pm_2_5_previous_4_day`, `right_fg0`.`pm_2_5_previous_5_day` `pm_2_5_previous_5_day`, `right_fg0`.`pm_2_5_previous_6_day` `pm_2_5_previous_6_day`, `right_fg0`.`pm_2_5_previous_7_day` `pm_2_5_previous_7_day`, `right_fg0`.`mean_7_days` `mean_7_days`, `right_fg0`.`mean_14_days` `mean_14_days`, `right_fg0`.`mean_28_days` `mean_28_days`, `right_fg0`.`std_7_days` `std_7_days`, `right_fg0`.`exp_mean_7_days` `exp_mean_7_days`, `right_fg0`.`exp_std_7_days` `exp_std_7_days`, `right_fg0`.`std_14_days` `std_14_days`, `right_fg0`.`exp_mean_14_days` `exp_mean_14_days`, `right_fg0`.`exp_std_14_days` `exp_std_14_days`, `right_fg0`.`std_28_days` `std_28_days`, `right_fg0`.`exp_mean_28_days` `exp_mean_28_days`, `right_fg0`.`exp_std_28_days` `exp_std_28_days`, `right_fg0`.`year` `year`, `right_fg0`.`day_of_month` `day_of_month`, `right_fg0`.`month` `month`, `right_fg0`.`day_of_week` `day_of_week`, `right_fg0`.`is_weekend` `is_weekend`, `right_fg0`.`sin_day_of_year` `sin_day_of_year`, `right_fg0`.`cos_day_of_year` `cos_day_of_year`, `right_fg0`.`sin_day_of_week` `sin_day_of_week`, `right_fg0`.`cos_day_of_week` `cos_day_of_week`, `right_fg0`.`unix_time` `unix_time`, `right_fg0`.`date` `date`, `right_fg0`.`temperature_max` `temperature_max`, `right_fg0`.`temperature_min` `temperature_min`, `right_fg0`.`precipitation_sum` `precipitation_sum`, `right_fg0`.`rain_sum` `rain_sum`, `right_fg0`.`snowfall_sum` `snowfall_sum`, `right_fg0`.`precipitation_hours` `precipitation_hours`, `right_fg0`.`wind_speed_max` `wind_speed_max`, `right_fg0`.`wind_gusts_max` `wind_gusts_max`, `right_fg0`.`wind_direction_dominant` `wind_direction_dominant`\nFROM right_fg0)\nTExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na:28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:337', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:203', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:266', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:253', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:541', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:516', 'sun.reflect.GeneratedMethodAccessor212:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1821', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy53:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:281', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:712', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624', 'java.lang.Thread:run:Thread.java:748', '*org.apache.hadoop.hive.ql.parse.SemanticException:Ambiguous column reference date in na:44:17', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:rewriteRRForSubQ:SemanticAnalyzer.java:11359', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11338', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11188', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11215', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genPlan:SemanticAnalyzer.java:11201', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genOPTree:SemanticAnalyzer.java:11987', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:genOPTree:CalcitePlanner.java:597', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12066', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:334', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:285', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:643', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1683', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1630', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1625', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:201'], sqlState='42000', errorCode=10007, errorMessage='Error while compiling statement: FAILED: SemanticException [Error 10007]: Ambiguous column reference date in na'), operationHandle=None)\nunable to rollback"
     ]
    }
   ],
   "source": [
    "query.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be427dca",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> 🖍 Feature View Creation and Retrieving </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3192d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = air_quality_fg.select_all().join(weather_fg.select_all(include_event_time=False))\n",
    "\n",
    "query_show = query.show(5)\n",
    "col_names = query_show.columns\n",
    "\n",
    "query_show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff1e11",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">🧑🏻‍🔬 Transformation functions</span>\n",
    "\n",
    "Hopsworks Feature Store provides functionality to attach transformation functions to training datasets.\n",
    "\n",
    "Hopsworks Feature Store also comes with built-in transformation functions such as `min_max_scaler`, `standard_scaler`, `robust_scaler` and `label_encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t_func.name for t_func in fs.get_transformation_functions()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28563812",
   "metadata": {},
   "source": [
    "You can retrieve transformation function you need.\n",
    "\n",
    "To attach transformation function to training dataset provide transformation functions as dict, where key is feature name and value is online transformation function name.\n",
    "\n",
    "Also training dataset must be created from the Query object. Once attached transformation function will be applied on whenever save, insert and get_serving_vector methods are called on training dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['city','date','conditions','aqi']\n",
    "\n",
    "mapping_transformers = {col_name:fs.get_transformation_function(name='standard_scaler') for col_name in col_names if col_name not in category_cols}\n",
    "category_cols = {col_name:fs.get_transformation_function(name='label_encoder') for col_name in category_cols if col_name not in ['date','aqi']}\n",
    "\n",
    "mapping_transformers.update(category_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a1681",
   "metadata": {},
   "source": [
    "`Feature Views` stands between **Feature Groups** and **Training Dataset**. Сombining **Feature Groups** we can create **Feature Views** which store a metadata of our data. Having **Feature Views** we can create **Training Dataset**.\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.create_feature_view()` method.\n",
    "\n",
    "You can specify next parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - functions to transform our features.\n",
    "\n",
    "- `query` - query object with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403df0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name = 'air_quality_fv',\n",
    "    version = 1,\n",
    "    transformation_functions = mapping_transformers,\n",
    "    query = query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c723c54",
   "metadata": {},
   "source": [
    "For now `Feature View` is saved in Hopsworks and you can retrieve it using `FeatureStore.get_feature_view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\n",
    "    name = 'air_quality_fv',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1187a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> 🏋️ Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "To create training dataset you use `FeatureView.create_training_data()` method.\n",
    "\n",
    "Here are some importand things:\n",
    "\n",
    "- It will inherit the name of FeatureView.\n",
    "\n",
    "- The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "- You can choose necessary format using **data_format** parameter.\n",
    "\n",
    "- **start_time** and **end_time** in order to filter dataset in specific time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.create_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51507c0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">⏭️ **Next:** Part 04 </span>\n",
    "\n",
    "In the next notebook you will train a model on the dataset, that was created in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "63265f9757e7c73c149a91832e3b2b12ced37a5390b9151ad08a04f276cd5846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
