{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32d6274",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\">**Hopsworks Feature Store** </span> <span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Pipeline</span>\n",
    "\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into 3 main sections:\n",
    "1. Feature selection.\n",
    "2. Feature transformations.\n",
    "3. Training datasets creation.\n",
    "4. Loading the training data.\n",
    "5. Train the model.\n",
    "6. Register model to Hopsworks model registry.\n",
    "\n",
    "![02_training-dataset](../../images/02_training-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6a645-fe38-4ed5-b5e6-f59f47b2a223",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c6a51",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "electricity_prices_fg = fs.get_feature_group(\n",
    "    name='electricity_prices',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "meteorological_measurements_fg = fs.get_feature_group(\n",
    "    name='meteorological_measurements',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "swedish_holidays_fg = fs.get_feature_group(\n",
    "    name='swedish_holidays',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b4aee",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üñç Feature View Creation and Retrieving </span>\n",
    "\n",
    "Let's start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = electricity_prices_fg.select_all()\\\n",
    "    .join(\n",
    "    meteorological_measurements_fg\\\n",
    "        .select_except([\"timestamp\"])\n",
    "    )\\\n",
    "    .join(\n",
    "        swedish_holidays_fg.select_all()\n",
    "    )\\\n",
    ".filter(meteorological_measurements_fg.precipitaton_type_se1.isin(['missing','Regn']))\\\n",
    ".filter(meteorological_measurements_fg.precipitaton_type_se2.isin(['missing','Regn']))\\\n",
    ".filter(meteorological_measurements_fg.precipitaton_type_se3.isin(['missing','Regn']))\\\n",
    ".filter(meteorological_measurements_fg.precipitaton_type_se4.isin(['missing','Regn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3e924",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions</span>\n",
    "\n",
    "Hopsworks Feature Store provides functionality to attach transformation functions to feature views and comes with built-in transformation functions such as `min_max_scaler`, `standard_scaler`, `robust_scaler` and `label_encoder`.\n",
    "\n",
    "You will preprocess your data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of price areas\n",
    "price_areas = [\"se1\", \"se2\", \"se3\", \"se4\"]\n",
    "\n",
    "# Retrieving transformation functions\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name='label_encoder')\n",
    "\n",
    "# Mapping features to their respective transformation functions\n",
    "transformation_functions = []\n",
    "\n",
    "# Iterate through each price area and map features to their transformation functions\n",
    "for area in price_areas:\n",
    "    transformation_functions.append(min_max_scaler(f\"price_{area}\"))\n",
    "    transformation_functions.append(min_max_scaler(f\"mean_temp_per_day_{area}\"))\n",
    "    transformation_functions.append(min_max_scaler(f\"mean_wind_speed_{area}\"))\n",
    "    transformation_functions.append(min_max_scaler(f\"precipitaton_amount_{area}\"))\n",
    "    transformation_functions.append(min_max_scaler(f\"total_sunshine_time_{area}\"))\n",
    "    transformation_functions.append(min_max_scaler(f\"mean_cloud_perc_{area}\"))\n",
    "    transformation_functions.append(label_encoder(f\"precipitaton_type_{area}\"))\n",
    "\n",
    "# Additional transformation for 'type_of_day'\n",
    "transformation_functions.append(label_encoder(\"type_of_day\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab31f59",
   "metadata": {},
   "source": [
    "`Feature Views` stands between **Feature Groups** and **Training Dataset**. –°ombining **Feature Groups** we can create **Feature Views** which store a metadata of our data. Having **Feature Views** we can create **Training Dataset**.\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.get_or_create_feature_view()` method.\n",
    "\n",
    "We can specify next parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - functions to transform our features.\n",
    "\n",
    "- `query` - query object with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='electricity_feature_view',\n",
    "    version=1,\n",
    "    labels=[], # you will define our 'y' later manualy\n",
    "    transformation_functions=transformation_functions,\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591203c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "To create training dataset you will use the `FeatureView.train_validation_test_split()` method.\n",
    "\n",
    "Here are some importand things:\n",
    "\n",
    "- It will inherit the name of FeatureView.\n",
    "\n",
    "- The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "- You can choose necessary format using **data_format** parameter.\n",
    "\n",
    "- **start_time** and **end_time** in order to filter dataset in specific time range.\n",
    "\n",
    "- You can create **train, test** splits using `create_train_test_split()`. \n",
    "\n",
    "- You can create **train,validation, test** splits using `create_train_validation_test_splits()` methods.\n",
    "\n",
    "- The only thing is that we should specify desired ratio of splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465766",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Dataset with train, test and validation splits</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d99544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since you didn't specify 'labels' in feature view creation, it will return None for Y.\n",
    "X_train, X_val, X_test, _, _, _ = feature_view.train_validation_test_split(\n",
    "    train_start=\"2021-01-01\",\n",
    "    train_end=\"2022-02-28\",\n",
    "    validation_start=\"2022-03-01\",\n",
    "    validation_end=\"2022-05-31\",\n",
    "    test_start=\"2022-06-01\",\n",
    "    test_end=\"2022-09-09\",\n",
    "    description='Electricity price prediction dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeef6d-32c6-4bc9-9bb7-2f071d78feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the training, validation, and test datasets based on the 'timestamp' column\n",
    "X_train.sort_values([\"timestamp\"], inplace=True)\n",
    "X_val.sort_values([\"timestamp\"], inplace=True)\n",
    "X_test.sort_values([\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c807074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'y_train', 'y_val' and 'y_test'\n",
    "y_train = X_train[[\"min_max_scaler_price_se1_\", \"min_max_scaler_price_se2_\", \"min_max_scaler_price_se3_\", \"min_max_scaler_price_se4_\"]]\n",
    "y_val = X_val[[\"min_max_scaler_price_se1_\", \"min_max_scaler_price_se2_\", \"min_max_scaler_price_se3_\", \"min_max_scaler_price_se4_\"]]\n",
    "y_test = X_test[[\"min_max_scaler_price_se1_\", \"min_max_scaler_price_se2_\", \"min_max_scaler_price_se3_\", \"min_max_scaler_price_se4_\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f264dc-4210-401a-9ace-9f9f62f7652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'day' and 'timestamp' columns from the training, validation, and test datasets\n",
    "X_train.drop([\"day\", \"timestamp\"], axis=1, inplace=True)\n",
    "X_val.drop([\"day\", \"timestamp\"], axis=1, inplace=True)\n",
    "X_test.drop([\"day\", \"timestamp\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d158720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 3 rows of the test dataset (X_test)\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869da71",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üóÉ Window timeseries dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 df_train, val_df, test_df,\n",
    "                 label_columns=None, batch_size=32):\n",
    "        # Store the raw data.\n",
    "        self.df_train = df_train\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(df_train.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'\n",
    "        ])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1\n",
    "            )\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, plot_col, model=None, max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n + 1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                            marker='X', edgecolors='k', label='Predictions',\n",
    "                            c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "\n",
    "    # make_dataset method will take a time series DataFrame and convert it to a tf.data.Dataset of (input_window, label_window)\n",
    "    # pairs using the tf.keras.utils.timeseries_dataset_from_array function:\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        ds = ds.repeat(1000)\n",
    "        ds = ds.prefetch(10)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.df_train)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.test))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a WindowGenerator instance\n",
    "# The window represents a time series data window with an input width of 4, label width of 4, and a shift of 1\n",
    "# The label columns are [\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"]\n",
    "n_step_window = WindowGenerator(\n",
    "    df_train=X_train, \n",
    "    val_df=X_val, \n",
    "    test_df=X_test, \n",
    "    input_width=4, \n",
    "    label_width=4, \n",
    "    shift=1, \n",
    "    label_columns=[\"min_max_scaler_price_se1_\", \"min_max_scaler_price_se2_\", \"min_max_scaler_price_se3_\", \"min_max_scaler_price_se4_\"],\n",
    ")\n",
    "\n",
    "# Displaying the WindowGenerator instance\n",
    "n_step_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting an example batch of inputs and labels from the WindowGenerator instance\n",
    "inputs, labels = n_step_window.example\n",
    "\n",
    "# Displaying the shapes of the input and label tensors\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Displaying the indices of label columns in the dataset\n",
    "print(n_step_window.label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the training dataset (n_step_window.train) to extract an example batch of inputs and labels\n",
    "for example_inputs, example_labels in n_step_window.train.take(1):\n",
    "    # Displaying the shape of the example batch of inputs\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    \n",
    "    # Displaying the shape of the example batch of labels\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3210c",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "    # Creating a Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding a 1D convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=1, padding='same', kernel_initializer=\"uniform\", input_shape=(input_dim[0], input_dim[1])))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Adding 1D convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=1, padding='same', kernel_initializer=\"uniform\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Adding 1D convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=16, kernel_size=1, padding='same', kernel_initializer=\"uniform\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Adding a 1D max pooling layer\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=1, padding='same'))\n",
    "\n",
    "    # Adding a Bidirectional LSTM layer\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16, return_sequences=True))) \n",
    "    model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "    # Adding a Dense layer\n",
    "    model.add(tf.keras.layers.Dense(units=4))\n",
    "\n",
    "    # Displaying the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Compiling the model with mean absolute error loss and the Adam optimizer\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a model using the specified input shape derived from the example batch of inputs\n",
    "model = build_model(inputs.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording the start time\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "# Training the model on the training dataset with specified parameters\n",
    "# - Using 50 epochs\n",
    "# - Displaying minimal training information (verbose=0)\n",
    "# - Specifying the number of steps per epoch (steps_per_epoch=200)\n",
    "# - Using the training dataset for validation (validation_data=n_step_window.train)\n",
    "# - Specifying the number of validation steps (validation_steps=1)\n",
    "history = model.fit(\n",
    "    n_step_window.train,\n",
    "    epochs=50,\n",
    "    verbose=0,\n",
    "    steps_per_epoch=200,\n",
    "    validation_data=n_step_window.train,\n",
    "    validation_steps=1,                    \n",
    ")\n",
    "\n",
    "# Recording the end time\n",
    "end = timer()\n",
    "\n",
    "# Displaying the time taken for training\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dade6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting an example batch of inputs and labels from the WindowGenerator instance (n_step_window)\n",
    "inputs, labels = n_step_window.example\n",
    "\n",
    "# Making predictions on the example batch using the trained model\n",
    "prediction_test = model.predict(inputs)\n",
    "\n",
    "# Displaying the shapes of the predicted outputs and the true labels\n",
    "print(prediction_test.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training history dictionary from the model training\n",
    "history_dict = history.history\n",
    "\n",
    "# Displaying the keys in the history dictionary\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c50149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting training and validation loss values from the history dictionary\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "# Creating separate variables for loss values (50 epochs)\n",
    "loss_values50 = loss_values\n",
    "val_loss_values50 = val_loss_values\n",
    "\n",
    "# Generating a plot for training and validation loss over epochs\n",
    "epochs = range(1, len(loss_values50) + 1)\n",
    "plt.plot(epochs, loss_values50, 'b', color='blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values50, 'b', color='red', label='Validation loss')\n",
    "\n",
    "# Setting plot details and labels\n",
    "plt.rc('font', size=18)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(epochs)\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the time series data for the 'price_se4' column\n",
    "n_step_window.plot(\n",
    "    plot_col=\"min_max_scaler_price_se4_\", \n",
    "    max_subplots=3, \n",
    "    model=model.predict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting actual values for each time step in the example batch\n",
    "se1_actual = []\n",
    "se2_actual = []\n",
    "se3_actual = []\n",
    "se4_actual = []\n",
    "\n",
    "# Extracting inputs and labels from the example batch of the WindowGenerator instance (n_step_window)\n",
    "inputs, labels = n_step_window.example\n",
    "\n",
    "# Iterating over batches and windows to collect actual values for each time step\n",
    "for batch_n in range(len(labels)):\n",
    "    batch = labels[batch_n]\n",
    "    for window_n in range(4):\n",
    "        se1_actual.append(batch[window_n][0].numpy())\n",
    "        se2_actual.append(batch[window_n][1].numpy())\n",
    "        se3_actual.append(batch[window_n][2].numpy())\n",
    "        se4_actual.append(batch[window_n][3].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53833583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting predicted values for each time step in the example batch using the trained model\n",
    "se1_pred = []\n",
    "se2_pred = []\n",
    "se3_pred = []\n",
    "se4_pred = []\n",
    "\n",
    "# Making predictions on the example batch using the trained model\n",
    "prediction_test = model.predict(inputs)\n",
    "\n",
    "# Iterating over batches and windows to collect predicted values for each time step\n",
    "for batch_n in range(len(prediction_test)):\n",
    "    batch = prediction_test[batch_n]\n",
    "    for window_n in range(4):\n",
    "        se1_pred.append(batch[window_n][0])\n",
    "        se2_pred.append(batch[window_n][1])\n",
    "        se3_pred.append(batch[window_n][2])\n",
    "        se4_pred.append(batch[window_n][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 10 rows of the true labels (y_test)\n",
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the value at index 1 in the list 'se3_actual'\n",
    "se3_actual[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed41b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predicted and actual values for \"SE1\" prices over time\n",
    "plt.plot(se1_pred, color='red', label='Test SE1 price prediction')\n",
    "plt.plot(se1_actual, color='blue', label='Test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485afbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predicted and actual values for \"SE2\" prices over time\n",
    "plt.plot(se2_pred, color='red', label='Test SE2 price prediction')\n",
    "plt.plot(se2_actual, color='blue', label='Test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predicted and actual values for \"SE3\" prices over time\n",
    "plt.plot(se3_pred, color='red', label='Test SE3 price prediction')\n",
    "plt.plot(se3_actual, color='blue', label='Test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predicted and actual values for \"SE4\" prices over time\n",
    "plt.plot(se4_pred, color='red', label='Test SE4 price prediction')\n",
    "plt.plot(se4_actual, color='blue', label='Test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Adjusting the size of the plot\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62aba50",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style='color:#ff5f27'>üóÑ Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c146ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the trained model to a directory\n",
    "model_dir = \"electricity_price_model\"\n",
    "print('Exporting trained model to: {}'.format(model_dir))\n",
    "\n",
    "# Saving the model using TensorFlow's saved_model.save function\n",
    "tf.saved_model.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Extracting loss value from the training history\n",
    "metrics = {'loss': history_dict['val_loss'][0]} \n",
    "\n",
    "# Creating a TensorFlow model in the Model Registry\n",
    "tf_model = mr.tensorflow.create_model(\n",
    "    name=\"electricity_price_prediction_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Daily electricity price prediction model.\",\n",
    "    input_example=n_step_window.example[0].numpy(),\n",
    ")\n",
    "\n",
    "# Saving the model to the specified directory\n",
    "tf_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a1ad7",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 04: Batch Inference </span>\n",
    "\n",
    "In the next notebook you will use your registered model to predict batch data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
