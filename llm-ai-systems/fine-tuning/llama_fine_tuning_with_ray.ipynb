{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea92ef66",
   "metadata": {},
   "source": [
    "## A Guide for Fine-tuning Llama 3.1 (8B parameter) using Ray Framework on Hopsworks\n",
    "This tutorial demonstrates how to perform fine-tuning (with LoRA and deepspeed) of a Llama 3.1 (8B) using the Ray framework on Hopsworks. Ray is an industry-leading distributed computing framework. This tutorial was run on OVH cluster but you can use any cloud provider of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8ec38",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "To perform the steps in this tutorial, you need to create a Hopsworks Kubernetes cluster with Ray enabled. For the fine-tuning task demonstrated in this example, these are the minimum resources required:\n",
    "* 1 x Node (16 CPU 64 GB RAM) for the Ray head\n",
    "* 4 x Nodes (15 CPU 45 GB RAM 300 GB disk 1 Tesla V100S) for the workers\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef231fe0",
   "metadata": {},
   "source": [
    "## 1️⃣ Dataset preparation\n",
    "We are going to fine-tune the model for question answering. We need to prepare the dataset that will be used for supervised fine-tuning in a certain format. There is no specific prompt format required for the pre-trained Llama 3.1 so the dataset preprocessing can follow any prompt-completion style. The instruction-tuned models (Meta-Llama-3.1-{8,70,405}B-Instruct) use a multi-turn conversation prompt format that structures the conversation between the users and the models.\n",
    "\n",
    "The dataset for QA typically includes the following fields:\n",
    "\n",
    "* Question: The input question to the model.\n",
    "* Context (optional): A passage or text providing information the model should use to answer.\n",
    "* Answer: The correct response.\n",
    "\n",
    "This example is configured to fine-tune the Llama 3.1 8B pre-trained model on the GSM8K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7678c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f503cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-21 00:27:34,288 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://hopsworks.ai.local/p/1146\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "ds = project.get_dataset_api()\n",
    "mr = project.get_model_registry()\n",
    "jb = project.get_jobs_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e701e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resources directory in HopsFS\n",
    "\n",
    "llama_ft_resources_dir = \"Resources/llama_finetuning\"\n",
    "HOPSFS_STORAGE_PATH = os.path.join(os.environ.get(\"PROJECT_PATH\"), llama_ft_resources_dir)\n",
    "if not os.path.exists(HOPSFS_STORAGE_PATH):\n",
    "    os.mkdir(HOPSFS_STORAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8f5b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e04ab05cd6441968094890ea704848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/977 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5fe2cd19144bb69c4bb5482ee4eb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/763 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527c2f029483499bbc7f590199cfc209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/980 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c276cc74d4424151884f116d15277bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/866 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e120ec00c2c341c19a3cb4fe2f9e1e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/263 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy fine-tunning configuration files to llama resources directory\n",
    "for root, dirs, files in os.walk(\"configs\"):\n",
    "    for filename in files:\n",
    "        ds.upload(os.path.join(root, filename), os.path.join(llama_ft_resources_dir, root), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae625c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data files\n",
    "\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "dataset_splits = {\"train\": dataset[\"train\"], \"test\": dataset[\"test\"]}\n",
    "dataset_dir = os.path.join(HOPSFS_STORAGE_PATH, \"datasets\")\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5988ac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='/hopsfs/Resources/llama_finetuning/datasets/tokens.json' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens to the dataset to optimize the fine-tuning of the model\n",
    "\n",
    "with open(os.path.join(dataset_dir, \"tokens.json\"), \"w\") as f:\n",
    "    tokens = {}\n",
    "    print(f)\n",
    "    tokens[\"tokens\"] = [\"<START_Q>\", \"<END_Q>\", \"<START_A>\", \"<END_A>\"]\n",
    "    f.write(json.dumps(tokens))\n",
    "    for key, split in dataset_splits.items():\n",
    "        with open(os.path.join(dataset_dir, f\"{key}.jsonl\"), \"w\") as f:\n",
    "            max_num_qas = 100 # 2 # Number of QAs\n",
    "            for item in split:\n",
    "                newitem = {}\n",
    "                newitem[\"input\"] = (\n",
    "                    f\"<START_Q>{item['question']}<END_Q>\"\n",
    "                    f\"<START_A>{item['answer']}<END_A>\"\n",
    "                )\n",
    "                f.write(json.dumps(newitem) + \"\\n\")  # write file into dataset resources dir\n",
    "                if max_num_qas is not None:\n",
    "                    max_num_qas -= 1\n",
    "                    if max_num_qas <= 0:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcc556",
   "metadata": {},
   "source": [
    "## 2️⃣ Download and Register the Base Llama3.1 Model\n",
    "The next step is to download the pre-trained Llama model from hugging face. For this you will need the hugging face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abf2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c67ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"<INSERT_YOUR_HF_TOKEN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ef4522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a6463addd74816a1e2d3664d4d3346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download the pre-trained model from Hugging face\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llama31_local_dir = snapshot_download(model_id, ignore_patterns=\"original/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b1c199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd711dcced28491b9b3b4d1bb9e9d13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://hopsworks.ai.local/p/1146/models/llama318binstruct/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'llama318binstruct', version: 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Llama3.1 model to the Hopsworks Model Registry\n",
    "base_model_name = \"llama318binstruct\"\n",
    "llama31 = mr.llm.create_model(base_model_name, description=\"Llama3.1-8B-Instruct model from HuggingFace\")\n",
    "llama31.save(llama31_local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863d1ac",
   "metadata": {},
   "source": [
    "## 3️⃣ Create the Ray job for the fine-tuning task\n",
    "We are going to use the hopsworks jobs api to create and run the job for the fine-tuning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5ff814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b104f3a69c48a7be949d1a73f04a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/29148 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0397598f5c1f46d985b513746332bfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/548 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_adapter_name = f\"lora{base_model_name}\"\n",
    "\n",
    "app_file_path = ds.upload(\"ray_llm_finetuning.py\", llama_ft_resources_dir, overwrite=True)\n",
    "environment_config_yaml_path = ds.upload(\"llama_fine_tune_runtime_env.yaml\", llama_ft_resources_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63d6ac",
   "metadata": {},
   "source": [
    "#### About the runtime environment file\n",
    "The runtime environment file contains the dependencies required for the Ray job including files, packages, environment variables, and more. This is useful when you need to install specific packages and set environment variables for this particular Ray job. It should be provided as a YAML file. In this example, the runtime environment file has the following configuration.\n",
    "```\n",
    "pip:\n",
    "  - transformers==4.44.0\n",
    "  - accelerate==0.31.0\n",
    "  - peft==0.11.1\n",
    "  - deepspeed==0.16.2\n",
    "env_vars:\n",
    "  LIBRARY_PATH: \"$CUDA_HOME/lib64:$LIBRARY_PATH\"\n",
    "  PROJECT_DIR: \"/home/yarnapp/hopsfs\"\n",
    "  TRAINED_MODEL_STORAGE_PATH: \"${PROJECT_DIR}/Resources/llama_finetuning/fine-tuned-model\" # Where the fine-tuned model will be saved\n",
    "  TRAINING_DATA_DIR: \"${PROJECT_DIR}/Resources/llama_finetuning/datasets\" # dataset location\n",
    "  TRAINING_CONFIGURATION_DIR: \"${PROJECT_DIR}/Resources/llama_finetuning/configs\" # location for deepspeed and lora configuration files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88989b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_args = f\"--base-model-name {base_model_name} --lora-model-name {lora_adapter_name}\"\n",
    "\n",
    "# Torch Trainer scaling config\n",
    "torch_trainer_num_workers = 4\n",
    "torch_trainer_worker_cpus = 11\n",
    "torch_trainer_worker_gpus = 1\n",
    "torch_trainer_scaling_args = f\"-ttnm {torch_trainer_num_workers} -ttwc {torch_trainer_worker_cpus} -ttwg {torch_trainer_worker_gpus}\"\n",
    "\n",
    "# Training config\n",
    "num_epochs = 2\n",
    "learning_rate = \"5e-4\"\n",
    "batch_size_per_device=4\n",
    "eval_batch_size_per_device=4\n",
    "training_config_args = f\"--lora --mx fp16 --num-epochs={num_epochs} --lr={learning_rate} --batch-size-per-device={batch_size_per_device} --eval-batch-size-per-device={eval_batch_size_per_device}\"\n",
    "\n",
    "# Ray cluster config\n",
    "ray_config = jb.get_configuration(\"RAY\")\n",
    "ray_config['appPath'] = os.path.join('/Projects/' + project.name, app_file_path)\n",
    "ray_config['environmentName'] = \"ray-torch-training-pipeline\"\n",
    "ray_config['driverCores'] = 1\n",
    "ray_config['driverMemory'] = 4096\n",
    "ray_config['workerCores'] = 12\n",
    "ray_config['workerMemory'] = 30816\n",
    "ray_config['workerMinInstances'] = 4\n",
    "ray_config['workerMaxInstances'] = 4\n",
    "ray_config['workerGpus'] = 1\n",
    "ray_config['runtimeEnvironment'] = os.path.join('/Projects/' + project.name, environment_config_yaml_path)\n",
    "\n",
    "ray_config['defaultArgs'] = f\"{model_args} {torch_trainer_scaling_args} {training_config_args}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660c7c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job created successfully, explore it at https://hopsworks.ai.local/p/1146/jobs/named/fine-tune-llama31\n"
     ]
    }
   ],
   "source": [
    "job_name = \"fine-tune-llama31\"\n",
    "job = jb.create_job(job_name, ray_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3c20f",
   "metadata": {},
   "source": [
    "## 4️⃣ Run the fine-tuning Ray job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b042c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_job = jb.get_job(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a59b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: fine-tune-llama31\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks.ai.local/p/1146/jobs/named/fine-tune-llama31/executions\n",
      "2025-02-21 00:33:33,865 INFO: Waiting for execution to finish. Current state: INITIALIZING\n",
      "2025-02-21 00:34:01,210 INFO: Waiting for execution to finish. Current state: PENDING\n",
      "2025-02-21 00:34:25,490 INFO: Waiting for execution to finish. Current state: RUNNING\n",
      "2025-02-21 00:56:06,842 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS\n",
      "2025-02-21 00:56:06,870 INFO: Waiting for log aggregation to finish.\n",
      "2025-02-21 00:59:16,513 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Execution('SUCCEEDED', 'FINISHED', '2025-02-21T08:33:30.000Z', '--base-model-name llama318binstruct --lora-model-name lorallama318binstruct -ttnm 4 -ttwc 11 -ttwg 1 --lora --mx fp16 --num-epochs=2 --lr=5e-4 --batch-size-per-device=4 --eval-batch-size-per-device=4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff557a8",
   "metadata": {},
   "source": [
    "After the job is run you can go to the hopsworks UI to monitor the job execution. From executions page, you can open the Ray dashboard. In the Ray Dashboard, you can monitor the resources used by the job, the number of workers, logs, and the tasks that are running. \n",
    "\n",
    "After the job finishes running successfully, the fine-tuned model will be saved in the directory specified in the TRAINED_MODEL_STORAGE_PATH variable defined in the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bab2fb",
   "metadata": {},
   "source": [
    "## 5️⃣ Export fine-tuned Llama3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b773c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be8aeba2e784beeaca18e882dd8d82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://hopsworks.ai.local/p/1146/models/ftllama318binstruct/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'ftllama318binstruct', version: 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replicate the base model first\n",
    "\n",
    "fine_tuned_model_name = f\"ft{llama31.name}\"\n",
    "ft_llama31 = mr.llm.create_model(fine_tuned_model_name, description=\"(LoRA fine-tuned) \" + llama31.description)\n",
    "ft_llama31.save(llama31.model_files_path, keep_original_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8273899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-21 01:07:11,140 WARNING: VersionWarning: No version provided for getting model `lorallama318binstruct`, defaulting to `1`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy fine-tuned lora adapter into model files directory\n",
    "\n",
    "ftllama31_lora_adapter_path = f\"{ft_llama31.model_files_path}/lora_adapter\"\n",
    "if not ds.exists(ftllama31_lora_adapter_path):\n",
    "    ds.mkdir(ftllama31_lora_adapter_path)\n",
    "\n",
    "lora_adapter = mr.get_model(lora_adapter_name)    \n",
    "count, files = ds.list_files(lora_adapter.model_files_path, 0, 100)\n",
    "for f in files:\n",
    "    ds.copy(f.path, f\"{ftllama31_lora_adapter_path}/{os.path.basename(f.path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d8f07",
   "metadata": {},
   "source": [
    "## 6️⃣ Deploy the fine-tuned Llama3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cadfe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7850a2689b4b2d81d9673bc4b4d29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/134 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_config_file = f\"/Projects/{project.name}/\" + ds.upload(\"llama_vllmconfig.yaml\", \"Resources\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd0ab4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://hopsworks.ai.local/p/1146/deployments/1035\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "ft_llama31_depl = ft_llama31.deploy(\n",
    "    name=\"ftllama31\",\n",
    "    description=\"(LoRA fine-tuned) Llama3.1 8B-Instruct from HuggingFace\",\n",
    "    config_file=path_to_config_file,\n",
    "    resources={\"num_instances\": 1, \"requests\": {\"cores\": 2, \"memory\": 1024*12, \"gpus\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1000352d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fab8595b7f4991a8bb1fe2dd87154b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "ft_llama31_depl.start(await_running=60*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f45ae",
   "metadata": {},
   "source": [
    "## 7️⃣ Prompting the fine-tuned Llama3.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a3d1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "# Get the istio endpoint from the Llama deployment page in the Hopsworks UI.\n",
    "istio_endpoint = \"<ISTIO_ENDPOINT>\" # with format \"http://<ip-address>\"\n",
    "\n",
    "chat_completions_url = istio_endpoint + \"/v1/chat/completions\"\n",
    "\n",
    "# Resolve API key for request authentication\n",
    "if \"SERVING_API_KEY\" in os.environ:\n",
    "    # if running inside Hopsworks\n",
    "    api_key_value = os.environ[\"SERVING_API_KEY\"]\n",
    "else:\n",
    "    # Create an API KEY using the Hopsworks UI and place the value below\n",
    "    api_key_value = \"<API_KEY>\"\n",
    "    \n",
    "# Prepare request headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'ApiKey ' + api_key_value,\n",
    "    'Host': f\"{ft_llama31_depl.name}.{project.name.lower().replace('_', '-')}.hopsworks.ai\", # also provided in the Hopsworks UI\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6321009",
   "metadata": {},
   "source": [
    "#### 🟨 Generate answer with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34fcf9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion request:  {'model': 'ftllama31', 'messages': [{'role': 'user', 'content': \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used. Let's think step by step. At the end, you MUST write the answer as an integer after '####'.\"}]}\n",
      "2025-02-21 01:17:20,225 INFO: HTTP Request: POST http://54.37.77.225/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "<Response [200 OK]>\n",
      "To find the number of teaspoonfuls of sugar Katy used, we first need to determine the total parts in the ratio. \n",
      "\n",
      "The ratio of sugar to water is 7:13. \n",
      "To find the total parts, we add the two parts together: \n",
      "7 + 13 = 20 parts.\n",
      "\n",
      "Since the total amount used was 120 (teaspoons of sugar and cups of water), we can find the value of one part by dividing 120 by 20: \n",
      "120 / 20 = 6.\n",
      "\n",
      "Now that we know one part is equal to 6, we can find the number of teaspoonfuls of sugar Katy used. \n",
      "In the ratio, the sugar part is 7. \n",
      "So, the number of teaspoonfuls of sugar is:\n",
      "7 * 6 = 42\n",
      "\n",
      "####42\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Chat Completion for a user message\n",
    "#\n",
    "\n",
    "user_message = \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. \" \\\n",
    "               \"If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used.\"\n",
    "\n",
    "# user_message = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \" +\n",
    "#                \"How many clips did Natalia sell altogether in April and May?\"\n",
    "\n",
    "# Improvement proposed by: https://arxiv.org/abs/2205.11916\n",
    "final_instruction = \" Let's think step by step. At the end, you MUST write the answer as an integer after '####'.\"\n",
    "    \n",
    "completion_request = {\n",
    "    \"model\": ft_llama31_depl.name,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message + final_instruction\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Completion request: \", completion_request, end=\"\\n\")\n",
    "\n",
    "response = httpx.post(chat_completions_url, headers=headers, json=completion_request, timeout=45.0)\n",
    "print(response)\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8f4db",
   "metadata": {},
   "source": [
    "#### 🟨 Generate answer via LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b080c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion request:  {'model': 'lora_adapter', 'messages': [{'role': 'user', 'content': \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used. Let's think step by step. At the end, you MUST write the answer as an integer after '####'.\"}]}\n",
      "2025-02-21 01:20:00,681 INFO: HTTP Request: POST http://54.37.77.225/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "<Response [200 OK]>\n",
      "To find the number of teaspoonfuls of sugar Katy used, we need to follow these steps:\n",
      "\n",
      "1. The ratio of sugar to water is 7:13. This means that for every 7 teaspoons of sugar, there are 13 cups of water. To simplify this ratio, we can find the least common multiple (LCM) of 7 and 13. The LCM of 7 and 13 is 91 (7 * 13 = 91).\n",
      "\n",
      "2. Now we know that for every 91 units of the ratio, there are 7 teaspoons of sugar. We also know that the total number of units (sugar and water) used is 120. \n",
      "\n",
      "3. To find the number of times the 91 units fit into 120, we can divide 120 by 91. \n",
      "\n",
      "   120 / 91 ≈ 1.319\n",
      "\n",
      "4. This means that the ratio fits into 120 approximately 1.319 times. However, since we cannot use a fraction of a ratio, we will round this number to the nearest whole number, which is 1.\n",
      "\n",
      "5. Now, we need to find the number of teaspoonfuls of sugar Katy used. We know that for every 91 units of the ratio, there are 7 teaspoons of sugar. Since the ratio fits into 120 approximately 1 time, we can multiply 7 by 1 to find the number of teaspoonfuls of sugar Katy used.\n",
      "\n",
      "   7 * 1 = 7\n",
      "\n",
      "So, Katy used 7#### teaspoonfuls of sugar.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Chat Completion for a user message (fine-tuned)\n",
    "#\n",
    "\n",
    "user_message = \"Katy makes coffee using teaspoons of sugar and cups of water in the ratio of 7:13. \" \\\n",
    "               \"If she used a total of 120 teaspoons of sugar and cups of water, calculate the number of teaspoonfuls of sugar she used.\"\n",
    "\n",
    "# user_message = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \" +\n",
    "#                \"How many clips did Natalia sell altogether in April and May?\"\n",
    "\n",
    "final_instruction = \" Let's think step by step. At the end, you MUST write the answer as an integer after '####'.\"\n",
    "        \n",
    "completion_request = {\n",
    "    \"model\": \"lora_adapter\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message + final_instruction\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Completion request: \", completion_request, end=\"\\n\")\n",
    "\n",
    "response = httpx.post(chat_completions_url, headers=headers, json=completion_request, timeout=45.0)\n",
    "print(response)\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e0182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
