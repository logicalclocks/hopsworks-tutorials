{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Load, Engineer & Connect</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This is the first part of the Neo4j and Hopsworks Feature Store integration. As part of this first module, we will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with **Neo4j** and the **Hopworks Feature Store**  for batch data with a goal of training and deploying a model that can predict fraudulent transactions.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 4 sections:** \n",
    "1. Import data into Neo4j\n",
    "2. Use Neo4j's GDS library and `node2vec` to calculate graph node embeddings\n",
    "3. More Feature Engineering of transactions\n",
    "5. Create Feature Groups in Hopsworks Feature Store from Neo4j embeddings and processed features\n",
    "\n",
    "![tutorial-flow](../../images/01_featuregroups.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Import librararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "from graphdatascience import GraphDataScience\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from features.transactions import get_in_out_transactions\n",
    "from features.party import get_transaction_labels, get_party_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n",
    "\n",
    "The data we will use comes from three different CSV files:\n",
    "\n",
    "- `transactions.csv`: transaction information such as timestamp, location, and the amount. \n",
    "- `alert_transactions.csv`: Suspicious Activity Report (SAR) transactions.\n",
    "- `party.csv`: User profile information.\n",
    "\n",
    "In a production system, these CSV files would originate from separate data sources or tables, and probably separate data pipelines. **All three files have a customer id column `id` in common, which we can use for joins.**\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data into Neo4j\n",
    "\n",
    "#### Neo4j setup\n",
    "Before executing the next cells, the Neo4j database must be installed and initialized:\n",
    "- Install Neo4j Desktop from https://neo4j.com/download/\n",
    "- Create a new database project and server\n",
    "- Install the APOC and GDS plugins\n",
    "- BOLT protocol should be set (if already is not) in the the [neo4j.conf](https://neo4j.com/docs/operations-manual/current/configuration/neo4j-conf/) file\n",
    "\n",
    "First, let's set a few parameters to connect with the Neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = \"bolt://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"hopsworks\")\n",
    "DATABASE = \"neo4j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a few indexes in Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.execute_query(\"CREATE CONSTRAINT party_id_constraint FOR (p:Party) REQUIRE p.partyId IS UNIQUE\", database_=DATABASE)\n",
    "    driver.execute_query(\"CREATE TEXT INDEX party_type_index FOR (p:Party) ON (p.partyType)\", database_=DATABASE)\n",
    "    driver.execute_query(\"CREATE CONSTRAINT transaction_id_constraint FOR ()-[r:TRANSACTION]-() REQUIRE r.tran_id is UNIQUE\", database_=DATABASE)\n",
    "    driver.execute_query(\"CREATE TEXT INDEX transaction_timestamp_index FOR ()-[r:TRANSACTION]-() ON r.tran_timestamp\", database_=DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the first import of the first .csv file, holding the (:Party) nodes. This will finish very quickly, as there are only 7-8k nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session(database=DATABASE) as session:\n",
    "            result = session.run(\"\"\"\n",
    "                load csv with headers from \"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\" as parties\n",
    "                create (p:Party)\n",
    "                set p = parties\n",
    "            \"\"\")\n",
    "print(result.consume().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the relationshops. There are approx 430k [:TRANSACTION] relationships, and importing these will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session(database=DATABASE) as session:\n",
    "            result = session.run(\"\"\"\n",
    "                LOAD CSV WITH HEADERS FROM \"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\" AS Transaction\n",
    "                    MATCH (startNode:Party)\n",
    "                    WHERE startNode.partyId = Transaction.src\n",
    "                    CALL {\n",
    "                        WITH Transaction, startNode\n",
    "                        MATCH (endNode:Party)\n",
    "                        WHERE endNode.partyId = Transaction.dst\n",
    "                        CREATE (startNode)-[rel:TRANSACTION {tran_id: Transaction.tran_id, tx_type: Transaction.tx_type, base_amt: Transaction.base_amt, tran_timestamp: datetime(Transaction.tran_timestamp)}]->(endNode)\n",
    "                    } IN TRANSACTIONS OF 2500 ROWS;\n",
    "            \"\"\")\n",
    "print(result.consume().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the importing of the data into Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading remaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/transactions.csv\", parse_dates = ['tran_timestamp'])\n",
    "transactions_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Alert Transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/alert_transactions.csv\")\n",
    "alert_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚õ≥Ô∏è Party dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/aml/party.csv\")\n",
    "party.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n",
    "## Calculating the node embeddings in Neo4j\n",
    "For each month of transactions, compute and store embeddings using the `node2vec` library. This uses the GDS library of Neo4j, which [needs to be installed](https://neo4j.com/docs/graph-data-science/current/installation/) on the Neo4j server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    try:\n",
    "        gds = GraphDataScience(URI, auth=AUTH, database=DATABASE)\n",
    "\n",
    "        # Determine months of transactions\n",
    "        start_date = transactions_df['tran_timestamp'].min().to_pydatetime().replace(tzinfo=None)\n",
    "        end_date = transactions_df['tran_timestamp'].max().to_pydatetime().replace(tzinfo=None)\n",
    "\n",
    "        # For each month of transactions\n",
    "        while start_date <= end_date:\n",
    "            last_day_of_month = datetime.datetime(start_date.year, start_date.month, 1) + datetime.timedelta(days=32)\n",
    "            end_date_of_month = last_day_of_month - datetime.timedelta(days=last_day_of_month.day)\n",
    "\n",
    "            # Convert dates as milliseconds\n",
    "            start = float(start_date.timestamp() / 10 ** 9)\n",
    "            end = float(end_date_of_month.timestamp() / 10 ** 9)\n",
    "\n",
    "            # Retrieve transactions within the month\n",
    "            gds.run_cypher(\n",
    "            f\"MATCH (p1:Party)-[t:TRANSACTION]->(p2:Party) WHERE t.tran_timestamp >= {start} AND t.tran_timestamp < {end} RETURN p1, t, p2\")\n",
    "\n",
    "            # Create a temporary graph\n",
    "            G, project_result = gds.graph.project(\"transaction_graph\", \"Party\", \"TRANSACTION\")\n",
    "\n",
    "            # Use the temporary graph to compute embeddings and save them as a property in the Neo4j database\n",
    "            node2vec_result = gds.node2vec.write(\n",
    "                G,                                #  Graph object\n",
    "                embeddingDimension=16,\n",
    "                walkLength=80,\n",
    "                inOutFactor=1,\n",
    "                returnFactor=1,\n",
    "                writeProperty=\"node2vec\"\n",
    "            )\n",
    "\n",
    "            # Increment to next month\n",
    "            start_date = end_date_of_month + datetime.timedelta(days=1)\n",
    "\n",
    "            # Remove the current monthly graph to generate a graph for the following month in the subsequent iteration\n",
    "            gds.run_cypher(\n",
    "                \"\"\"\n",
    "                CALL gds.graph.drop('transaction_graph') YIELD graphName\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering outside Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To investigate patterns of suspicious activities you will make time window aggregates such monthly frequency, total, mean and standard deviation of amount of incoming and outgoing transasactions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for clarity\n",
    "transactions_df.columns = ['tran_id', 'tx_type', 'base_amt', 'tran_timestamp', 'source', 'target']\n",
    "\n",
    "# Reordering columns for better readability\n",
    "transactions_df = transactions_df[[\"source\", \"target\", \"tran_timestamp\", \"tran_id\", \"base_amt\"]]\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "transactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incoming and Outgoing transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a DataFrame with monthly incoming and outgoing transaction statistics\n",
    "in_out_df = get_in_out_transactions(transactions_df)\n",
    "\n",
    "# Displaying the first few rows of the resulting DataFrame\n",
    "in_out_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions identified as suspicious activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the 'alert_transactions' DataFrame\n",
    "alert_transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating transaction labels based on transaction and alert transaction data\n",
    "transaction_labels = get_transaction_labels(\n",
    "    transactions_df, \n",
    "    alert_transactions,\n",
    ")\n",
    "\n",
    "# Displaying the first three rows of the resulting DataFrame\n",
    "transaction_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Party dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for clarity\n",
    "party.columns = [\"id\", \"type\"]\n",
    "\n",
    "# Mapping 'type' values to numerical values for better representation\n",
    "party.type = party.type.map({\"Individual\": 0, \"Organization\": 1})\n",
    "\n",
    "# Displaying the first three rows of the DataFrame\n",
    "party.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering transactions with SAR(Suspicious Activity Reports) labels from the generated transaction labels DataFrame\n",
    "alert_transactions = transaction_labels[transaction_labels.is_sar == 1]\n",
    "\n",
    "# Displaying the first few rows of transactions flagged as SAR\n",
    "alert_transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating party labels based on transaction labels and party information\n",
    "party_labels = get_party_labels(\n",
    "    transaction_labels, \n",
    "    party,\n",
    ")\n",
    "\n",
    "# Displaying the first three rows of the resulting DataFrame\n",
    "party_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date time to unix epoc milliseconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'tran_timestamp' values to milliseconds for consistency\n",
    "transaction_labels.tran_timestamp = transaction_labels.tran_timestamp.values.astype(np.int64) // 10 ** 6\n",
    "\n",
    "# Converting 'tran_timestamp' values in 'party_labels' to milliseconds\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.map(lambda x: datetime.datetime.timestamp(x) * 1000)\n",
    "party_labels.tran_timestamp = party_labels.tran_timestamp.values.astype(np.int64)\n",
    "\n",
    "# Displaying the first three rows of the DataFrame\n",
    "transaction_labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <span style=\"color:#ff5f27;\">üëÆüèº‚Äç‚ôÄÔ∏è Data Validation</span>\n",
    "\n",
    "Before you define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. You do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value you can decide whether to stop it to `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section you will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation with Greate Expectations in Hopsworks\n",
    "You can use GE library for validation in Hopsworks features store. \n",
    "\n",
    "##  <img src=\"../../images/icon102.png\" width=\"18px\"></img> Hopsworks feature store\n",
    "\n",
    "The Hopsworks feature feature store library is Apache V2 licensed and available [here](https://github.com/logicalclocks/feature-store-api). The library is currently available for Python and JVM languages such as Scala and Java.\n",
    "In this notebook, we are going to cover Python part.\n",
    "\n",
    "You can find the complete documentation of the library here: \n",
    "\n",
    "The first step is to establish a connection with your Hopsworks feature store instance and retrieve the object that represents the feature store you'll be working with. \n",
    "\n",
    "> By default `project.get_feature_store()` returns the feature store of the project we are working with. However, it accepts also a project name as parameter to select a different feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Expectations suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Expectation Suite - no use of HSFS\n",
    "import great_expectations as ge\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "expectation_suite = ge.core.ExpectationSuite(expectation_suite_name=\"aml_project_validations\")\n",
    "pprint(expectation_suite.to_json_dict(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite.add_expectation(\n",
    "  ge.core.ExpectationConfiguration(\n",
    "  expectation_type=\"expect_column_max_to_be_between\",\n",
    "  kwargs={\"column\": \"monthly_in_count\", \"min_value\": 0, \"max_value\": 10000000}) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(expectation_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Register Feature Groups in Hopsworks </span>\n",
    "\n",
    "A `Feature Groups` is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The `Feature Group` lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them.\n",
    "\n",
    "Generally, the features in a feature group are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, `feature groups` provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in.\n",
    "\n",
    "> It is important to note that `feature groups` are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read embeddings from Neo4j, and store them in the Hopsworks feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Neo4j, getting the embeddings, and putting them into a data frame\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    graph_embeddings_df = driver.execute_query(\n",
    "        \"\"\"MATCH (p:Party)-[t:TRANSACTION]->(:Party) \n",
    "            return \n",
    "            p.partyId as id, \n",
    "            p.node2vec as party_graph_embedding, \n",
    "            datetime(t.tran_timestamp).epochmillis as tran_timestamp\"\"\",\n",
    "        database_=DATABASE,\n",
    "        result_transformer_=neo4j.Result.to_df\n",
    "    )\n",
    "    \n",
    "    print(type(graph_embeddings_df))  # <class 'pandas.core.frame.DataFrame'>\n",
    "    print(graph_embeddings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase data size to store embeddings\n",
    "from hsfs import engine\n",
    "features = engine.get_instance().parse_schema_feature_group(graph_embeddings_df)\n",
    "for f in features:\n",
    "    if f.type == \"array<double>\" or f.type == \"array<float>\":\n",
    "        f.online_type = \"VARBINARY(20000)\"\n",
    "\n",
    "# Define Feature Group\n",
    "graph_embeddings_fg = fs.get_or_create_feature_group(name=\"graph_embeddings\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions graph\",\n",
    "                                       event_time = 'tran_timestamp',     \n",
    "                                       online_enabled=True,\n",
    "                                       features=features,\n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}\n",
    "                                       )\n",
    "\n",
    "# Insert data frame into Feature Group\n",
    "graph_embeddings_fg.insert(graph_embeddings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transactions monthly aggregates feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Group\n",
    "transactions_fg = fs.get_or_create_feature_group(\n",
    "    name = \"transactions_monthly\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    partition_key = [\"tran_timestamp\"],   \n",
    "    description = \"transactions monthly aggregates features\",\n",
    "    event_time = 'tran_timestamp',\n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False},\n",
    "    expectation_suite=expectation_suite\n",
    ")   \n",
    "\n",
    "# Insert data frame into Feature Group\n",
    "transactions_fg.insert(in_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Party feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Group\n",
    "party_fg = fs.get_or_create_feature_group(\n",
    "    name = \"party_labels\",\n",
    "    version = 1,\n",
    "    primary_key = [\"id\"],\n",
    "    description = \"party fg with labels\",\n",
    "    event_time = 'tran_timestamp',        \n",
    "    online_enabled = True,\n",
    "    statistics_config = {\"enabled\": True, \"histograms\": True, \"correlations\": True, \"exact_uniqueness\": False}\n",
    ")\n",
    "\n",
    "# Insert data frame into Feature Group\n",
    "party_fg.insert(party_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è **Next:** Part 02 </span>\n",
    "    \n",
    "In the following notebook you will use feature groups to create feature views and training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
