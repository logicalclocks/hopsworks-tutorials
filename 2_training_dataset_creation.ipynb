{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Warning! You should use **PySpark kernel** for this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#533a7b\"> **Dataset Creation** </a>\n",
    "---\n",
    "\n",
    "\n",
    "In this notebook, we will create the actual dataset that we will train our model on. In particular, we will:\n",
    "1. Select the features we want to train our model on.\n",
    "2. Specify how the features should be preprocessed.\n",
    "3. Create a feature view and a dataset.\n",
    "\n",
    "![tutorial-flow](images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#e363a3\"> **üìù Importing Libraries** </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>4</td><td>application_1654459018768_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-19-98.eu-north-1.compute.internal:8089/proxy/application_1654459018768_0003/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-21-191.eu-north-1.compute.internal:8044/node/containerlogs/container_e02_1654459018768_0003_01_000001/testing__romankah\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#3772ff\"> **‚¨áÔ∏è Data retrieving from Feature Groups** </a>\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for getting feature group `fares_fg`, defaulting to `1`.\n",
      "VersionWarning: No version provided for getting feature group `rides_fg`, defaulting to `1`."
     ]
    }
   ],
   "source": [
    "# Load feature groups.\n",
    "fares_fg = fs.get_feature_group(\"fares_fg\")\n",
    "rides_fg = fs.get_feature_group(\"rides_fg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `commit_details()` method we can look at all commits of this specific feature group. And then make a Point-in-Time join (retrieve records only within a certain time period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fares_commit_details = fares_fg.commit_details()\n",
    "rides_commit_details = rides_fg.commit_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1654465884060: {'committedOn': '20220605215124060', 'rowsUpdated': 0, 'rowsInserted': 41078, 'rowsDeleted': 0}}"
     ]
    }
   ],
   "source": [
    "fares_commit_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1654465941636: {'committedOn': '20220605215221636', 'rowsUpdated': 0, 'rowsInserted': 40907, 'rowsDeleted': 0}}"
     ]
    }
   ],
   "source": [
    "rides_commit_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time is in UNIX format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets just pick the last commit time for data query creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_time = max(fares_commit_details[sorted(fares_commit_details)[0]][\"committedOn\"], rides_commit_details[sorted(rides_commit_details)[0]][\"committedOn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'20220605215221636'"
     ]
    }
   ],
   "source": [
    "commit_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data.\n",
    "fg_query = fares_fg.select(['total_fare', 'pickup_datetime', 'month_of_the_ride'])\\\n",
    "                            .join(rides_fg.select_except(['taxi_id',\n",
    "                                  'driver_id']), on=['ride_id', 'pickup_datetime', 'month_of_the_ride']).as_of(commit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+-------+----------------+---------------+-----------------+----------------+---------------+------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----+-------+----+\n",
      "|total_fare|pickup_datetime|month_of_the_ride|ride_id|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|          distance|pickup_distance_to_jfk|dropoff_distance_to_jfk|pickup_distance_to_ewr|dropoff_distance_to_ewr|pickup_distance_to_lgr|dropoff_distance_to_lgr|year|weekday|hour|\n",
      "+----------+---------------+-----------------+-------+----------------+---------------+-----------------+----------------+---------------+------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----+-------+----+\n",
      "|      54.0|  1577882080000|           202001|    104|       -73.84859|       40.78362|        -73.75883|       40.897854|              3| 9.182044795302243|    10.503512118613543|      17.75482529176697|     18.25969729365474|      26.07673203447437|    1.4081705695938043|     10.299818578508603|2020|      2|  12|\n",
      "|     114.0|  1577882220000|           202001|    111|        -73.7743|      40.878162|        -73.78682|        40.86223|              3|1.2804898196329144|     16.36679144066612|     15.271610145607475|    24.663122018006018|      23.53473741831039|     8.724751532066003|      7.452462462401097|2020|      2|  12|\n",
      "+----------+---------------+-----------------+-------+----------------+---------------+-----------------+----------------+---------------+------------------+----------------------+-----------------------+----------------------+-----------------------+----------------------+-----------------------+----+-------+----+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "fg_query.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#772f1a\"> **„Ä∞Ô∏è Transformation Functions** </a>\n",
    "\n",
    "\n",
    "We can preprocess our data using several encoding methods like *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label_encoder', 'min_max_scaler', 'standard_scaler', 'robust_scaler']"
     ]
    }
   ],
   "source": [
    "[t_func.name for t_func in fs.get_transformation_functions()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load transformation functions.\n",
    "# min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "# label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# # Map features to transformations.\n",
    "# transformation_functions = {\n",
    "#     \"total_fare\": min_max_scaler,\n",
    "#     \"distance\": min_max_scaler\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#2db3f0\"> **üîÆ Feature View Creation** </a>\n",
    "\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference.\n",
    "\n",
    "After we have made a query from desired features, we should make a corresponding `Feature View`.\n",
    "In order to do it we may use `fs.create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at https://1e87e1e0-e1b3-11ec-8067-e932b2b957b4.cloud.hopsworks.ai/p/121/fs/69/fv/nyc_taxi_fares/version/1"
     ]
    }
   ],
   "source": [
    "nyc_fares_fv = fs.create_feature_view(\n",
    "    name='nyc_taxi_fares',\n",
    "    query=fg_query,\n",
    "    label=[\"total_fare\"]\n",
    "#     transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "nyc_fares_fv.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1577881300000, '202001', 561, -73.8609, 40.76794, -73.77363, 40.879017, 3, 8.928644115786534, 9.765811172299426, 16.426323820562693, 17.29121759663983, 24.724006251389003, 0.923638390507464, 8.793034715642845, 2020, 2, 15]\n",
      "SADeprecationWarning: The LegacyRow.items() method is deprecated and will be removed in a future release.  Use the Row._mapping attribute, i.e., 'row._mapping.items()'. (deprecated since: 1.4)"
     ]
    }
   ],
   "source": [
    "nyc_fares_fv.preview_feature_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#525252\"> **üì¶ Dataset Creation** </a>\n",
    "\n",
    "\n",
    "Finally we create the dataset using `fs.create_training_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `1`."
     ]
    }
   ],
   "source": [
    "td_metadata = nyc_fares_fv.create_training_dataset(\n",
    "    description = 'NYC taxi fares dataset.',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check that the transformation functions have been applied by loading the training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset has been splitted into two parts: **train (80% of original dataset)** and **validation (20% of original dataset)**.\n",
    "\n",
    "- To get training dataset we can use `FeatureView.get_training_dataset()` method.\n",
    "\n",
    "- To retrieve specific part of training dataset use `FeatureView.get_training_dataset_splits()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(total_fare=72.0, pickup_datetime=1577882700000, month_of_the_ride='202001', ride_id=135, pickup_longitude=-73.92254, pickup_latitude=40.689495, dropoff_longitude=-73.7673, dropoff_latitude=40.887066, passenger_count=2, distance=15.883820321741128, pickup_distance_to_jfk=8.27002312716127, dropoff_distance_to_jfk=16.99019305788723, pickup_distance_to_ewr=13.200250445207633, dropoff_distance_to_ewr=25.300090740530067, pickup_distance_to_lgr=6.5520408082752635, dropoff_distance_to_lgr=9.43680957272608, year=2020, weekday=2, hour=12)\n",
      "VersionWarning: No version provided for creating training dataset, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "td_version, df = nyc_fares_fv.get_training_dataset()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}