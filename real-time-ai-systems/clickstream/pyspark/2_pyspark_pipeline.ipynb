{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Streaming Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from hsfs import engine\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CTR_Streaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_config = engine.get_instance()._get_kafka_config(fs.id, {})\n",
    "EVENTS_TOPIC = \"clickstream_events\"\n",
    "CTR_TOPIC = f\"ctr_5min_{project.id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "events_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_config[\"bootstrap.servers\"]) \\\n",
    "    .option(\"subscribe\", EVENTS_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", LongType())\n",
    "])\n",
    "\n",
    "parsed_df = events_df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"timestamp\")/1000).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CTR in 5-minute windows\n",
    "ctr_df = parsed_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        \"user_id\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        sum(when(col(\"event_type\") == \"impression\", 1).otherwise(0)).alias(\"impressions\"),\n",
    "        sum(when(col(\"event_type\") == \"click\", 1).otherwise(0)).alias(\"clicks\")\n",
    "    ) \\\n",
    "    .withColumn(\"ctr\", \n",
    "        when(col(\"impressions\") > 0, col(\"clicks\") / col(\"impressions\"))\n",
    "        .otherwise(lit(None))\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"user_id\"),\n",
    "        col(\"impressions\"),\n",
    "        col(\"clicks\"),\n",
    "        col(\"ctr\"),\n",
    "        col(\"window.end\").alias(\"window_end\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Hopsworks via Kafka\n",
    "query = ctr_df \\\n",
    "    .selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_config[\"bootstrap.servers\"]) \\\n",
    "    .option(\"topic\", CTR_TOPIC) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/ctr_checkpoint\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Streaming to {CTR_TOPIC}\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}