{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e21e38c",
   "metadata": {},
   "source": [
    "## Apache Airflow (OPTIONAL) \n",
    "\n",
    "In this section of the hands-on-lab, we will utilize Snowpark's Python client-side Dataframe API as well as the Snowpark server-side runtime and Apache Airflow to create an operational pipeline.  We will take the functions created by the ML Ops team and create a directed acyclic graph (DAG) of operations to run each month when new data is available. \n",
    "\n",
    "Note: This code requires the ability to run docker containers locally.  If you do not have Docker Desktop you can run the same pipeline from a python kernel via the 04_ML_Ops.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c266ce3",
   "metadata": {},
   "source": [
    "We will use the dev CLI from Astronomer. https://docs.astronomer.io/astro/cli/get-started#step-1-install-the-astro-cli\n",
    "\n",
    "Follow the instructions to install the `astro` CLI for your particular local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/airflow_tasks.py\n",
    "\n",
    "from airflow.decorators import task\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def snowpark_database_setup(state_dict:dict)-> dict: \n",
    "    import snowflake.snowpark.functions as F\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.elt import reset_database\n",
    "\n",
    "    session, _ = snowpark_connect('./include/state.json')\n",
    "    reset_database(session=session, state_dict=state_dict, prestaged=True)\n",
    "\n",
    "    _ = session.sql('CREATE STAGE '+state_dict['model_stage_name']).collect()\n",
    "    _ = session.sql('CREATE TAG model_id_tag').collect()\n",
    "\n",
    "    session.close()\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def incremental_elt_task(state_dict: dict, files_to_download:list)-> dict:\n",
    "    from dags.ingest import incremental_elt\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    print('Ingesting '+str(files_to_download))\n",
    "    download_base_url=state_dict['connection_parameters']['download_base_url']\n",
    "\n",
    "    _ = session.use_warehouse(state_dict['compute_parameters']['load_warehouse'])\n",
    "\n",
    "    _ = incremental_elt(session=session, \n",
    "                        state_dict=state_dict, \n",
    "                        files_to_ingest=files_to_download,\n",
    "                        download_base_url=download_base_url,\n",
    "                        use_prestaged=True)\n",
    "\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['load_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def initial_bulk_load_task(state_dict:dict)-> dict:\n",
    "    from dags.ingest import bulk_elt\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.use_warehouse(state_dict['compute_parameters']['load_warehouse'])\n",
    "\n",
    "    print('Running initial bulk ingest from '+state_dict['connection_parameters']['download_base_url'])\n",
    "    \n",
    "    _ = bulk_elt(session=session, \n",
    "                 state_dict=state_dict, \n",
    "                 download_base_url=state_dict['connection_parameters']['download_base_url'],\n",
    "                 use_prestaged=True)\n",
    "\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['load_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def materialize_holiday_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import materialize_holiday_table\n",
    "\n",
    "    print('Materializing holiday table.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = materialize_holiday_table(session=session, \n",
    "                                  holiday_table_name=state_dict['holiday_table_name'])\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def subscribe_to_weather_data_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import subscribe_to_weather_data\n",
    "\n",
    "    print('Subscribing to weather data')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = subscribe_to_weather_data(session=session, \n",
    "                                  weather_database_name=state_dict['weather_database_name'], \n",
    "                                  weather_listing_id=state_dict['weather_listing_id'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def create_weather_view_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_weather_view\n",
    "\n",
    "    print('Creating weather view')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = create_weather_view(session=session,\n",
    "                            weather_table_name=state_dict['weather_table_name'],\n",
    "                            weather_view_name=state_dict['weather_view_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "    \n",
    "@task.virtualenv(python_version=3.8)\n",
    "def deploy_model_udf_task(state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import deploy_pred_train_udf\n",
    "\n",
    "    print('Deploying station model')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()\n",
    "\n",
    "    _ = deploy_pred_train_udf(session=session, \n",
    "                              udf_name=state_dict['train_udf_name'],\n",
    "                              function_name=state_dict['train_func_name'],\n",
    "                              model_stage_name=state_dict['model_stage_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def deploy_eval_udf_task(state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import deploy_eval_udf\n",
    "\n",
    "    print('Deploying station model')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()\n",
    "\n",
    "    _ = deploy_eval_udf(session=session, \n",
    "                        udf_name=state_dict['eval_udf_name'],\n",
    "                        function_name=state_dict['eval_func_name'],\n",
    "                        model_stage_name=state_dict['model_stage_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def generate_feature_table_task(state_dict:dict, \n",
    "                                holiday_state_dict:dict, \n",
    "                                weather_state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_feature_table\n",
    "\n",
    "    print('Generating features for all stations.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "    _ = session.sql(\"CREATE OR REPLACE TABLE \"+state_dict['clone_table_name']+\\\n",
    "                    \" CLONE \"+state_dict['trips_table_name']).collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['clone_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    _ = create_feature_table(session, \n",
    "                             trips_table_name=state_dict['clone_table_name'], \n",
    "                             holiday_table_name=state_dict['holiday_table_name'], \n",
    "                             weather_view_name=state_dict['weather_view_name'],\n",
    "                             feature_table_name=state_dict['feature_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['feature_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def generate_forecast_table_task(state_dict:dict, \n",
    "                                 holiday_state_dict:dict, \n",
    "                                 weather_state_dict:dict)-> dict: \n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_forecast_table\n",
    "\n",
    "    print('Generating forecast features.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = create_forecast_table(session, \n",
    "                              trips_table_name=state_dict['trips_table_name'],\n",
    "                              holiday_table_name=state_dict['holiday_table_name'], \n",
    "                              weather_view_name=state_dict['weather_view_name'], \n",
    "                              forecast_table_name=state_dict['forecast_table_name'],\n",
    "                              steps=state_dict['forecast_steps'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['forecast_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def bulk_train_predict_task(state_dict:dict, \n",
    "                            feature_state_dict:dict, \n",
    "                            forecast_state_dict:dict)-> dict: \n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import train_predict\n",
    "\n",
    "    state_dict = feature_state_dict\n",
    "\n",
    "    print('Running bulk training and forecast.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    session.use_warehouse(state_dict['compute_parameters']['train_warehouse'])\n",
    "\n",
    "    pred_table_name = train_predict(session, \n",
    "                                    station_train_pred_udf_name=state_dict['train_udf_name'], \n",
    "                                    feature_table_name=state_dict['feature_table_name'], \n",
    "                                    forecast_table_name=state_dict['forecast_table_name'],\n",
    "                                    pred_table_name=state_dict['pred_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['pred_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['train_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def eval_station_models_task(state_dict:dict, \n",
    "                             pred_state_dict:dict,\n",
    "                             run_date:str)-> dict:\n",
    "\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import evaluate_station_model\n",
    "\n",
    "    print('Running eval UDF for model output')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    eval_table_name = evaluate_station_model(session, \n",
    "                                             run_date=run_date, \n",
    "                                             eval_model_udf_name=state_dict['eval_udf_name'], \n",
    "                                             pred_table_name=state_dict['pred_table_name'], \n",
    "                                             eval_table_name=state_dict['eval_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['eval_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    session.close()\n",
    "    return state_dict                                               \n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def flatten_tables_task(pred_state_dict:dict, state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import flatten_tables\n",
    "\n",
    "    print('Flattening tables for end-user consumption.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    flat_pred_table, flat_forecast_table, flat_eval_table = flatten_tables(session,\n",
    "                                                                           pred_table_name=state_dict['pred_table_name'], \n",
    "                                                                           forecast_table_name=state_dict['forecast_table_name'], \n",
    "                                                                           eval_table_name=state_dict['eval_table_name'])\n",
    "    state_dict['flat_pred_table'] = flat_pred_table\n",
    "    state_dict['flat_forecast_table'] = flat_forecast_table\n",
    "    state_dict['flat_eval_table'] = flat_eval_table\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_pred_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_forecast_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_eval_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/airflow_setup_pipeline.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from dags.airflow_tasks import snowpark_database_setup\n",
    "from dags.airflow_tasks import incremental_elt_task\n",
    "from dags.airflow_tasks import initial_bulk_load_task\n",
    "from dags.airflow_tasks import materialize_holiday_task\n",
    "from dags.airflow_tasks import subscribe_to_weather_data_task\n",
    "from dags.airflow_tasks import create_weather_view_task\n",
    "from dags.airflow_tasks import deploy_model_udf_task\n",
    "from dags.airflow_tasks import deploy_eval_udf_task\n",
    "from dags.airflow_tasks import generate_feature_table_task\n",
    "from dags.airflow_tasks import generate_forecast_table_task\n",
    "from dags.airflow_tasks import bulk_train_predict_task\n",
    "from dags.airflow_tasks import eval_station_models_task \n",
    "from dags.airflow_tasks import flatten_tables_task\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "#local_airflow_path = '/usr/local/airflow/'\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, start_date=datetime(2020, 3, 1), catchup=False, tags=['setup'])\n",
    "def citibikeml_setup_taskflow(run_date:str):\n",
    "    \"\"\"\n",
    "    Setup initial Snowpark / Astronomer ML Demo\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    with open('./include/state.json') as sdf:\n",
    "        state_dict = json.load(sdf)\n",
    "    \n",
    "    model_id = str(uuid.uuid1()).replace('-', '_')\n",
    "\n",
    "    state_dict.update({'model_id': model_id})\n",
    "    state_dict.update({'run_date': run_date})\n",
    "    state_dict.update({'weather_database_name': 'WEATHER_NYC'})\n",
    "    state_dict.update({'load_table_name': 'RAW_',\n",
    "                       'trips_table_name': 'TRIPS',\n",
    "                       'load_stage_name': 'LOAD_STAGE',\n",
    "                       'model_stage_name': 'MODEL_STAGE',\n",
    "                       'weather_table_name': state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY',\n",
    "                       'weather_view_name': 'WEATHER_NYC_VW',\n",
    "                       'holiday_table_name': 'HOLIDAYS',\n",
    "                       'clone_table_name': 'CLONE_'+model_id,\n",
    "                       'feature_table_name' : 'FEATURE_'+model_id,\n",
    "                       'pred_table_name': 'PRED_'+model_id,\n",
    "                       'eval_table_name': 'EVAL_'+model_id,\n",
    "                       'forecast_table_name': 'FORECAST_'+model_id,\n",
    "                       'forecast_steps': 30,\n",
    "                       'train_udf_name': 'station_train_predict_udf',\n",
    "                       'train_func_name': 'station_train_predict_func',\n",
    "                       'eval_udf_name': 'eval_model_output_udf',\n",
    "                       'eval_func_name': 'eval_model_func'\n",
    "                      })\n",
    "    \n",
    "    #Task order - one-time setup\n",
    "    setup_state_dict = snowpark_database_setup(state_dict)\n",
    "    load_state_dict = initial_bulk_load_task(setup_state_dict)\n",
    "    holiday_state_dict = materialize_holiday_task(setup_state_dict)\n",
    "    subscribe_state_dict = subscribe_to_weather_data_task(setup_state_dict)\n",
    "    weather_state_dict = create_weather_view_task(subscribe_state_dict)\n",
    "    model_udf_state_dict = deploy_model_udf_task(setup_state_dict)\n",
    "    eval_udf_state_dict = deploy_eval_udf_task(setup_state_dict)\n",
    "    feature_state_dict = generate_feature_table_task(load_state_dict, holiday_state_dict, weather_state_dict) \n",
    "    foecast_state_dict = generate_forecast_table_task(load_state_dict, holiday_state_dict, weather_state_dict)\n",
    "    pred_state_dict = bulk_train_predict_task(model_udf_state_dict, feature_state_dict, foecast_state_dict)\n",
    "    eval_state_dict = eval_station_models_task(eval_udf_state_dict, pred_state_dict, run_date)  \n",
    "    state_dict = flatten_tables_task(pred_state_dict, eval_state_dict)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "run_date='2020_01_01'\n",
    "\n",
    "state_dict = citibikeml_setup_taskflow(run_date=run_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/airflow_incremental_pipeline.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from dags.airflow_tasks import snowpark_database_setup\n",
    "from dags.airflow_tasks import incremental_elt_task\n",
    "from dags.airflow_tasks import initial_bulk_load_task\n",
    "from dags.airflow_tasks import materialize_holiday_task\n",
    "from dags.airflow_tasks import deploy_model_udf_task\n",
    "from dags.airflow_tasks import deploy_eval_udf_task\n",
    "from dags.airflow_tasks import generate_feature_table_task\n",
    "from dags.airflow_tasks import generate_forecast_table_task\n",
    "from dags.airflow_tasks import bulk_train_predict_task\n",
    "from dags.airflow_tasks import eval_station_models_task \n",
    "from dags.airflow_tasks import flatten_tables_task\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "#local_airflow_path = '/usr/local/airflow/'\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, start_date=datetime(2020, 4, 1), catchup=False, tags=['monthly'])\n",
    "def citibikeml_monthly_taskflow(files_to_download:list, run_date:str):\n",
    "    \"\"\"\n",
    "    End to end Snowpark / Astronomer ML Demo\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    with open('./include/state.json') as sdf:\n",
    "        state_dict = json.load(sdf)\n",
    "    \n",
    "    model_id = str(uuid.uuid1()).replace('-', '_')\n",
    "\n",
    "    state_dict.update({'model_id': model_id})\n",
    "    state_dict.update({'run_date': run_date})\n",
    "    state_dict.update({'weather_database_name': 'WEATHER_NYC'})\n",
    "    state_dict.update({'load_table_name': 'RAW_',\n",
    "                       'trips_table_name': 'TRIPS',\n",
    "                       'load_stage_name': 'LOAD_STAGE',\n",
    "                       'model_stage_name': 'MODEL_STAGE',\n",
    "                       'weather_table_name': state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY',\n",
    "                       'weather_view_name': 'WEATHER_NYC_VW',\n",
    "                       'holiday_table_name': 'HOLIDAYS',\n",
    "                       'clone_table_name': 'CLONE_'+model_id,\n",
    "                       'feature_table_name' : 'FEATURE_'+model_id,\n",
    "                       'pred_table_name': 'PRED_'+model_id,\n",
    "                       'eval_table_name': 'EVAL_'+model_id,\n",
    "                       'forecast_table_name': 'FORECAST_'+model_id,\n",
    "                       'forecast_steps': 30,\n",
    "                       'train_udf_name': 'station_train_predict_udf',\n",
    "                       'train_func_name': 'station_train_predict_func',\n",
    "                       'eval_udf_name': 'eval_model_output_udf',\n",
    "                       'eval_func_name': 'eval_model_func'\n",
    "                      })\n",
    "\n",
    "    incr_state_dict = incremental_elt_task(state_dict, files_to_download)\n",
    "    feature_state_dict = generate_feature_table_task(incr_state_dict, incr_state_dict, incr_state_dict) \n",
    "    forecast_state_dict = generate_forecast_table_task(incr_state_dict, incr_state_dict, incr_state_dict)\n",
    "    pred_state_dict = bulk_train_predict_task(feature_state_dict, feature_state_dict, forecast_state_dict)\n",
    "    eval_state_dict = eval_station_models_task(pred_state_dict, pred_state_dict, run_date)\n",
    "    state_dict = flatten_tables_task(pred_state_dict, eval_state_dict)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "run_date='2020_02_01'\n",
    "files_to_download = ['202001-citibike-tripdata.csv.zip']\n",
    "\n",
    "state_dict = citibikeml_monthly_taskflow(files_to_download=files_to_download, \n",
    "                                         run_date=run_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183b8fa",
   "metadata": {},
   "source": [
    "Now open a new browser tab to localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# generate an URL\n",
    "url = 'https://localhost:8080'\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1ebc5",
   "metadata": {},
   "source": [
    "Lets run the initial setup, ingest and forecast DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This sample code can be used to trigger the Airflow pipeline from a command-line shell.\n",
    "# !curl -X POST 'http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns' \\\n",
    "# -H 'Content-Type: application/json' \\\n",
    "# --user \"admin:admin\" \\\n",
    "# -d '{\"conf\": {\"files_to_download\": [\"202003-citibike-tripdata.csv.zip\"], \"run_date\": \"2020_04_01\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d7c97",
   "metadata": {},
   "source": [
    "Alternatively we can use a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84288d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "\n",
    "dag_url='http://localhost:8080/api/v1/dags/citibikeml_setup_taskflow/dagRuns'\n",
    "json_payload = {\"conf\": {\"run_date\": \"2020_01_01\"}}\n",
    "\n",
    "response = requests.post(dag_url, \n",
    "                        json=json_payload,\n",
    "                        auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "run_id = json.loads(response.text)['dag_run_id']\n",
    "\n",
    "state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "while state != 'success':\n",
    "    print('DAG running...'+state)\n",
    "    time.sleep(10)\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3dbec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "\n",
    "dag_url='http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns'\n",
    "json_payload = {\"conf\": {\"files_to_download\": [\"202001-citibike-tripdata.csv.zip\"], \"run_date\": \"2020_02_01\"}}\n",
    "\n",
    "response = requests.post(dag_url, \n",
    "                        json=json_payload,\n",
    "                        auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "run_id = json.loads(response.text)['dag_run_id']\n",
    "\n",
    "state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "while state != 'success':\n",
    "    print('DAG running...'+state)\n",
    "    time.sleep(10)\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad097d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
