{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbdfd41",
   "metadata": {},
   "source": [
    "# Hopsworks Feature Store - Part 03 - Model training, Lineage.\n",
    "\n",
    "In this notebook, we'll train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch.\n",
    "\n",
    "![tutorial-flow](images/model_training_other.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5847a",
   "metadata": {},
   "source": [
    "### Load Training Data\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.get_training_dataset(\"transactions_dataset_splitted\")\n",
    "X_train = td.read(\"train\")\n",
    "X_val = td.read(\"validation\")\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59d4ca",
   "metadata": {},
   "source": [
    "Next, we'll one-hot encode the categorical feature `category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "one_hot_train = pd.DataFrame(enc.fit_transform(X_train[[\"category\"]]))\n",
    "one_hot_val = pd.DataFrame(enc.transform(X_val[[\"category\"]]))\n",
    "X_train = pd.concat([X_train.drop(columns=\"category\"), one_hot_train], axis=1)\n",
    "X_val = pd.concat([X_val.drop(columns=\"category\"), one_hot_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24436cd",
   "metadata": {},
   "source": [
    "We will train a model to predict `fraud_label` given the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = td.label[0] # \"fraud_label\"\n",
    "\n",
    "y_train = X_train.pop(target)\n",
    "y_val = X_val.pop(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84b1d0",
   "metadata": {},
   "source": [
    "Let's check the distribution of our target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4895016",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a0a96",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus we should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, we'll use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4d28c",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9813de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train model.\n",
    "clf = LogisticRegression(class_weight={0: 1, 1: 2}, solver='liblinear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df622df",
   "metadata": {},
   "source": [
    "Let's see how well it performs on our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a711e90",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "We have now performed a simple training with training data that we have created in the feature store. This will also allow us to navigate in the lineage graph as seen in the following GIF. \n",
    "\n",
    "[//]: <> (insert GIF here)\n",
    "\n",
    "This concludes the fisrt module and introduction to the core aspect of the feauture store. In the second module we will introduce streaming and external feature groups for a similar fraud use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
