{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 5 main sections:** \n",
    "1. **Loading the training data.**\n",
    "2. **Train the model.**\n",
    "3. **Register model to Hopsworks model registry.**\n",
    "4. **Deploy the model on KServe behind Hopsworks for real-time inference requests.**\n",
    "5. **Test model deployment and use model serving rest APIs.**\n",
    "\n",
    "![tutorial-flow](../images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hsfs and retrieve datasets for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get feature view objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view('aml_feature_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™ù Training Dataset retreival </span>\n",
    "\n",
    "To retrieve training data from storage (already materialised) or from feature groups direcly we can use `get_training_dataset_splits` or `get_training_dataset` methods. If version is not provided or provided version has not already existed, it creates a new version of training data according to given arguments and returns a dataframe. If version is provided and has already existed, it reads training data from storage or feature groups and returns a dataframe. If split is provided, it reads the specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y  = feature_view.get_training_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode embeddings\n",
    "train_x['graph_embeddings'] = train_x['graph_embeddings'].apply(literal_eval)\n",
    "train_x = train_x.merge( pd.DataFrame(train_x['graph_embeddings'].to_list()).add_prefix('emb_'), left_index=True, right_index=True).drop('graph_embeddings', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for suspicios activity.  We will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_transactions = train_x[train_y.values == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_transactions = non_sar_transactions.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets define Tensorflow Dataset as we are going to train keras tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    return ds.batch(batch_size,True).prefetch(1)\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(non_sar_transactions.values, tf.float32))\n",
    "training_dataset = windowed_dataset(training_dataset, window_size=2, batch_size=16)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "![tutorial-flow](images/model_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanEncAnomalyDetector(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(GanEncAnomalyDetector, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = [1, input_dim[1]] \n",
    "        self.d_steps = 3\n",
    "        self.gp_weight = 10 \n",
    "        \n",
    "        self.encoder = self.make_encoder_model(self.input_dim)\n",
    "        self.generator = self.make_generator(self.input_dim, self.latent_dim)\n",
    "        self.discriminator = self.make_discriminator_model(self.input_dim)\n",
    "\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.epoch_e_loss_avg = tf.keras.metrics.Mean(name=\"epoch_e_loss_avg\")\n",
    "        self.epoch_d_loss_avg = tf.keras.metrics.Mean(name=\"epoch_d_loss_avg\")\n",
    "        self.epoch_g_loss_avg = tf.keras.metrics.Mean(name=\"epoch_g_loss_avg\")\n",
    "        self.epoch_a_score_avg = tf.keras.metrics.Mean(name=\"epoch_a_score_avg\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.epoch_e_loss_avg,\n",
    "                self.epoch_d_loss_avg,\n",
    "                self.epoch_g_loss_avg,\n",
    "                self.epoch_a_score_avg,\n",
    "            ]\n",
    "\n",
    "    # define model architectures\n",
    "    def make_encoder_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "        encoder = tf.keras.Model(inputs=inputs, outputs=x, name=\"encoder_model\")\n",
    "        return encoder\n",
    "\n",
    "    def make_generator(self, input_dim, latent_dim):\n",
    "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim[0],latent_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 8, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(latent_inputs) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        #x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        generator = tf.keras.Model(inputs=latent_inputs, outputs=x, name=\"generator_model\")        \n",
    "        return generator\n",
    "\n",
    "    def make_discriminator_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 128, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "\n",
    "        # dense output layer\n",
    "        x = tf.keras.layers.Flatten()(x)    \n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(128)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        prediction = tf.keras.layers.Dense(1)(x)\n",
    "        discriminator = tf.keras.Model(inputs=inputs, outputs=prediction, name=\"discriminator_model\" )               \n",
    "        return discriminator\n",
    "        \n",
    "    # Training function\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        if isinstance(real_data, tuple):\n",
    "            real_data = real_data[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 1a. Train the encoder and get the encoder loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1])), \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake data from the latent vector\n",
    "                fake_data = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "                #(somewhere here step forward?)\n",
    "                # Get the logits for the fake data\n",
    "                fake_logits = self.discriminator(fake_data, training=True)\n",
    "                # Get the logits for the real data\n",
    "                real_logits = self.discriminator(real_data, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real sample logits\n",
    "                d_cost = self.discriminator_loss(real_sample=real_logits, fake_sample=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_data, fake_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake data using the generator\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            gen_sample_logits = self.discriminator(generated_data, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the encoder\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Compress generate fake data from the latent vector\n",
    "            encoded_fake_data = self.encoder(generated_data, training=True)\n",
    "            # Reconstruct encoded generate fake data\n",
    "            generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=True)\n",
    "            # Encode the latent vector\n",
    "            encoded_random_latent_vectors = self.encoder(tf.random.normal(shape=(batch_size, self.input_dim[0], self.input_dim[1])), \n",
    "                                                         training=True)\n",
    "            # Calculate encoder loss\n",
    "            e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        enc_gradient = tape.gradient(e_loss, self.encoder.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.e_optimizer.apply_gradients(\n",
    "            zip(enc_gradient, self.encoder.trainable_variables)\n",
    "        )\n",
    "\n",
    "        anomaly_score = self.compute_anomaly_score(real_data)\n",
    "\n",
    "        self.epoch_d_loss_avg.update_state(d_loss)\n",
    "        self.epoch_g_loss_avg.update_state(g_loss)\n",
    "        self.epoch_e_loss_avg.update_state(e_loss)\n",
    "        self.epoch_a_score_avg.update_state(anomaly_score[\"anomaly_score\"])\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"e_loss\": e_loss, \"anomaly_score\": anomaly_score[\"anomaly_score\"]}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        batch_size = tf.shape(input)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        # Generate fake data using the generator\n",
    "        generated_data = self.generator(random_latent_vectors, training=False)\n",
    "        # Get the discriminator logits for fake data\n",
    "        gen_sample_logits = self.discriminator(generated_data, training=False)\n",
    "        # Calculate the generator loss\n",
    "        g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        \n",
    "        # Compress generate fake data from the latent vector\n",
    "        encoded_fake_data = self.encoder(generated_data, training=False)\n",
    "        # Reconstruct encoded generate fake data\n",
    "        generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=False)\n",
    "\n",
    "        # Calculate encoder loss\n",
    "        e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "        \n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"e_loss\": e_loss,\n",
    "            \"anomaly_score\": anomaly_score[\"anomaly_score\"]\n",
    "        }\n",
    "    \n",
    "    # define custom server function\n",
    "    @tf.function\n",
    "    def serve_function(self, input):\n",
    "        return self.compute_anomaly_score(input)\n",
    "\n",
    "    def call(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        encoded = self.encoder(input)\n",
    "        decoded = self.generator(encoded)\n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return anomaly_score[\"anomaly_score\"], decoded\n",
    "\n",
    "    def compile(self):\n",
    "        super(GanEncAnomalyDetector, self).compile()     \n",
    "        # Define optimizers\n",
    "        self.e_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)        \n",
    "        self.d_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "        self.g_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "\n",
    "    def gradient_penalty(self, real_data, fake_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated sample\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated sample\n",
    "        real_data_shape = tf.shape(real_data)\n",
    "        alpha = tf.random.normal(shape=[real_data_shape[0], real_data_shape[1], real_data_shape[2]], mean=0.0, stddev=2.0, dtype=tf.dtypes.float32)\n",
    "        #alpha = tf.random_uniform([self.batch_size, 1], minval=-2, maxval=2, dtype=tf.dtypes.float32)\n",
    "        interpolated = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated sample.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated sample.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[-2, -1]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp    \n",
    "        \n",
    "    def encoder_loss(self,generated_fake_data, generator_reconstructed_encoded_fake_data):\n",
    "        generator_reconstracted_data = tf.cast(generator_reconstructed_encoded_fake_data, tf.float32)\n",
    "        loss = self.mse(generated_fake_data, generator_reconstracted_data)\n",
    "        beta_cycle_gen = 10.0\n",
    "        loss = loss * beta_cycle_gen\n",
    "        return loss\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def discriminator_loss(self, real_sample, fake_sample):\n",
    "        real_loss = tf.reduce_mean(real_sample)\n",
    "        fake_loss = tf.reduce_mean(fake_sample)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(self, fake_sample):\n",
    "        return -tf.reduce_mean(fake_sample)\n",
    "    \n",
    "    def compute_anomaly_score(self, input):\n",
    "        \"\"\"anomaly score.\n",
    "          See https://arxiv.org/pdf/1905.11034.pdf for more details\n",
    "        \"\"\"\n",
    "        # Encode the real data\n",
    "        encoded_real_data = self.encoder(input, training=False)\n",
    "        # Reconstruct encoded real data\n",
    "        generator_reconstructed_encoded_real_data = self.generator(encoded_real_data, training=False)\n",
    "        # Calculate distance between real and reconstructed data (Here may be step forward?)\n",
    "        gen_rec_loss_predict = self.mse(input,generator_reconstructed_encoded_real_data)\n",
    "\n",
    "        # # Compute anomaly score\n",
    "        # real_to_orig_dist_predict = tf.math.reduce_sum(tf.math.pow(encoded_random_latent - encoded_real_data, 2), axis=[-1])\n",
    "        # anomaly_score = (gen_rec_loss_predict * self.anomaly_alpha) + ((1 - self.anomaly_alpha) * real_to_orig_dist_predict)\n",
    "        anomaly_score = gen_rec_loss_predict\n",
    "        return {'anomaly_score': anomaly_score} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GanEncAnomalyDetector([2, 41])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_dataset, epochs=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics={'loss': history.history[\"g_loss\"][0]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">  Register anomaly detection model to hopsworks model registry </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = \"aml_model\"\n",
    "print('Exporting trained model to: {}'.format(export_path))\n",
    "    \n",
    "call = model.serve_function.get_concrete_function(tf.TensorSpec([None,None,None], tf.float32))\n",
    "tf.saved_model.save(model, export_path, signatures=call)\n",
    "\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "mr_model = mr.tensorflow.create_model(\n",
    "    name=\"aml_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Adversarial anomaly detection model.\",\n",
    "    input_example=[\"70408aef\"]\n",
    ")\n",
    "\n",
    "mr_model.save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> Deploy model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile aml_model_transformer.py\n",
    "\n",
    "import os\n",
    "import hsfs\n",
    "import numpy as np\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):        \n",
    "        # get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # get feature views\n",
    "        self.fv = self.fs.get_feature_view(\"aml_feature_view\", 1)\n",
    "        \n",
    "        # initialise serving\n",
    "        self.fv.init_serving(1)\n",
    "    \n",
    "    def preprocess(self, inputs):\n",
    "        feature_vector = self.fv.get_feature_vector({\"id\": inputs[\"inputs\"][0]})\n",
    "        return { \"inputs\" :  np.array(list(flat2gen(feature_view.get_feature_vector({'id': node_id})))).reshape(1,41).tolist() }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "    def flat2gen(self, alist):\n",
    "        for item in alist:\n",
    "            if isinstance(item, list):\n",
    "                for subitem in item: yield subitem\n",
    "            else:\n",
    "                yield item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hsml.transformer import Transformer\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"aml_model_transformer.py\", \"Models\", overwrite=True)\n",
    "transformer_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)\n",
    "transformer_script = Transformer(script_file=transformer_script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"aml_model\", version=1)\n",
    "\n",
    "# Give it any name you want\n",
    "deployment = model.deploy(\n",
    "    name=\"amlmodeldeployment\",\n",
    "    model_server=\"TENSORFLOW_SERVING\", \n",
    "    serving_tool=\"KSERVE\",\n",
    "    transformer=transformer_script\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For trouble shooting one can use `get_logs` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the deployment\n",
    "Let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": model.input_example\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trouble shooting one can use get_logs methods\n",
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use REST endpoint\n",
    "You can also use a REST endpoint for your model. To do this you need to create an API key with 'serving' enabled, and retrieve the endpoint URL from the Model Serving UI.\n",
    "\n",
    "Go to the Model Serving UI and click on the eye icon next to a model to retrieve the endpoint URL. The shorter URL is an internal endpoint that you can only reach from within Hopsworks. If you want to call it from outside, you need one of the longer URLs. \n",
    "\n",
    "![serving-endpoints](images/serving_endpoints.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"fraud_tutorial_model\", version=1)\n",
    "\n",
    "test_inputs = model.input_example\n",
    "\n",
    "API_KEY = \"...\"  # Put your API key here.\n",
    "MODEL_SERVING_URL = \"...\" # Put model serving endppoint here.\n",
    "HOST_NAME = \"...\" # Put your hopsworks model serving hostname here \n",
    "\n",
    "data = {\"inputs\": test_inputs}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\", \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {API_KEY}\",\n",
    "    \"Host\": HOST_NAME}\n",
    "\n",
    "response = requests.post(MODEL_SERVING_URL, verify=False, headers=headers, json=data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets test feature vectors from online store\n",
    "ids_to_score = [\"0016359b\", \n",
    "                \"001dcc27\", \n",
    "                \"0054a022\", \n",
    "                \"00d6b609\", \n",
    "                \"00e14860\", \n",
    "                \"00e39a1b\", \n",
    "                \"014ed5cb\", \n",
    "                \"01ce3306\", \n",
    "                \"01fa19ae\", \n",
    "                \"01fa1d01\", \n",
    "                \"036dce03\", \n",
    "                \"03e09be4\", \n",
    "                \"04b23f4b\"]\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    data = {\"inputs\": [node_id]}\n",
    "    print(\" anomaly score for node_id \", node_id, \" : \",   deployment.predict(data)[\"outputs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Deployment\n",
    "To stop the deployment we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ Wrapping things up </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module you perforemed feature engineering, created feature view and traning dataset, trained advesarial anomaly detection model and depoyed it in production. To setup this pipeline in your enterprise settings contuct us.\n",
    "\n",
    "<img src=\"images/contuct_us.png\" width=\"400px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
