{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32d6274",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\">**Hopsworks Feature Store** </span> <span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Dataset and Modeling</span>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/advanced_tutorials/3_training_dataset_and_modeling.ipynb)\n",
    "\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into 3 main sections:\n",
    "1. Feature selection.\n",
    "2. Feature transformations.\n",
    "3. Training datasets creation.\n",
    "4. Loading the training data.\n",
    "5. Train the model.\n",
    "6. Register model to Hopsworks model registry.\n",
    "\n",
    "![02_training-dataset](../../images/02_training-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c6a51",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_prices_fg = fs.get_or_create_feature_group(\n",
    "    name = 'electricity_prices',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorological_measurements_fg = fs.get_or_create_feature_group(\n",
    "    name = 'meteorological_measurements',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc293e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "swedish_holidays_fg = fs.get_or_create_feature_group(\n",
    "    name = 'swedish_holidays',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b4aee",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üñç Feature View Creation and Retrieving </span>\n",
    "\n",
    "Let's start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data.\n",
    "fg_query = electricity_prices_fg.select_all()\\\n",
    "                        .join(\n",
    "                            meteorological_measurements_fg.select_except([\"timestamp\"])\n",
    "                        )\\\n",
    "                        .join(\n",
    "                            swedish_holidays_fg.select_all()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this if you would like to view query results\n",
    "fg_query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3e924",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions</span>\n",
    "\n",
    "Hopsworks Feature Store provides functionality to attach transformation functions to feature views and comes with built-in transformation functions such as `min_max_scaler`, `standard_scaler`, `robust_scaler` and `label_encoder`.\n",
    "\n",
    "You will preprocess your data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_areas = [\"se1\", \"se2\", \"se3\", \"se4\"]\n",
    "\n",
    "#Map features to transformations\n",
    "mapping_transformers = {}\n",
    "for area in price_areas:\n",
    "    mapping_transformers[f\"price_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "    mapping_transformers[f\"mean_temp_per_day_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "    mapping_transformers[f\"mean_wind_speed_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "    mapping_transformers[f\"precipitaton_amount_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "    mapping_transformers[f\"total_sunshine_time_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "    mapping_transformers[f\"mean_cloud_perc_{area}\"] = fs.get_transformation_function(name=\"min_max_scaler\")    \n",
    "    mapping_transformers[f\"precipitaton_type_{area}\"] = fs.get_transformation_function(name='label_encoder')\n",
    "\n",
    "mapping_transformers[\"type_of_day\"] = fs.get_transformation_function(name='label_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab31f59",
   "metadata": {},
   "source": [
    "`Feature Views` stands between **Feature Groups** and **Training Dataset**. –°ombining **Feature Groups** we can create **Feature Views** which store a metadata of our data. Having **Feature Views** we can create **Training Dataset**.\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.get_or_create_feature_view()` method.\n",
    "\n",
    "We can specify next parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - functions to transform our features.\n",
    "\n",
    "- `query` - query object with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='electricity_feature_view',\n",
    "    version=1,\n",
    "    transformation_functions=mapping_transformers,\n",
    "    query=fg_query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591203c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "To create training dataset we use `FeatureView.create_training_data()` method.\n",
    "\n",
    "Here are some importand things:\n",
    "\n",
    "- It will inherit the name of FeatureView.\n",
    "\n",
    "- The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "- We can choose necessary format using **data_format** parameter.\n",
    "\n",
    "- **start_time** and **end_time** in order to filter dataset in specific time range.\n",
    "\n",
    "- We can create **train, test** splits using `create_train_test_split()`. \n",
    "\n",
    "- We can create **train,validation, test** splits using `create_train_validation_test_splits()` methods.\n",
    "\n",
    "- The only thing is that we should specify desired ratio of splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465766",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ‚õ≥Ô∏è Dataset with train, test and validation splits</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d99544",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_version, td_job = feature_view.create_train_validation_test_split(\n",
    "    train_start=\"20210101\",\n",
    "    train_end=\"20220228\",\n",
    "    validation_start=\"20220301\",\n",
    "    validation_end=\"20220531\",\n",
    "    test_start=\"20220601\",\n",
    "    test_end=\"20220909\",\n",
    "    description='Electricity price prediction dataset',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options = {'wait_for_job': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c1132",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">ü™ù Training Dataset Retrieval</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, _, _, _  = feature_view.get_train_validation_test_split(\n",
    "    training_dataset_version=td_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c807074",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sort_values([\"timestamp\"], inplace=True)\n",
    "X_val.sort_values([\"timestamp\"], inplace=True)\n",
    "X_test.sort_values([\"timestamp\"], inplace=True)\n",
    "\n",
    "y_train = X_train[[\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"]]\n",
    "y_val = X_val[[\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"]]\n",
    "y_test = X_test[[\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"]]\n",
    "\n",
    "X_train.drop([\"day\", \"timestamp\"], axis = 1, inplace = True)\n",
    "X_val.drop([\"day\", \"timestamp\"], axis = 1, inplace = True)\n",
    "X_test.drop([\"day\", \"timestamp\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d158720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869da71",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üóÉ Window timeseries dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b6ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "# https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "               df_train, val_df, test_df,\n",
    "               label_columns=None, batch_size=32):\n",
    "        # Store the raw data.\n",
    "        self.df_train = df_train\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "          self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(df_train.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "      inputs = features[:, self.input_slice, :]\n",
    "      labels = features[:, self.labels_slice, :]\n",
    "      if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "      # Slicing doesn't preserve static shape information, so set the shapes\n",
    "      # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "      inputs.set_shape([None, self.input_width, None])\n",
    "      labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "      return inputs, labels\n",
    "\n",
    "    def plot(self, plot_col, model=None, max_subplots=3):\n",
    "      inputs, labels = self.example\n",
    "      plt.figure(figsize=(12, 8))\n",
    "      plot_col_index = self.column_indices[plot_col]\n",
    "      max_n = min(max_subplots, len(inputs))\n",
    "      for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "        if self.label_columns:\n",
    "          label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "        else:\n",
    "          label_col_index = plot_col_index\n",
    "\n",
    "        if label_col_index is None:\n",
    "          continue\n",
    "\n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                      marker='X', edgecolors='k', label='Predictions',\n",
    "                      c='#ff7f0e', s=64)\n",
    "\n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "\n",
    "      plt.xlabel('Time [h]')\n",
    "\n",
    "    #make_dataset method will take a time series DataFrame and convert it to a tf.data.Dataset of (input_window, label_window) \n",
    "    # pairs using the tf.keras.utils.timeseries_dataset_from_array function:\n",
    "    def make_dataset(self, data):\n",
    "      data = np.array(data, dtype=np.float32)\n",
    "      ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "          data=data,\n",
    "          targets=None,\n",
    "          sequence_length=self.total_window_size,\n",
    "          sequence_stride=1,\n",
    "          shuffle=False,\n",
    "          batch_size=self.batch_size,)    \n",
    "      ds = ds.map(self.split_window)\n",
    "      ds = ds.repeat(1000)\n",
    "      ds = ds.prefetch(10)  \n",
    "      return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "      return self.make_dataset(self.df_train)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "      return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "      return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "      \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "      result = getattr(self, '_example', None)\n",
    "      if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.test))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step_window = WindowGenerator(df_train=X_train, val_df=X_val, test_df=X_test, input_width=4, label_width=4, shift=1, label_columns=[\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"])\n",
    "n_step_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = n_step_window.example\n",
    "print(inputs.shape)\n",
    "print(labels.shape)\n",
    "print(n_step_window.label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_inputs, example_labels in n_step_window.train.take(1):\n",
    "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3210c",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters = 64, kernel_size=1, padding='same', kernel_initializer=\"uniform\", input_shape=(input_dim[0], input_dim[1])))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))        \n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size= 1,padding='same',  kernel_initializer=\"uniform\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))       \n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same',  kernel_initializer=\"uniform\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))   \n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=1, padding='same'))       \n",
    "    \n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16, return_sequences=True))) \n",
    "    model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "    model.add(tf.keras.layers.Dense(units=4))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model([4, 29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "history = model.fit(n_step_window.train,\n",
    "                    epochs=50,\n",
    "                    verbose=0,\n",
    "                    steps_per_epoch=200,\n",
    "                    validation_data=n_step_window.train,\n",
    "                    validation_steps=1,                    \n",
    "                   )\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dade6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = n_step_window.example\n",
    "prediction_test = model.predict(inputs)\n",
    "print(prediction_test.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c50149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "loss_values50 = loss_values\n",
    "val_loss_values50 = val_loss_values\n",
    "epochs = range(1, len(loss_values50) + 1)\n",
    "plt.plot(epochs, loss_values50, 'b',color = 'blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values50, 'b',color='red', label='Validation loss')\n",
    "plt.rc('font', size = 18)\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(epochs)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step_window.plot(plot_col=\"price_se4\", max_subplots=3, model=model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "se1_actual = []\n",
    "se2_actual = []\n",
    "se3_actual = []\n",
    "se4_actual = []\n",
    "\n",
    "inputs, labels = n_step_window.example\n",
    "for batch_n in range(len(labels)):\n",
    "    batch = labels[batch_n]\n",
    "    for window_n in range(4):\n",
    "        se1_actual.append(batch[window_n][0].numpy())\n",
    "        se2_actual.append(batch[window_n][1].numpy())\n",
    "        se3_actual.append(batch[window_n][2].numpy())\n",
    "        se4_actual.append(batch[window_n][3].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53833583",
   "metadata": {},
   "outputs": [],
   "source": [
    "se1_pred = []\n",
    "se2_pred = []\n",
    "se3_pred = []\n",
    "se4_pred = []\n",
    "\n",
    "prediction_test = model.predict(inputs)\n",
    "for batch_n in range(len(prediction_test)):\n",
    "    batch = prediction_test[batch_n]\n",
    "    for window_n in range(4):\n",
    "        se1_pred.append(batch[window_n][0])\n",
    "        se2_pred.append(batch[window_n][1])\n",
    "        se3_pred.append(batch[window_n][2])\n",
    "        se4_pred.append(batch[window_n][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ff2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "se3_actual[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed41b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(se1_pred,color='red', label='test SE1 price prediction')\n",
    "plt.plot(se1_actual, color='blue', label='test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485afbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(se2_pred,color='red', label='test SE2 price prediction')\n",
    "plt.plot(se2_actual, color='blue', label='test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(se3_pred,color='red', label='test SE3 price prediction')\n",
    "plt.plot(se3_actual, color='blue', label='test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(se4_pred,color='red', label='test SE4 price prediction')\n",
    "plt.plot(se4_actual, color='blue', label='test actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (scaled)')\n",
    "plt.legend(loc='upper left')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62aba50",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style='color:#ff5f27'>üóÑ Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c146ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = \"electricity_price_model\"\n",
    "print('Exporting trained model to: {}'.format(export_path))\n",
    "\n",
    "tf.saved_model.save(model, export_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "metrics={'loss': history_dict['val_loss'][0]} \n",
    "\n",
    "mr_model = mr.tensorflow.create_model(\n",
    "    name=\"electricity_price_prediction_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Daily electricity price prediction model.\",\n",
    "    input_example=n_step_window.example[0].numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ac42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_model.save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8652650",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load recent batch data</span>\n",
    "\n",
    "First, you will need to fetch the January-February training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57552710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=5)) \n",
    "start_time = int(start_date.timestamp()) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00951c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\"electricity_feature_view\", 1)\n",
    "feature_view.init_batch_scoring(training_dataset_version=1)\n",
    "df = feature_view.get_batch_data(start_time=start_time)\n",
    "df.sort_values([\"timestamp\"], inplace=True)\n",
    "df = df.drop([\"day\", \"timestamp\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da7176",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">üöÄ Using the model to predict electricity prices</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b833e",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'>üóÑ Retrieving model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "model = mr.get_model(\"electricity_price_prediction_model\", version=1)\n",
    "model_dir = model.download()\n",
    "loaded_model = tf.saved_model.load(model_dir)\n",
    "serving_function = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c401a",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'>ü§ñ Making the predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = serving_function(tf.constant(df.values.reshape(-1, df.shape[0], 29), tf.float32))\n",
    "prediction = z[list(z.keys())[0]].numpy() #.flatten().tolist()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a799d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_se1_pred = []\n",
    "price_se2_pred = []\n",
    "price_se3_pred = []\n",
    "price_se4_pred = []\n",
    "\n",
    "for batch_n in range(len(prediction)):\n",
    "    batch = prediction[batch_n]\n",
    "    for window_n in range(df.shape[0]):\n",
    "        price_se1_pred.append(batch[window_n][0])\n",
    "        price_se2_pred.append(batch[window_n][1])\n",
    "        price_se3_pred.append(batch[window_n][2])\n",
    "        price_se4_pred.append(batch[window_n][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.init_serving(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be327df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td_transformation_functions = feature_view._single_vector_server._transformation_function\n",
    "\n",
    "td_transformation_functions = feature_view._batch_scoring_server._transformation_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding our features\n",
    "\n",
    "preds = pd.DataFrame(data={'price_se1': price_se1_pred,\n",
    "                           'price_se2': price_se2_pred,\n",
    "                           'price_se3': price_se3_pred,\n",
    "                           'price_se4': price_se4_pred,})\n",
    "import inspect \n",
    "\n",
    "res = {}\n",
    "for feature_name in td_transformation_functions:\n",
    "    if feature_name in [\"price_se1\", \"price_se2\", \"price_se3\", \"price_se4\"]:\n",
    "        td_transformation_function = td_transformation_functions[feature_name]\n",
    "        sig, foobar_locals = inspect.signature(td_transformation_function.transformation_fn), locals()\n",
    "        param_dict = dict([(param.name, param.default) for param in sig.parameters.values() if param.default != inspect._empty])\n",
    "        if td_transformation_function.name == \"min_max_scaler\":\n",
    "            preds[feature_name] = preds[feature_name].map(\n",
    "                lambda x: x * (param_dict[\"max_value\"] - param_dict[\"min_value\"]) + param_dict[\"min_value\"]\n",
    "                )\n",
    "            \n",
    "preds = preds.apply(lambda x: -x)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(preds[\"price_se1\"],color='red', label='SE1 price prediction')\n",
    "plt.plot(preds[\"price_se2\"], color='blue', label='SE2 price prediction')\n",
    "plt.plot(preds[\"price_se3\"], color='green', label='SE3 price prediction')\n",
    "plt.plot(preds[\"price_se4\"], color='black', label='SE4 price prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#save figure to PNG file\n",
    "plt.savefig('model_preds.png', dpi=300)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b150bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <span style=\"color:#ff5f27;\">ü•≥ <b> Next Steps  </b> </span>\n",
    "Congratulations you've now completed the Electricity price tutorial for Managed Hopsworks.\n",
    "\n",
    "Check out our other tutorials on ‚û° https://github.com/logicalclocks/hopsworks-tutorials\n",
    "\n",
    "Or documentation at ‚û° https://docs.hopsworks.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
