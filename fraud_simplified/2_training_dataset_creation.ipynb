{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "In this notebook, we will create the actual dataset that we will train our model on. In particular, we will:\n",
    "1. Select the features we want to train our model on.\n",
    "2. Specify how the features should be preprocessed.\n",
    "3. Create a dataset split for training and validation data.\n",
    "\n",
    "![tutorial-flow](images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature groups.\n",
    "trans_fg = fs.get_feature_group(\"transactions\")\n",
    "window_aggs_fg = fs.get_feature_group(\"transactions_4h_aggs\")\n",
    "\n",
    "# Select features for training data.\n",
    "ds_query = trans_fg.select([\"fraud_label\", \"category\", \"amount\", \"age_at_transaction\", \"days_until_card_expires\", \"loc_delta\"])\\\n",
    "    .join(window_aggs_fg.select_except([\"tid\"]), on=\"tid\")\\\n",
    "\n",
    "ds_query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we computed the features in `transactions_4h_aggs` using 4-hour aggregates. If we had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join we would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"category\": label_encoder,\n",
    "    \"amount\": min_max_scaler,\n",
    "    \"trans_volume_mavg\": min_max_scaler,\n",
    "    \"trans_volume_mstd\": min_max_scaler,\n",
    "    \"trans_freq\": min_max_scaler,\n",
    "    \"loc_delta\": min_max_scaler,\n",
    "    \"loc_delta_mavg\": min_max_scaler,\n",
    "    \"age_at_transaction\": min_max_scaler,\n",
    "    \"days_until_card_expires\": min_max_scaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature View Creation\n",
    "\n",
    "In order to create a Feature View we may use `fs.create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='transactions_view',\n",
    "    query=ds_query,\n",
    "    label=[\"fraud_label\"],\n",
    "    transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.get_batch_data().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation\n",
    "\n",
    "Finally we create the dataset using `fs.create_training_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = feature_view.create_training_dataset(\n",
    "    description = 'transactions_dataset_splitted',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO add chronological split here.\n",
    "# td = fs.create_training_dataset(\n",
    "#     name=\"transactions_dataset_splitted\",\n",
    "#     label=[\"fraud_label\"],\n",
    "#     data_format=\"csv\",\n",
    "#     transformation_functions=transformation_functions,\n",
    "#     splits={'train': 70, 'validation': 30},\n",
    "#     train_split=\"train\"\n",
    "# )\n",
    "\n",
    "# # We can save the dataset using the query alone.\n",
    "# td.save(ds_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check that the transformation functions have been applied by loading the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td.read(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td.read(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}