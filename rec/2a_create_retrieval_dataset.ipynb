{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Retrieval Dataset\n",
    "\n",
    "In this notebook, we'll create a dataset for our retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell and fill in details if you are running external Python\n",
    "import os\n",
    "key=\"\"\n",
    "with open(\"api-key.txt\", \"r\") as f:\n",
    "    key = f.read().rstrip()\n",
    "os.environ['HOPSWORKS_PROJECT']=\"hm\"\n",
    "os.environ['HOPSWORKS_HOST']=\"35.240.81.237\"\n",
    "os.environ['HOPSWORKS_API_KEY']=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://35.240.81.237:443/p/119\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "First, we'll load the feature groups we created in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_feature_group(\"transactions\",version=1)\n",
    "customers_fg = fs.get_feature_group(\"customers\",version=1)\n",
    "articles_fg = fs.get_feature_group(\"articles\",version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to join these three data sources to make the data compatible with out retrieval model. Recall that each row in the `transactions` feature group relates information about which customer bought which item. We'll join this feature group with the `customers` and `articles` feature groups to inject customer and item features into each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = trans_fg.select([\"customer_id\", \"article_id\", \"month_sin\", \"month_cos\"])\\\n",
    "    .join(customers_fg.select([\"age\"]), on=\"customer_id\")\\\n",
    "    .join(articles_fg.select([\"garment_group_name\", \"index_group_name\"]), on=\"article_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature View Creation\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://35.240.81.237:443/p/119/fs/67/fv/retrieval/version/1\n"
     ]
    }
   ],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name='retrieval',\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and explore data in the feature view we can retrieve batch data using the `get_batch_data()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset Creation\n",
    "\n",
    "Finally, we can create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on FeatureView in module hsfs.feature_view object:\n",
      "\n",
      "class FeatureView(builtins.object)\n",
      " |  FeatureView(name: str, query, featurestore_id, id=None, version: Optional[int] = None, description: Optional[str] = '', labels: Optional[List[str]] = [], transformation_functions: Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] = {})\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name: str, query, featurestore_id, id=None, version: Optional[int] = None, description: Optional[str] = '', labels: Optional[List[str]] = [], transformation_functions: Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] = {})\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_tag(self, name: str, value)\n",
      " |  \n",
      " |  add_training_dataset_tag(self, training_dataset_version: int, name: str, value)\n",
      " |  \n",
      " |  create_train_test_split(self, test_size: Optional[float] = None, train_start: Optional[str] = '', train_end: Optional[str] = '', test_start: Optional[str] = '', test_end: Optional[str] = '', storage_connector: Optional[hsfs.storage_connector.StorageConnector] = None, location: Optional[str] = '', description: Optional[str] = '', data_format: Optional[str] = 'csv', coalesce: Optional[bool] = False, seed: Optional[int] = None, statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, write_options: Optional[Dict[Any, Any]] = {})\n",
      " |      Create a training dataset and save data into `location`.\n",
      " |      \n",
      " |      !!! info \"Data Formats\"\n",
      " |          The feature store currently supports the following data formats for\n",
      " |          training datasets:\n",
      " |      \n",
      " |          1. tfrecord\n",
      " |          2. csv\n",
      " |          3. tsv\n",
      " |          4. parquet\n",
      " |          5. avro\n",
      " |          6. orc\n",
      " |      \n",
      " |          Currently not supported petastorm, hdf5 and npy file formats.\n",
      " |      \n",
      " |      \n",
      " |      # Arguments\n",
      " |          test_size: size of test set.\n",
      " |          train_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          train_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          test_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          test_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          storage_connector: Storage connector defining the sink location for the\n",
      " |              training dataset, defaults to `None`, and materializes training dataset\n",
      " |              on HopsFS.\n",
      " |          location: Path to complement the sink storage connector with, e.g if the\n",
      " |              storage connector points to an S3 bucket, this path can be used to\n",
      " |              define a sub-directory inside the bucket to place the training dataset.\n",
      " |              Defaults to `\"\"`, saving the training dataset at the root defined by the\n",
      " |              storage connector.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          data_format: The data format used to save the training dataset,\n",
      " |              defaults to `\"csv\"`-format.\n",
      " |          coalesce: If true the training dataset data will be coalesced into\n",
      " |              a single partition before writing. The resulting training dataset\n",
      " |              will be a single file per split. Default False.\n",
      " |          seed: Optionally, define a seed to create the random splits with, in order\n",
      " |              to guarantee reproducability, defaults to `None`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          write_options: Additional write options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, write_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |              * key `wait_for_job` and value `True` or `False` to configure\n",
      " |                whether or not to the save call should return only\n",
      " |                after the Hopsworks Job has finished. By default it waits.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (td_version, `Job`): Tuple of training dataset version and job.\n",
      " |              When using the `python` engine, it returns the Hopsworks Job\n",
      " |              that was launched to create the training dataset.\n",
      " |  \n",
      " |  create_train_validation_test_splits(self, val_size: Optional[float] = None, test_size: Optional[float] = None, train_start: Optional[str] = '', train_end: Optional[str] = '', val_start: Optional[str] = '', val_end: Optional[str] = '', test_start: Optional[str] = '', test_end: Optional[str] = '', storage_connector: Optional[hsfs.storage_connector.StorageConnector] = None, location: Optional[str] = '', description: Optional[str] = '', data_format: Optional[str] = 'csv', coalesce: Optional[bool] = False, seed: Optional[int] = None, statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, write_options: Optional[Dict[Any, Any]] = {})\n",
      " |      Create a training dataset and save data into `location`.\n",
      " |      \n",
      " |      !!! info \"Data Formats\"\n",
      " |          The feature store currently supports the following data formats for\n",
      " |          training datasets:\n",
      " |      \n",
      " |          1. tfrecord\n",
      " |          2. csv\n",
      " |          3. tsv\n",
      " |          4. parquet\n",
      " |          5. avro\n",
      " |          6. orc\n",
      " |      \n",
      " |          Currently not supported petastorm, hdf5 and npy file formats.\n",
      " |      \n",
      " |      \n",
      " |      # Arguments\n",
      " |          val_size: size of validation set.\n",
      " |          test_size: size of test set.\n",
      " |          train_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          train_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          val_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          val_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          test_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          test_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          storage_connector: Storage connector defining the sink location for the\n",
      " |              training dataset, defaults to `None`, and materializes training dataset\n",
      " |              on HopsFS.\n",
      " |          location: Path to complement the sink storage connector with, e.g if the\n",
      " |              storage connector points to an S3 bucket, this path can be used to\n",
      " |              define a sub-directory inside the bucket to place the training dataset.\n",
      " |              Defaults to `\"\"`, saving the training dataset at the root defined by the\n",
      " |              storage connector.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          data_format: The data format used to save the training dataset,\n",
      " |              defaults to `\"csv\"`-format.\n",
      " |          coalesce: If true the training dataset data will be coalesced into\n",
      " |              a single partition before writing. The resulting training dataset\n",
      " |              will be a single file per split. Default False.\n",
      " |          seed: Optionally, define a seed to create the random splits with, in order\n",
      " |              to guarantee reproducability, defaults to `None`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          write_options: Additional write options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, write_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |              * key `wait_for_job` and value `True` or `False` to configure\n",
      " |                whether or not to the save call should return only\n",
      " |                after the Hopsworks Job has finished. By default it waits.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (td_version, `Job`): Tuple of training dataset version and job.\n",
      " |              When using the `python` engine, it returns the Hopsworks Job\n",
      " |              that was launched to create the training dataset.\n",
      " |  \n",
      " |  create_training_data(self, start_time: Optional[str] = '', end_time: Optional[str] = '', storage_connector: Optional[hsfs.storage_connector.StorageConnector] = None, location: Optional[str] = '', description: Optional[str] = '', data_format: Optional[str] = 'csv', coalesce: Optional[bool] = False, seed: Optional[int] = None, statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, write_options: Optional[Dict[Any, Any]] = {})\n",
      " |      Create a training dataset and save data into `location`.\n",
      " |      \n",
      " |      !!! info \"Data Formats\"\n",
      " |          The feature store currently supports the following data formats for\n",
      " |          training datasets:\n",
      " |      \n",
      " |          1. tfrecord\n",
      " |          2. csv\n",
      " |          3. tsv\n",
      " |          4. parquet\n",
      " |          5. avro\n",
      " |          6. orc\n",
      " |      \n",
      " |          Currently not supported petastorm, hdf5 and npy file formats.\n",
      " |      \n",
      " |      \n",
      " |      # Arguments\n",
      " |          start_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          end_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          storage_connector: Storage connector defining the sink location for the\n",
      " |              training dataset, defaults to `None`, and materializes training dataset\n",
      " |              on HopsFS.\n",
      " |          location: Path to complement the sink storage connector with, e.g if the\n",
      " |              storage connector points to an S3 bucket, this path can be used to\n",
      " |              define a sub-directory inside the bucket to place the training dataset.\n",
      " |              Defaults to `\"\"`, saving the training dataset at the root defined by the\n",
      " |              storage connector.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          data_format: The data format used to save the training dataset,\n",
      " |              defaults to `\"csv\"`-format.\n",
      " |          coalesce: If true the training dataset data will be coalesced into\n",
      " |              a single partition before writing. The resulting training dataset\n",
      " |              will be a single file per split. Default False.\n",
      " |          seed: Optionally, define a seed to create the random splits with, in order\n",
      " |              to guarantee reproducability, defaults to `None`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          write_options: Additional write options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, write_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |              * key `wait_for_job` and value `True` or `False` to configure\n",
      " |                whether or not to the save call should return only\n",
      " |                after the Hopsworks Job has finished. By default it waits.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (td_version, `Job`): Tuple of training dataset version and job.\n",
      " |              When using the `python` engine, it returns the Hopsworks Job\n",
      " |              that was launched to create the training dataset.\n",
      " |  \n",
      " |  delete(self)\n",
      " |      Delete current feature view and all associated metadata.\n",
      " |      \n",
      " |      !!! danger \"Potentially dangerous operation\"\n",
      " |          This operation drops all metadata associated with **this version** of the\n",
      " |          feature view **and** related training dataset **and** materialized data in HopsFS.\n",
      " |      \n",
      " |      # Raises\n",
      " |          `RestAPIError`.\n",
      " |  \n",
      " |  delete_all_training_datasets(self)\n",
      " |  \n",
      " |  delete_tag(self, name: str)\n",
      " |  \n",
      " |  delete_training_dataset(self, version: int)\n",
      " |  \n",
      " |  delete_training_dataset_tag(self, training_dataset_version: int, name: str)\n",
      " |  \n",
      " |  get_batch_data(self, start_time=None, end_time=None, read_options=None)\n",
      " |      start_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |          following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |      end_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |          following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |      read_options: User provided read options. Defaults to `{}`.\n",
      " |  \n",
      " |  get_batch_query(self, start_time: Optional[datetime.datetime] = None, end_time: Optional[datetime.datetime] = None)\n",
      " |      Get a query string of batch query.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          start_time: Optional. Start time of the batch query.\n",
      " |          end_time: Optional. End time of the batch query.\n",
      " |      \n",
      " |      # Returns\n",
      " |          `str`: batch query\n",
      " |  \n",
      " |  get_feature_vector(self, entry: List[Dict[str, Any]], passed_features: Optional[Dict[str, Any]] = {}, external: Optional[bool] = None)\n",
      " |      Returns assembled serving vector from online feature store.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          entry: dictionary of feature group primary key and values provided by serving application.\n",
      " |          passed_features: dictionary of feature values provided by the application at runtime.\n",
      " |              They can replace features values fetched from the feature store as well as\n",
      " |              providing feature values which are not available in the feature store.\n",
      " |          external: boolean, optional. If set to True, the connection to the\n",
      " |              online feature store is established using the same host as\n",
      " |              for the `host` parameter in the [`hsfs.connection()`](project.md#connection) method.\n",
      " |              If set to False, the online feature store storage connector is used\n",
      " |              which relies on the private IP. Defaults to True if connection to Hopsworks is established from\n",
      " |              external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.\n",
      " |      # Returns\n",
      " |          `list` List of feature values related to provided primary keys, ordered according to positions of this\n",
      " |          features in the feature view query.\n",
      " |  \n",
      " |  get_feature_vectors(self, entry: List[Dict[str, Any]], passed_features: Optional[List[Dict[str, Any]]] = {}, external: Optional[bool] = None)\n",
      " |      Returns assembled serving vectors in batches from online feature store.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          entry: a list of dictionary of feature group primary key and values provided by serving application.\n",
      " |          passed_features: a list of dictionary of feature values provided by the application at runtime.\n",
      " |              They can replace features values fetched from the feature store as well as\n",
      " |              providing feature values which are not available in the feature store.\n",
      " |          external: boolean, optional. If set to True, the connection to the\n",
      " |              online feature store is established using the same host as\n",
      " |              for the `host` parameter in the [`hsfs.connection()`](project.md#connection) method.\n",
      " |              If set to False, the online feature store storage connector is used\n",
      " |              which relies on the private IP. Defaults to True if connection to Hopsworks is established from\n",
      " |              external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.\n",
      " |      # Returns\n",
      " |          `List[list]` List of lists of feature values related to provided primary keys, ordered according to positions of this features in the feature view query.\n",
      " |  \n",
      " |  get_tag(self, name: str)\n",
      " |  \n",
      " |  get_tags(self)\n",
      " |  \n",
      " |  get_train_test_split(self, training_dataset_version, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from storage or feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          version: training dataset version\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X_train, y_train, X_test, y_test):\n",
      " |              Tuple of dataframe of features and labels\n",
      " |  \n",
      " |  get_train_validation_test_splits(self, training_dataset_version, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from storage or feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          version: training dataset version\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X_train, y_train, X_val, y_val, X_test, y_test):\n",
      " |              Tuple of dataframe of features and labels\n",
      " |  \n",
      " |  get_training_data(self, training_dataset_version, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from storage or feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          version: training dataset version\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X, y): Tuple of dataframe of features and labels\n",
      " |  \n",
      " |  get_training_dataset_tag(self, training_dataset_version: int, name: str)\n",
      " |  \n",
      " |  get_training_dataset_tags(self, training_dataset_version: int)\n",
      " |  \n",
      " |  init_batch_scoring(self, training_dataset_version: Optional[int] = None)\n",
      " |      Initialise and cache parametrized transformation functions.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          training_dataset_version: int, optional. Default to be 1. Transformation statistics\n",
      " |              are fetched from training dataset and apply in serving vector.\n",
      " |  \n",
      " |  init_serving(self, training_dataset_version: Optional[int] = None, external: Optional[bool] = None)\n",
      " |      Initialise and cache parametrized prepared statement to\n",
      " |         retrieve feature vector from online feature store.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          training_dataset_version: int, optional. Default to be 1. Transformation statistics\n",
      " |              are fetched from training dataset and apply in serving vector.\n",
      " |          batch: boolean, optional. If set to True, prepared statements will be\n",
      " |              initialised for retrieving serving vectors as a batch.\n",
      " |          external: boolean, optional. If set to True, the connection to the\n",
      " |              online feature store is established using the same host as\n",
      " |              for the `host` parameter in the [`hsfs.connection()`](project.md#connection) method.\n",
      " |              If set to False, the online feature store storage connector is used which relies on the private IP.\n",
      " |              Defaults to True if connection to Hopsworks is established from external environment (e.g AWS\n",
      " |              Sagemaker or Google Colab), otherwise to False.\n",
      " |  \n",
      " |  json(self)\n",
      " |  \n",
      " |  preview_feature_vector(self, external: Optional[bool] = None)\n",
      " |      Returns a sample of assembled serving vector from online feature store.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          external: boolean, optional. If set to True, the connection to the\n",
      " |              online feature store is established using the same host as\n",
      " |              for the `host` parameter in the [`hsfs.connection()`](project.md#connection) method.\n",
      " |              If set to False, the online feature store storage connector is used\n",
      " |              which relies on the private IP. Defaults to True if connection to Hopsworks is established from\n",
      " |              external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.\n",
      " |      # Returns\n",
      " |          `list` List of feature values, ordered according to positions of this\n",
      " |          features in training dataset query.\n",
      " |  \n",
      " |  preview_feature_vectors(self, n: int, external: Optional[bool] = None)\n",
      " |      Returns n samples of assembled serving vectors in batches from online feature store.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          n: int. Number of feature vectors to return.\n",
      " |          external: boolean, optional. If set to True, the connection to the\n",
      " |              online feature store is established using the same host as\n",
      " |              for the `host` parameter in the [`hsfs.connection()`](project.md#connection) method.\n",
      " |              If set to False, the online feature store storage connector is used\n",
      " |              which relies on the private IP. Defaults to True if connection to Hopsworks is established from\n",
      " |              external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.\n",
      " |      # Returns\n",
      " |          `List[list]` List of lists of feature values , ordered according to\n",
      " |          positions of this features in training dataset query.\n",
      " |  \n",
      " |  purge_all_training_data(self)\n",
      " |  \n",
      " |  purge_training_data(self, version: int)\n",
      " |  \n",
      " |  recreate_training_dataset(self, version: int, write_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Recreate a training dataset.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          version: training dataset version\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          `Job`: When using the `python` engine, it returns the Hopsworks Job\n",
      " |              that was launched to create the training dataset.\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |  \n",
      " |  train_test_split(self, test_size: Optional[float] = None, train_start: Optional[str] = '', train_end: Optional[str] = '', test_start: Optional[str] = '', test_end: Optional[str] = '', description: Optional[str] = '', statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          test_size: size of test set. Should be between 0 and 1.\n",
      " |          train_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          train_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          test_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          test_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X_train, y_train, X_test, y_test):\n",
      " |              Tuple of dataframe of features and labels\n",
      " |  \n",
      " |  train_validation_test_splits(self, val_size: Optional[float] = None, test_size: Optional[float] = None, train_start: Optional[str] = '', train_end: Optional[str] = '', val_start: Optional[str] = '', val_end: Optional[str] = '', test_start: Optional[str] = '', test_end: Optional[str] = '', description: Optional[str] = '', statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          val_size: size of validation set. Should be between 0 and 1.\n",
      " |          test_size: size of test set. Should be between 0 and 1.\n",
      " |          train_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          train_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          val_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          val_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          test_start: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          test_end: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X_train, y_train, X_val, y_val, X_test, y_test):\n",
      " |              Tuple of dataframe of features and labels\n",
      " |  \n",
      " |  training_data(self, start_time: Optional = None, end_time: Optional = None, description: Optional[str] = '', statistics_config: Union[hsfs.statistics_config.StatisticsConfig, bool, dict, NoneType] = None, read_options: Optional[Dict[Any, Any]] = None)\n",
      " |      Get training data from feature groups.\n",
      " |      \n",
      " |      !!! info\n",
      " |      If a materialised training data has deleted. Use `recreate_training_dataset()` to\n",
      " |      recreate the training data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          start_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`, or `%Y%m%d%H%M%S%f`.\n",
      " |          end_time: timestamp in second or wallclock_time: Datetime string. The String should be formatted in one of the\n",
      " |              following formats `%Y%m%d`, `%Y%m%d%H`, `%Y%m%d%H%M`, `%Y%m%d%H%M%S`,  or `%Y%m%d%H%M%S%f`.\n",
      " |          description: A string describing the contents of the training dataset to\n",
      " |              improve discoverability for Data Scientists, defaults to empty string\n",
      " |              `\"\"`.\n",
      " |          statistics_config: A configuration object, or a dictionary with keys\n",
      " |              \"`enabled`\" to generally enable descriptive statistics computation for\n",
      " |              this feature group, `\"correlations`\" to turn on feature correlation\n",
      " |              computation and `\"histograms\"` to compute feature value frequencies. The\n",
      " |              values should be booleans indicating the setting. To fully turn off\n",
      " |              statistics computation pass `statistics_config=False`. Defaults to\n",
      " |              `None` and will compute only descriptive statistics.\n",
      " |          read_options: Additional read options as key-value pairs, defaults to `{}`.\n",
      " |              When using the `python` engine, read_options can contain the\n",
      " |              following entries:\n",
      " |              * key `spark` and value an object of type\n",
      " |              [hsfs.core.job_configuration.JobConfiguration](../job_configuration)\n",
      " |                to configure the Hopsworks Job used to compute the training dataset.\n",
      " |      \n",
      " |      # Returns\n",
      " |          (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns `None`.\n",
      " |  \n",
      " |  update(self)\n",
      " |  \n",
      " |  update_from_response_json(self, json_dict)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_response_json(json_dict) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  description\n",
      " |  \n",
      " |  featurestore_id\n",
      " |      Feature store id.\n",
      " |  \n",
      " |  id\n",
      " |      Feature view id.\n",
      " |  \n",
      " |  labels\n",
      " |      The labels/prediction feature of the feature view.\n",
      " |      \n",
      " |      Can be a composite of multiple features.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the feature view.\n",
      " |  \n",
      " |  query\n",
      " |  \n",
      " |  schema\n",
      " |      Feature view schema.\n",
      " |  \n",
      " |  transformation_functions\n",
      " |      Set transformation functions.\n",
      " |  \n",
      " |  version\n",
      " |      Version number of the feature view.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ENTITY_TYPE = 'featureview'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_view = fs.get_feature_view(\"retrieval\", version=1)\n",
    "help(feature_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://35.240.81.237/p/119/jobs/named/retrieval_1_2_create_fv_td_28062022103918/executions\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "The Hopsworks Job failed, use the Hopsworks UI to access the job logs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_251702/3526838152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m td_version, td_job = feature_view.create_train_validation_test_splits(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'retrieval_dataset_split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mcreate_train_validation_test_splits\u001b[0;34m(self, val_size, test_size, train_start, train_end, val_start, val_end, test_start, test_end, storage_connector, location, description, data_format, coalesce, seed, statistics_config, write_options)\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# td_job is used only if the python engine is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         td, td_job = self._feature_view_engine.create_training_dataset(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mcreate_training_dataset\u001b[0;34m(self, feature_view_obj, training_dataset_obj, user_write_options)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n\u001b[0;32m--> 144\u001b[0;31m         td_job = self.compute_training_dataset(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0muser_write_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mcompute_training_dataset\u001b[0;34m(self, feature_view_obj, user_write_options, training_dataset_obj, training_dataset_version)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mwith_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n\u001b[0;32m--> 286\u001b[0;31m         td_job = engine.get_instance().write_training_dataset(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mbatch_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mwrite_training_dataset\u001b[0;34m(self, training_dataset, dataset, user_write_options, save_mode, feature_view_obj, to_df)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# If the user passed the wait_for_job option consider it,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# otherwise use the default True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_write_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtd_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36m_wait_for_job\u001b[0;34m(self, job, user_write_options)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 raise exceptions.FeatureStoreException(\n\u001b[0m\u001b[1;32m    705\u001b[0m                     \u001b[0;34m\"The Hopsworks Job failed, use the Hopsworks UI to access the job logs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 )\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: The Hopsworks Job failed, use the Hopsworks UI to access the job logs"
     ]
    }
   ],
   "source": [
    "\n",
    "td_version, td_job = feature_view.create_train_validation_test_splits(\n",
    "    description = 'retrieval_dataset_split',\n",
    "    data_format = 'csv',\n",
    "    val_size = 0.1,\n",
    "    test_size = 0.1,\n",
    "    write_options = {'wait_for_job': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
