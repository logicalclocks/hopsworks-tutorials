{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Model training</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This notebook explains how to read from a feature group and create training dataset within the feature store. You will train a model on the created training dataset. You will train your model using TensorFlow, although it could just as well be trained with other machine learning frameworks such as Scikit-learn, Keras, and PyTorch. You will also see some of the exploration that can be done in Hopsworks, notably the search functions and the lineage.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided into the following steps:** \n",
    "\n",
    "1. **Feature Selection**: Select the features you want to train your model on.\n",
    "2. **Feature Transformation**: How the features should be preprocessed.\n",
    "3. **Training Dataset Creation**: Create a dataset for training anomaly detection model.\n",
    "2. **Model Training**: Train your anomaly detection model.\n",
    "3. **Model Registry**: Register model to Hopsworks model registry.\n",
    "4. **Model Deployment**: Deploy the model for real-time inference.\n",
    "\n",
    "![tutorial-flow](../../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from anomaly_detection import GanEncAnomalyDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Feature Groups\n",
    "transactions_monthly_fg = fs.get_feature_group(\n",
    "    name=\"transactions_monthly\", \n",
    "    version=1,\n",
    ")\n",
    "\n",
    "graph_embeddings_fg = fs.get_feature_group(\n",
    "    name=\"graph_embeddings\", \n",
    "    version=1,\n",
    ") \n",
    "\n",
    "party_fg = fs.get_feature_group(\n",
    "    name=\"party_labels\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = transactions_monthly_fg.select(\n",
    "    [\n",
    "        \"monthly_in_count\", \n",
    "        \"monthly_in_total_amount\", \n",
    "        \"monthly_in_mean_amount\", \n",
    "        \"monthly_in_std_amount\", \n",
    "        \"monthly_out_count\", \n",
    "        \"monthly_out_total_amount\", \n",
    "        \"monthly_out_mean_amount\", \n",
    "        \"monthly_out_std_amount\",\n",
    "    ]\n",
    ").join(\n",
    "    graph_embeddings_fg.select([\"graph_embeddings\"]),\n",
    ").join(\n",
    "    party_fg.select([\"type\", \"is_sar\"]), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions </span>\n",
    "\n",
    "Transformation functions are a mathematical mapping of input data that may be stateful - requiring statistics from the partent feature view (such as number of instances of a category, or mean value of a numerical feature)\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load built in transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = [\n",
    "    min_max_scaler(\"monthly_in_count\"),\n",
    "    min_max_scaler(\"monthly_in_total_amount\"),\n",
    "    min_max_scaler(\"monthly_in_mean_amount\"),\n",
    "    min_max_scaler(\"monthly_in_std_amount\"),\n",
    "    min_max_scaler(\"monthly_out_count\"),\n",
    "    min_max_scaler(\"monthly_out_total_amount\"),\n",
    "    min_max_scaler(\"monthly_out_mean_amount\"),\n",
    "    min_max_scaler(\"monthly_out_std_amount\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models. The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View we may use `fs.create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'aml_feature_view' feature view\n",
    "feature_view = fs.create_feature_view(\n",
    "    name='aml_feature_view',\n",
    "    query=selected_features,\n",
    "    labels=[\"is_sar\"],\n",
    "    transformation_functions=transformation_functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `feature_view.training_data()` method.\n",
    "\n",
    "**From feature view APIs we can also create training datasts based on even time filters specifing `start_time` and `end_time`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "X_train, y_train = feature_view.training_data(\n",
    "    description='AML training dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first three rows of the training data\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first three rows of the target data\n",
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üëì Exploration </span>\n",
    "\n",
    "### Similar to Feature Groups Feature Views and Training Tatasets are now accessible and searchable in the UI\n",
    "![fv-overview](images/feature_views_explore.gif)\n",
    "\n",
    "## üìä Statistics\n",
    "We can explore feature statistics in the feature views. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fv-stats](images/feature_view_stats.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff5f27;\">ü§ñ Model Building</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string representations of Python literals in 'graph_embeddings' column to actual objects\n",
    "X_train['graph_embeddings'] = X_train['graph_embeddings'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each element in the 'graph_embeddings' column to a NumPy array\n",
    "X_train['graph_embeddings'] = X_train['graph_embeddings'].apply(np.array)\n",
    "\n",
    "# Merge the original DataFrame with a DataFrame of exploded embeddings\n",
    "X_train = X_train.merge(\n",
    "    pd.DataFrame(X_train['graph_embeddings'].to_list()).add_prefix('emb_'), \n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").drop('graph_embeddings', axis=1)\n",
    "\n",
    "# Display the first three rows of the modified DataFrame\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). During training step  you will provide only features of accounts that have never been reported for suspicios activity.  You will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-suspicious transactions from X_train based on y_train values equal to 0\n",
    "non_sar_transactions = X_train[y_train.values == 0]\n",
    "\n",
    "# Drop any rows with missing values from the non-suspicious transactions DataFrame\n",
    "non_sar_transactions = non_sar_transactions.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define Tensorflow Dataset as we are going to train keras tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    # Create a windowed dataset using the specified window_size and shift of 1\n",
    "    # Drop any remaining elements that do not fit in complete windows\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the nested datasets into a single dataset of windows\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "\n",
    "    # Batch the windows into batches of the specified batch_size\n",
    "    # Use drop_remainder=True to ensure that all batches have the same size\n",
    "    # Prefetch one batch to improve performance\n",
    "    return ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non_sar_transactions to a TensorFlow dataset, casting the values to float32\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(non_sar_transactions.astype('float32'), tf.float32)\n",
    ")\n",
    "\n",
    "# Use the windowed_dataset function to create a windowed dataset\n",
    "# Parameters: window_size=2 (sequence length), batch_size=16 (number of sequences in each batch)\n",
    "training_dataset = windowed_dataset(\n",
    "    training_dataset, \n",
    "    window_size=2, \n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Model architecture</span>\n",
    "\n",
    "Key components:\n",
    "\n",
    "- `Encoder`(encoder_model) takes input data and compresses it into a latent representation. The encoder consists of two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions.\n",
    "\n",
    "- `Generator`(generator_model) takes a latent vector and generates synthetic data. The generator consists of two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions. The last layer produces data with the same shape as the input data.\n",
    "\n",
    "- `Discriminator`(discriminator_model) distinguishes between real and generated (fake) data. It comprises two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions, followed by a fully connected layer. The output is a single value representing the probability that the input is real.\n",
    "\n",
    "![tutorial-flow](images/model_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GanEncAnomalyDetector model with input dimensions [2, n_features]\n",
    "model = GanEncAnomalyDetector([2, training_dataset.element_spec.shape[-1]])\n",
    "\n",
    "# Compile the model\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each layer in the model\n",
    "for layer in model.layers:\n",
    "    # Print the name and output shape of each layer\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training_dataset\n",
    "# Set the number of epochs to 2 and suppress verbose output during training\n",
    "history = model.fit(\n",
    "    training_dataset,  # Training dataset used for model training\n",
    "    epochs=2,          # Number of training epochs\n",
    "    verbose=0,         # Verbosity mode (0: silent, 1: progress bar, 2: one line per epoch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store metrics\n",
    "# The key is 'loss', and the value is the initial value of the generator loss from the training history\n",
    "metrics = {\n",
    "    'loss': history.history[\"g_loss\"][0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/3.0/user_guides/mlops/registry/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Define the input schema using the values of X_train\n",
    "input_schema = Schema(X_train)\n",
    "\n",
    "# Define the output schema using y_train\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "# Create a ModelSchema object specifying the input and output schemas\n",
    "model_schema = ModelSchema(\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema,\n",
    ")\n",
    "\n",
    "# Convert the model schema to a dictionary for further inspection or serialization\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for exporting the trained model\n",
    "export_path = \"aml_model\"\n",
    "print('Exporting trained model to: {}'.format(export_path))\n",
    "\n",
    "# Get the concrete function for serving the model\n",
    "call = model.serve_function.get_concrete_function(tf.TensorSpec([None, None, None], tf.float32))\n",
    "\n",
    "# Save the model to the specified export path with the serving signature\n",
    "tf.saved_model.save(\n",
    "    model, \n",
    "    export_path, \n",
    "    signatures=call,\n",
    ")\n",
    "\n",
    "# Access the model registry in your project\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Create a TensorFlow model in the model registry with specified metadata\n",
    "mr_model = mr.tensorflow.create_model(\n",
    "    name=\"aml_model\",                                    # Specify the model name\n",
    "    metrics=metrics,                                     # Include model metrics\n",
    "    model_schema=model_schema,                           # Include model schema\n",
    "    description=\"Adversarial anomaly detection model.\",  # Model description\n",
    "    input_example=[\"70408aef\"],                          # Input example\n",
    ")\n",
    "\n",
    "# Save the registered model to the model registry\n",
    "mr_model.save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üöÄ Model Deployment</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile aml_model_transformer.py\n",
    "\n",
    "import os\n",
    "import hsfs\n",
    "import numpy as np\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):        \n",
    "        # Get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # Get feature views\n",
    "        self.fv = self.fs.get_feature_view(\n",
    "            name=\"aml_feature_view\", \n",
    "            version=1,\n",
    "        )\n",
    "        \n",
    "        # Initialise serving\n",
    "        self.fv.init_serving(1)\n",
    "    \n",
    "    def preprocess(self, inputs):\n",
    "        # Retrieve feature vector using the feature vector provider\n",
    "        feature_vector = self.fv.get_feature_vector({\"id\": inputs[\"inputs\"][0]})\n",
    "\n",
    "        # Explode embeddings (flatten the list of embeddings)\n",
    "        feature_vector_exploded_emb = [*self.flat2gen(feature_vector)]\n",
    "\n",
    "        # Reshape feature vector to match the model's input shape\n",
    "        feature_vector_reshaped = np.array(feature_vector_exploded_emb).reshape(1, 41)\n",
    "\n",
    "        # Convert the feature vector to a TensorFlow constant\n",
    "        input_vector = tf.constant(feature_vector_reshaped, dtype=tf.float32)\n",
    "\n",
    "        # Add a time dimension (axis=1) to the input vector\n",
    "        input_vector = tf.expand_dims(input_vector, axis=1)\n",
    "\n",
    "        # Duplicate the input vector to create a pair\n",
    "        input_vector = tf.tile(input_vector, [1, 2, 1])\n",
    "\n",
    "        # Return the preprocessed input dictionary\n",
    "        return {\n",
    "            'inputs': input_vector\n",
    "        }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "    def flat2gen(self, alist):\n",
    "        for item in alist:\n",
    "            if isinstance(item, list):\n",
    "                for subitem in item: yield subitem\n",
    "            else:\n",
    "                yield item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "# Get the dataset API from the project\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "# Upload the transformer script file to the \"Models\" dataset\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"aml_model_transformer.py\",   # Name of the script file\n",
    "    \"Models\",                     # Destination folder in the dataset\n",
    "    overwrite=True,               # Overwrite the file if it already exists\n",
    ")\n",
    "\n",
    "# Construct the full path to the uploaded transformer script file\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")\n",
    "\n",
    "# Create a Transformer object using the uploaded script\n",
    "transformer_script = Transformer(\n",
    "    script_file=transformer_script_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"aml_model\" from the model registry\n",
    "model = mr.get_model(\n",
    "    name=\"aml_model\", \n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Deploy the model with the specified name (\"amlmodeldeployment\") and associated transformer\n",
    "deployment = model.deploy(\n",
    "    name=\"amlmodeldeployment\",      # Specify the deployment name\n",
    "    transformer=transformer_script, # Associate the transformer script with the deployment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.start(await_running=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For trouble shooting one can use `get_logs` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To stop the deployment you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è Next: Part 03: Online Inference </span>\n",
    "    \n",
    "In the next notebook you will use your deployment for online inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
