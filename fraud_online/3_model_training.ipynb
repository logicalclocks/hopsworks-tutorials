{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20fb65",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"../images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 5 main sections:** \n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Register model to Hopsworks model registry**.\n",
    "4. **Deploy the model on KServe behind Hopsworks for real-time inference requests**.\n",
    "5. **Test model deployment and use model serving rest APIs**.\n",
    "\n",
    "![tutorial-flow](../images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55aa324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\"transactions_fraud_online_fv\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc16b3",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load Training Data </span>\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook. We will use January - February data training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_jan_feb_x, train_jan_feb_y = feature_view.get_training_data(1)\n",
    "test_mar_x, test_mar_y = feature_view.get_training_data(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15c76e",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c84fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# fit the model\n",
    "clf = IsolationForest()\n",
    "clf.fit(train_jan_feb_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa94d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Predictions\n",
    "y_pred_train = clf.predict(train_jan_feb_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predictions\n",
    "y_pred_test = clf.predict(test_mar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if_cm=confusion_matrix(test_mar_y, y_pred_test)\n",
    "pd.DataFrame(if_cm)\n",
    "df_cm = pd.DataFrame(if_cm, ['step', 'True Normal',  'True Fraud'],['Pred Normal', 'step', 'Pred Fraud'])\n",
    "df_cm.drop(index=\"step\",inplace=True)\n",
    "df_cm.drop(\"step\", axis=1, inplace=True)\n",
    "\n",
    "pyplot.figure(figsize = (8,4))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_cm=confusion_matrix(train_jan_feb_y, y_pred_train)\n",
    "pd.DataFrame(if_cm)\n",
    "df_cm = pd.DataFrame(if_cm, ['step', 'True Normal',  'True Fraud'],['Pred Normal', 'step', 'Pred Fraud'])\n",
    "df_cm.drop(index=\"step\",inplace=True)\n",
    "df_cm.drop(\"step\", axis=1, inplace=True)\n",
    "\n",
    "pyplot.figure(figsize = (8,4))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Compute f1 score\n",
    "metrics = {\"fscore\": f1_score(test_mar_y, y_pred_test, average='micro')}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb49da8",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color:#ff5f27;\"> Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints.\n",
    "\n",
    "Let's connect to the model registry using the HSML library from Hopsworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19182d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c23e2f",
   "metadata": {},
   "source": [
    "The model needs to be set up with a Model Schema, which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be846b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(train_jan_feb_x)\n",
    "output_schema = Schema(train_jan_feb_y)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_credit_card = [4467360740682089]\n",
    "model = mr.sklearn.create_model(\n",
    "    name=\"transactions_fraud_online_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Isolation forest anomaly detection model\",\n",
    "    input_example = test_credit_card,\n",
    "    model_schema=model_schema\n",
    ")\n",
    "\n",
    "model.save('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6374ff1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> Deploy model</span>\n",
    "### About Model Serving\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster.\n",
    "\n",
    "### Create the Prediction File\n",
    "In order to deploy a model, you need to write a Python file containing the logic to return a prediction from the model. Don't worry, this is usually a matter of just modifying some paths in a template script. An example can be seen in the code block below, where we have taken this Scikit-learn template script and changed two paths (see comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_example.py\n",
    "import os\n",
    "import numpy as np\n",
    "import hsfs\n",
    "import joblib\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model\"\"\"        \n",
    "        # get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # get feature views\n",
    "        self.fv = self.fs.get_feature_view(\"transactions_fraud_online_fv\", 1)\n",
    "        \n",
    "        # initialise serving\n",
    "        self.fv.init_serving(1)\n",
    "\n",
    "        # load the trained model\n",
    "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/model.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(np.asarray(self.fv.get_feature_vector({\"cc_num\": inputs[0]})).reshape(1, -1)).tolist() # Numpy Arrays are not JSON serializable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bda80",
   "metadata": {},
   "source": [
    "If you wonder why we use the path Models/fraud_tutorial_model/1/model.pkl, it is useful to know that the Data Sets tab in the Hopsworks UI lets you browse among the different files in the project. Registered models will be found underneath the Models directory. Since we saved our model with the name fraud_tutorial_model, that's the directory we should look in. 1 is just the version of the model we want to deploy.\n",
    "\n",
    "This script needs to be put into a known location in the Hopsworks file system. Let's call the file predict_example.py and put it in the Models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\", overwrite=True)\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed095e57",
   "metadata": {},
   "source": [
    "## Create the deployment\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"transactions_fraud_online_model\", version=1)\n",
    "\n",
    "# Give it any name you want\n",
    "deployment = model.deploy(\n",
    "    name=\"fraudonlinemodeldeployment\", \n",
    "    model_server=\"PYTHON\",\n",
    "    serving_tool=\"KSERVE\",\n",
    "    script_file=predictor_script_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0026274",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff73c5",
   "metadata": {},
   "source": [
    "#### The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01a006",
   "metadata": {},
   "source": [
    "## Using the deployment\n",
    "Let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": model.input_example\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8176a34",
   "metadata": {},
   "source": [
    "### Retrieving our deployment and making a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = project.get_model_serving()\n",
    "fraud_detector = ms.get_deployment(\"fraudonlinemodeldeployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c79ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = fraud_detector.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903f395",
   "metadata": {},
   "source": [
    "## Stop Deployment\n",
    "To stop the deployment we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593479e",
   "metadata": {},
   "source": [
    "## üëæ StreamLit App\n",
    "\n",
    "\n",
    "If you want to see interactive dashboards - use a **StreamLit App**.\n",
    "\n",
    "Type the next commands in terminal to run a Streamlit App:\n",
    "\n",
    "`cd {%path_to_hopsworks_tutorials%}/online_fraid`\n",
    "\n",
    "`conda activate {%path_to_hopsworks_tutorials%}/miniconda/envs/hopsworks`\n",
    "\n",
    "`python -m streamlit run streamlit_app.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755636c4",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ  Wrapping things up </span>\n",
    "\n",
    "In this module we introduced stream feature group, performed with training data that we have created from feature view and depoyed model in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
