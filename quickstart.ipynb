{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quickstart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0F88iCqvzVm"
      },
      "outputs": [],
      "source": [
        "!pip install -U 'git+https://github.com/logicalclocks/feature-store-api@master#egg=hsfs[python]&subdirectory=python' --quiet\n",
        "!pip uninstall -y hopsworks\n",
        "!pip install -U 'git+https://github.com/logicalclocks/hopsworks-api@main#egg=hopsworks&subdirectory=python' --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hopsworks"
      ],
      "metadata": {
        "id": "Kiu01wZhwSUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project = hopsworks.login()"
      ],
      "metadata": {
        "id": "nsMZFplMw0MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üíΩ Loading the Data\n",
        "\n",
        "The data we will use comes from three different CSV files:\n",
        "\n",
        "    credit_cards.csv: credit card information such as expiration date and provider.\n",
        "    transactions.csv: transaction information such as timestamp, location, and the amount. Importantly, the binary fraud_label variable tells us whether a transaction was fraudulent or not.\n",
        "    profiles.csv: credit card user information such as birthdate and city of residence.\n",
        "\n",
        "We can conceptualize these CSV files as originating from separate data sources. All three files have a credit card number column cc_num in common, which we can use for joins.\n",
        "\n",
        "Let's go ahead and load the data.\n"
      ],
      "metadata": {
        "id": "NpwpPe1wxQ5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "credit_cards_df = pd.read_csv(\"https://repo.hops.works/dev/davit/card_fraud_data/credit_cards.csv\")\n",
        "credit_cards_df.head(3)\n",
        "\n",
        "profiles_df = pd.read_csv(\"https://repo.hops.works/dev/davit/card_fraud_data/profiles.csv\", parse_dates=[\"birthdate\"])\n",
        "profiles_df.head(3)\n",
        "\n",
        "trans_df = pd.read_csv(\"https://repo.hops.works/dev/davit/card_fraud_data/transactions.csv\", parse_dates=[\"datetime\"])\n",
        "trans_df.head(3)"
      ],
      "metadata": {
        "id": "ARrJ_Bp5xMIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üõ†Ô∏è Feature Engineering\n",
        "\n",
        "Fraudulent transactions can differ from regular ones in many different ways. Typical red flags would for instance be a large transaction volume/frequency in the span of a few hours. It could also be the case that elderly people in particular are targeted by fraudsters. To facilitate model learning we will create additional features based on these patterns. In particular, we will create two types of features:\n",
        "\n",
        "    Features that aggregate data from different data sources. This could for instance be the age of a customer at the time of a transaction, which combines the birthdate feature from profiles.csv with the datetime feature from transactions.csv.\n",
        "    Features that aggregate data from multiple time steps. An example of this could be the transaction frequency of a credit card in the span of a few hours, which is computed using a window function.\n",
        "\n",
        "Let's start with the first category.\n"
      ],
      "metadata": {
        "id": "HPq2qUtNxjaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute age at transaction.\n",
        "age_df = trans_df.merge(profiles_df, on=\"cc_num\", how=\"left\")\n",
        "trans_df[\"age_at_transaction\"] = (age_df[\"datetime\"] - age_df[\"birthdate\"]) / np.timedelta64(1, \"Y\")\n",
        "\n",
        "# Compute days until card expires.\n",
        "card_expiry_df = trans_df.merge(credit_cards_df, on=\"cc_num\", how=\"left\")\n",
        "card_expiry_df[\"expires\"] = pd.to_datetime(card_expiry_df[\"expires\"], format=\"%m/%y\")\n",
        "trans_df[\"days_until_card_expires\"] = (card_expiry_df[\"expires\"] - card_expiry_df[\"datetime\"]) / np.timedelta64(1, \"D\")\n",
        "\n",
        "trans_df[[\"age_at_transaction\", \"days_until_card_expires\"]].head()\n"
      ],
      "metadata": {
        "id": "ngEPnNzAxqsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Next, we create features that for each credit card aggregate data from multiple time steps.\n",
        "\n",
        "We start by computing the distance between consecutive transactions, which we will call loc_delta. Here we use the Haversine distance to quantify the distance between two longitude and latitude coordinates.\n"
      ],
      "metadata": {
        "id": "zEC12W4ux2Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import radians\n",
        "\n",
        "# Do some simple preprocessing.\n",
        "trans_df.sort_values(\"datetime\", inplace=True)\n",
        "trans_df[[\"longitude\", \"latitude\"]] = trans_df[[\"longitude\", \"latitude\"]].applymap(radians)\n",
        "\n",
        "def haversine(long, lat):\n",
        "    \"\"\"Compute Haversine distance between each consecutive coordinate in (long, lat).\"\"\"\n",
        "\n",
        "    long_shifted = long.shift()\n",
        "    lat_shifted = lat.shift()\n",
        "    long_diff = long_shifted - long\n",
        "    lat_diff = lat_shifted - lat\n",
        "\n",
        "    a = np.sin(lat_diff/2.0)**2\n",
        "    b = np.cos(lat) * np.cos(lat_shifted) * np.sin(long_diff/2.0)**2\n",
        "    c = 2*np.arcsin(np.sqrt(a + b))\n",
        "\n",
        "    return c\n",
        "\n",
        "\n",
        "trans_df[\"loc_delta\"] = trans_df.groupby(\"cc_num\")\\\n",
        "    .apply(lambda x : haversine(x[\"longitude\"], x[\"latitude\"]))\\\n",
        "    .reset_index(level=0, drop=True)\\\n",
        "    .fillna(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "rQ-g4ETOx4O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we compute windowed aggregates. Here we will use 4-hour windows, but feel free to experiment with different window lengths by setting window_len below to a value of your choice."
      ],
      "metadata": {
        "id": "a_MHfwYsGfbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_len = \"4h\"\n",
        "cc_group = trans_df[[\"cc_num\", \"amount\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\")\n",
        "\n",
        "# Moving average of transaction volume.\n",
        "df_4h_mavg = pd.DataFrame(cc_group.mean())\n",
        "df_4h_mavg.columns = [\"trans_volume_mavg\", \"datetime\"]\n",
        "df_4h_mavg = df_4h_mavg.reset_index(level=[\"cc_num\"])\n",
        "df_4h_mavg = df_4h_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
        "df_4h_mavg = df_4h_mavg.sort_index()\n",
        "\n",
        "# Moving standard deviation of transaction volume.\n",
        "df_4h_std = pd.DataFrame(cc_group.mean())\n",
        "df_4h_std.columns = [\"trans_volume_mstd\", \"datetime\"]\n",
        "df_4h_std = df_4h_std.reset_index(level=[\"cc_num\"])\n",
        "df_4h_std = df_4h_std.drop(columns=[\"cc_num\", \"datetime\"])\n",
        "df_4h_std = df_4h_std.fillna(0)\n",
        "df_4h_std = df_4h_std.sort_index()\n",
        "window_aggs_df = df_4h_std.merge(df_4h_mavg,left_index=True, right_index=True)\n",
        "\n",
        "# Moving average of transaction frequency.\n",
        "df_4h_count = pd.DataFrame(cc_group.mean())\n",
        "df_4h_count.columns = [\"trans_freq\", \"datetime\"]\n",
        "df_4h_count = df_4h_count.reset_index(level=[\"cc_num\"])\n",
        "df_4h_count = df_4h_count.drop(columns=[\"cc_num\", \"datetime\"])\n",
        "df_4h_count = df_4h_count.sort_index()\n",
        "window_aggs_df = window_aggs_df.merge(df_4h_count,left_index=True, right_index=True)\n",
        "\n",
        "# Moving average of location difference between consecutive transactions.\n",
        "cc_group = trans_df[[\"cc_num\", \"loc_delta\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\").mean()\n",
        "df_4h_loc_delta_mavg = pd.DataFrame(cc_group)\n",
        "df_4h_loc_delta_mavg.columns = [\"loc_delta_mavg\", \"datetime\"]\n",
        "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.reset_index(level=[\"cc_num\"])\n",
        "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
        "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.sort_index()\n",
        "window_aggs_df = window_aggs_df.merge(df_4h_loc_delta_mavg,left_index=True, right_index=True)\n",
        "\n",
        "window_aggs_df = window_aggs_df.merge(trans_df[[\"cc_num\", \"datetime\"]].sort_index(),left_index=True, right_index=True)\n",
        "window_aggs_df.tail()\n"
      ],
      "metadata": {
        "id": "jmywmIVKGgLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert date time object to unix epoch in milliseconds"
      ],
      "metadata": {
        "id": "6fMRN7kEy-rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df.datetime = trans_df.datetime.values.astype(np.int64) // 10 ** 6\n",
        "window_aggs_df.datetime = window_aggs_df.datetime.values.astype(np.int64) // 10 ** 6"
      ],
      "metadata": {
        "id": "jMDoudDtzHXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ü™Ñ Creating Feature Groups\n",
        "\n",
        "A feature group can be seen as a collection of conceptually related features. In our case, we will create a feature group for the transaction data and a feature group for the windowed aggregations on the transaction data. Both will have tid as primary key, which will allow us to join them when creating a dataset in the next tutorial.\n",
        "\n",
        "Feature groups can also be used to define a namespace for features. For instance, in a real-life setting we would likely want to experiment with different window lengths. In that case, we can create feature groups with identical schema for each window length.\n",
        "\n",
        "Before we can create a feature group we need to connect to our feature store.\n"
      ],
      "metadata": {
        "id": "yB90r9qszLe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fs = project.get_feature_store()"
      ],
      "metadata": {
        "id": "WFmD_15TzMHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a feature group we need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group and a version number, if it is not defined it will automatically be incremented to 1."
      ],
      "metadata": {
        "id": "EdpXZQnxzWkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans_fg = fs.create_feature_group(\n",
        "    name=\"transactions\",\n",
        "    version=\"1\",\n",
        "    description=\"Transaction data\",\n",
        "    primary_key=['cc_num'],\n",
        "    event_time=['datetime']\n",
        ")"
      ],
      "metadata": {
        "id": "kDUHXzJ3zXxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Here we have also set online_enabled=True, which enables low latency access to the data. A full list of arguments can be found in the documentation.\n",
        "\n",
        "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent we populate it with its associated data using the save function.\n"
      ],
      "metadata": {
        "id": "J3ZePNxmzbdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trans_fg.save(trans_df)"
      ],
      "metadata": {
        "id": "ptNT4PpPzcfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can move on and do the same thing for the feature group with our windows aggregation."
      ],
      "metadata": {
        "id": "KzmLXQqizoZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_aggs_fg = fs.create_feature_group(\n",
        "    name=f\"transactions_{window_len}_aggs\",\n",
        "    description=f\"Aggregate transaction data over {window_len} windows.\",\n",
        "    primary_key=['cc_num'],\n",
        "    event_time=['datetime']\n",
        ")"
      ],
      "metadata": {
        "id": "PCE0H2TxzruP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_aggs_fg.save(window_aggs_df)"
      ],
      "metadata": {
        "id": "TYk1qaE7zu0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both feature groups are now accessible and searchable in the UI."
      ],
      "metadata": {
        "id": "83QAknPe0Jui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üî™ Feature Selection\n",
        "\n",
        "We start by selecting all the features we want to include for model training/inference."
      ],
      "metadata": {
        "id": "Kp5O29jQ0YrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load feature groups.\n",
        "trans_fg = fs.get_feature_group('transactions', version=1)\n",
        "window_aggs_fg = fs.get_feature_group('transactions_4h_aggs', version=1)\n",
        "\n",
        "# Select features for training data.\n",
        "ds_query = trans_fg.select([\"fraud_label\", \"category\", \"amount\", \"age_at_transaction\", \"days_until_card_expires\", \"loc_delta\"])\\\n",
        "    .join(window_aggs_fg.select_except([\"cc_num\"]), on=\"cc_num\")\\\n",
        "\n",
        "ds_query.show(5)"
      ],
      "metadata": {
        "id": "DpM-7xwj0N4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Recall that we computed the features in transactions_4h_aggs using 4-hour aggregates. If we had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join we would need to include a prefix argument in the join to avoid feature name clash. See the documentation for more details.\n",
        "\n",
        "ü§ñ Transformation Functions </span>\n",
        "\n",
        "We will preprocess our data using min-max scaling on numerical features and label encoding on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as min-max scaling are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage.\n"
      ],
      "metadata": {
        "id": "fh8y7CWg0di9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformation functions.\n",
        "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
        "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
        "\n",
        "# Map features to transformations.\n",
        "transformation_functions = {\n",
        "    \"category\": label_encoder,\n",
        "    \"amount\": min_max_scaler,\n",
        "    \"trans_volume_mavg\": min_max_scaler,\n",
        "    \"trans_volume_mstd\": min_max_scaler,\n",
        "    \"trans_freq\": min_max_scaler,\n",
        "    \"loc_delta\": min_max_scaler,\n",
        "    \"loc_delta_mavg\": min_max_scaler,\n",
        "    \"age_at_transaction\": min_max_scaler,\n",
        "    \"days_until_card_expires\": min_max_scaler,\n",
        "}"
      ],
      "metadata": {
        "id": "MbIo70e50VgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚öôÔ∏è Feature View Creation\n",
        "\n",
        "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions. In order to create a Feature View we may use fs.create_feature_view()\n"
      ],
      "metadata": {
        "id": "1tIvbPlJ0xJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_view = fs.create_feature_view(\n",
        "    name='transactions_view',\n",
        "    query=ds_query,\n",
        "    labels=[\"fraud_label\"],\n",
        "    transformation_functions=transformation_functions\n",
        ")"
      ],
      "metadata": {
        "id": "TantyGyl0zqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üèãÔ∏è Training Dataset Creation\n",
        "\n",
        "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
        "\n",
        "Training Dataset may contain splits such as:\n",
        "\n",
        "    Training set - the subset of training data used to train a model.\n",
        "    Validation set - the subset of training data used to evaluate hparams when training a model\n",
        "    Test set - the holdout subset of training data used to evaluate a mode\n",
        "\n",
        "Training dataset is created using fs.create_training_dataset() method.\n",
        "\n",
        "From feature view APIs we can also create training datasts based on even time filters specifing start_time and end_time\n"
      ],
      "metadata": {
        "id": "hzwtiWqC1Gs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
        "\n",
        "# Create training datasets based event time filter\n",
        "start_time = int(float(datetime.strptime(\"2022-01-01 00:00:01\", date_format).timestamp()) * 1000)\n",
        "end_time = int(float(datetime.strptime(\"2022-02-28 23:59:59\", date_format).timestamp()) * 1000)\n",
        "\n",
        "td_version, td_job = feature_view.create_training_dataset(\n",
        "    description = 'transactions_dataset_jan_feb',\n",
        "    data_format = 'csv',\n",
        "    splits = {'train': 80, 'validation': 20},\n",
        "    train_split = \"train\",    \n",
        "    write_options = {'wait_for_job': True},\n",
        "    coalesce = True,\n",
        "    start_time = start_time,\n",
        "    end_time = end_time,\n",
        ")"
      ],
      "metadata": {
        "id": "efhLRL8l1X4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view and explore data in the feature view we can retrieve batch data using get_batch_data() method"
      ],
      "metadata": {
        "id": "mgzOp0nhJ9B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_view.get_batch_data().head(5)"
      ],
      "metadata": {
        "id": "uyThBbWFJ_sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ü™ù Training Dataset retreival\n",
        "\n",
        "To retrieve training data from storage (already materialised) or from feature groups direcly we can use get_training_dataset_splits or get_training_dataset methods. If version is not provided or provided version has not already existed, it creates a new version of training data according to given arguments and returns a dataframe. If version is provided and has already existed, it reads training data from storage or feature groups and returns a dataframe. If split is provided, it reads the specific split.\n"
      ],
      "metadata": {
        "id": "jQgohnOX1bUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, df = feature_view.get_training_dataset_splits({'train': 80, 'validation': 20}, version = td_version)"
      ],
      "metadata": {
        "id": "FMp73uBD1eER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['train']"
      ],
      "metadata": {
        "id": "RIidxsXb1hG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['validation']"
      ],
      "metadata": {
        "id": "LQ0F4B5s1lJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}